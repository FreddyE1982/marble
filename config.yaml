# Default configuration for MARBLE

# ===================
# Dataset and Logging
# ===================

dataset:
  source: null           # Local path or URL of the default dataset
  num_shards: 1       # Total shards for distributed datasets
  shard_index: 0      # Index of this shard for multi-node setups
  offline: false      # Disable remote dataset downloads
  encryption_key: null  # Optional key for dataset encryption
  cache_url: null      # Optional URL of DatasetCacheServer
  use_kuzu_graph: false  # When true, load training data from a K첫zu graph instead of ``source``
  kuzu_graph:
    db_path: "training.kuzu"  # Filesystem path of the K첫zu database containing training data
    query: "MATCH (n:Sample) RETURN n.input AS input, n.target AS target"  # Cypher query returning training pairs
    input_column: "input"     # Column name for input values
    target_column: "target"   # Column name for target values
    limit: null               # Optional limit on number of pairs loaded from the graph
logging:
  structured: false   # Output logs in JSON format when true
  log_file: "marble.log"  # Path of the main log file
plugins: []           # Additional plugin modules to load

topology_graph:
  enabled: false        # Mirror core topology into a persistent K첫zu graph
  db_path: "topology.kuzu"  # Filesystem path of the K첫zu database

pipeline:
  async_enabled: false  # Execute HighLevelPipeline steps asynchronously when true
  cache_dir: null       # Directory for cached step outputs; defaults to pipeline_cache_gpu or pipeline_cache_cpu

tool_manager:
  enabled: false        # When true the ToolManagerPlugin allows external tool use
  policy: "heuristic"    # Strategy for selecting tools; only "heuristic" is currently available
  tools:                # Mapping of tool names to configuration dictionaries
    web_search: {}
    database_query:
      db_path: "knowledge.kuzu"


cross_validation:
  folds: 5
  seed: 0

serve_model:
  host: "localhost"
  port: 5000

sync:
  interval_ms: 1000     # Interval in milliseconds between cross-device tensor synchronization cycles

evolution:
  population_size: 4        # Number of configurations evaluated per generation
  selection_size: 2         # Top configurations kept after each generation
  generations: 2            # How many generations to run
  steps_per_candidate: 1    # Training steps per candidate during evaluation
  mutation_rate: 0.1        # Fraction of parameters mutated in each offspring
  parallelism: 2            # Number of candidates evaluated concurrently

# =====
# Core
# =====
core:
  backend: numpy               # Tensor backend: "numpy" or "jax"
  xmin: -2.0                    # Mandelbrot minimum x value
  xmax: 1.0                     # Mandelbrot maximum x value
  ymin: -1.5                    # Mandelbrot minimum y value
  ymax: 1.5                     # Mandelbrot maximum y value
  width: 30                     # Grid width
  height: 30                    # Grid height
  max_iter: 50                  # Iterations per wander step
  representation_size: 4        # Number of floats in neuron representation
  message_passing_alpha: 0.5    # Scaling for message aggregation
  weight_init_min: 0.5
  weight_init_max: 1.5
  mandelbrot_escape_radius: 2.0
  mandelbrot_power: 2
  tier_autotune_enabled: true
  memory_cleanup_interval: 60
  representation_noise_std: 0.0
  gradient_clip_value: 1.0
  synapse_weight_decay: 0.0
  message_passing_iterations: 1
  cluster_algorithm: "kmeans"
  vram_limit_mb: 0.5
  ram_limit_mb: 1.0
  disk_limit_mb: 10
  file_tier_path: "data/marble_file_tier.dat"
  init_noise_std: 0.0
  default_growth_tier: "vram"
  random_seed: 42
  backend: "numpy"
  message_passing_dropout: 0.0
  synapse_dropout_prob: 0.0
  synapse_batchnorm_momentum: 0.1
  representation_activation: "tanh"
  apply_layer_norm: true
  use_mixed_precision: false
  quantization_bits: 0
  weight_init_mean: 0.0
  weight_init_std: 1.0
  weight_init_type: "uniform"
  weight_init_strategy: "uniform"
  show_message_progress: false
  message_passing_beta: 1.0
  attention_temperature: 1.0
  attention_dropout: 0.0
  attention_causal: false
  attention_gating:
    enabled: false
    mode: "sine"        # "sine" or "chaos"
    frequency: 1.0      # Oscillations over sequence length for sine mode
    chaos: 3.7          # Logistic map coefficient when mode="chaos"
  salience_weight: 1.0
  energy_threshold: 0.5
  reinforcement_learning_enabled: false
  rl_discount: 0.9
  rl_learning_rate: 0.1
  rl_epsilon: 1.0
  rl_epsilon_decay: 0.95
  rl_min_epsilon: 0.1
  early_cleanup_enabled: false
  pretraining_epochs: 0
  min_cluster_k: 2
  diffusion_steps: 10
  noise_start: 1.0
  noise_end: 0.1
  noise_schedule: "linear"
  workspace_broadcast: false
  activation_output_dir: "activations"
  activation_colormap: "viridis"
  memory_system:
    long_term_path: "diffusion_memory.pkl"
    threshold: 0.5
    consolidation_interval: 10
  cwfl:
    num_basis: 10
    bandwidth: 1.0
    reg_lambda: 0.01
    learning_rate: 0.01
  harmonic:
    base_frequency: 1.0
    decay: 0.99
  fractal:
    target_dimension: 4.0
# Neuronenblitz learning system parameters

  synapse_echo_length: 5
  synapse_echo_decay: 0.9
  interconnection_prob: 0.05
# ============
# Neuronenblitz
# ============
neuronenblitz:
  backtrack_probability: 0.3
  consolidation_probability: 0.2
  consolidation_strength: 1.1
  route_potential_increase: 0.5
  route_potential_decay: 0.9
  route_visit_decay_interval: 10
  alternative_connection_prob: 0.1
  split_probability: 0.2
  merge_tolerance: 0.01
  plasticity_threshold: 10.0
  continue_decay_rate: 0.85
  struct_weight_multiplier1: 1.5
  struct_weight_multiplier2: 1.2
  attention_decay: 0.9
  max_wander_depth: 100
  learning_rate: 0.01
  weight_decay: 0.0
  dropout_probability: 0.0
  dropout_decay_rate: 1.0
  exploration_decay: 0.99
  reward_scale: 1.0
  stress_scale: 1.0
  remote_fallback: false
  noise_injection_std: 0.0
  dynamic_attention_enabled: true
  backtrack_depth_limit: 10
  synapse_update_cap: 1.0
  structural_plasticity_enabled: true
  backtrack_enabled: true
  loss_scale: 1.0
  loss_module: null
  exploration_bonus: 0.0
  synapse_potential_cap: 100.0
  attention_update_scale: 1.0
  plasticity_modulation: 1.0
  wander_depth_noise: 0.0
  reward_decay: 1.0
  synapse_prune_interval: 10
  structural_learning_rate: 0.1
  remote_timeout: 2.0
  gradient_noise_std: 0.0
  min_learning_rate: 0.0001
  max_learning_rate: 0.1
  top_k_paths: 5
  parallel_wanderers: 1
  parallel_update_strategy: best
  beam_width: 1
  wander_cache_ttl: 300
  wander_anomaly_threshold: 3.0
  wander_history_size: 100
  phase_rate: 0.1
  phase_adaptation_rate: 0.05
  synaptic_fatigue_enabled: true
  fatigue_increase: 0.05
  fatigue_decay: 0.95
  lr_adjustment_factor: 0.1
  lr_scheduler: none
  scheduler_steps: 100
  scheduler_gamma: 0.99
  epsilon_scheduler: none
  epsilon_scheduler_steps: 100
  epsilon_scheduler_gamma: 0.99
  momentum_coefficient: 0.0
  use_echo_modulation: false
  reinforcement_learning_enabled: false
  rl_discount: 0.9
  rl_epsilon: 1.0
  rl_epsilon_decay: 0.95
  rl_min_epsilon: 0.1
  entropy_epsilon_enabled: false
  shortcut_creation_threshold: 5
  chaotic_gating_enabled: false
  chaotic_gating_param: 3.7
  chaotic_gate_init: 0.5
  context_history_size: 10
  context_embedding_decay: 0.9
  emergent_connection_prob: 0.05
  concept_association_threshold: 5
  concept_learning_rate: 0.1
  weight_limit: 1000000.0
  wander_cache_size: 50
  rmsprop_beta: 0.99
# Brain-level training configuration

  grad_epsilon: 1.0e-08
  use_experience_replay: false
  replay_buffer_size: 1000
  replay_alpha: 0.6
  replay_beta: 0.4
  replay_batch_size: 32
  exploration_entropy_scale: 1.0
  exploration_entropy_shift: 0.0
  gradient_score_scale: 1.0
  memory_gate_decay: 0.99
  memory_gate_strength: 1.0
  episodic_memory_size: 50
  episodic_memory_threshold: 0.1
  episodic_memory_prob: 0.1
  episodic_sim_length: 5
  curiosity_strength: 0.0
  depth_clip_scaling: 1.0
  forgetting_rate: 0.99
  structural_dropout_prob: 0.0
  gradient_path_score_scale: 1.0
  use_gradient_path_scoring: true
  rms_gradient_path_scoring: false
  activity_gate_exponent: 1.0
  subpath_cache_size: 100
  subpath_cache_ttl: 300
  monitor_wander_factor: 0.0
  monitor_epsilon_factor: 0.0
  use_mixed_precision: false
# =====
# Brain
# =====
brain:
  save_threshold: 0.05
  max_saved_models: 5
  save_dir: "saved_models"
  firing_interval_ms: 500
  initial_neurogenesis_factor: 1.0
  offload_enabled: false
  torrent_offload_enabled: false
  mutation_rate: 0.01
  mutation_strength: 0.05
  prune_threshold: 0.01
  dream_num_cycles: 10
  dream_interval: 5
  neurogenesis_base_neurons: 5
  neurogenesis_base_synapses: 10
  max_training_epochs: 100
  memory_cleanup_enabled: true
  manual_seed: 0
  log_interval: 10
  evaluation_interval: 1
  early_stopping_patience: 5
  early_stopping_delta: 0.001
  auto_cluster_interval: 5
  cluster_method: "kmeans"
  auto_save_enabled: true
  offload_threshold: 1.0
  torrent_offload_threshold: 1.0
  cluster_high_threshold: 1.0
  cluster_medium_threshold: 0.1
  dream_synapse_decay: 0.995
  dream_decay_arousal_scale: 0.0
  dream_decay_stress_scale: 0.0
  neurogenesis_increase_step: 0.1
  neurogenesis_decrease_step: 0.05
  max_neurogenesis_factor: 3.0
  cluster_k: 3
  auto_save_interval: 5
  backup_enabled: false
  backup_interval: 3600
  backup_dir: "backups"
  auto_firing_enabled: false
  dream_enabled: true
  vram_age_threshold: 300
  ram_age_threshold: 600
  status_display_interval: 0
  neurogenesis_interval: 1
  min_cluster_size: 1
  prune_frequency: 1
  auto_offload: false
  benchmark_enabled: false
  benchmark_interval: 2
  loss_growth_threshold: 0.1
  auto_neurogenesis_prob: 0.0
  dream_cycle_sleep: 0.1
  dream_replay_buffer_size: 100
  dream_replay_batch_size: 8
  dream_replay_weighting: "linear"
  dream_instant_buffer_size: 10
  dream_housekeeping_threshold: 0.05
  tier_decision_params:
    vram_usage_threshold: 0.9
    ram_usage_threshold: 0.9
  model_name: "marble_default"
  checkpoint_format: "pickle"
  checkpoint_compress: false
  metrics_history_size: 100
  profile_enabled: false
  profile_log_path: "profile.csv"
  profile_interval: 1
  early_stop_enabled: true
  lobe_sync_interval: 60
  cleanup_batch_size: 500
  remote_sync_enabled: false
  default_activation_function: "tanh"
  neuron_reservoir_size: 1000
  lobe_decay_rate: 0.98
  super_evolution_mode: false
  dimensional_search:
    enabled: false
    max_size: 12
    improvement_threshold: 0.02
    plateau_epochs: 2
lobe_manager:
  attention_increase_factor: 1.05
  attention_decrease_factor: 0.95
formula: "log(1+T)/log(1+I)"
formula_num_neurons: 100
meta_controller:
  history_length: 5
  adjustment: 0.5
  min_threshold: 1.0
  max_threshold: 20.0
neuromodulatory_system:
  initial:
    arousal: 0.0
    stress: 0.0
    reward: 0.0
    emotion: "neutral"
memory_system:
  long_term_path: "long_term_memory.pkl"
  threshold: 0.5
  consolidation_interval: 10
hybrid_memory:
  vector_store_path: "vector_store.pkl"
  symbolic_store_path: "symbolic_memory.pkl"
  kuzu_store_path: null
  max_entries: 1000
data_compressor:
  compression_level: 6
  compression_enabled: true
  delta_encoding: false
  compression_algorithm: "zlib"
  sparse_threshold: null
dataloader:
  tensor_dtype: "uint8"
  track_metadata: true
  enable_round_trip_check: false
  round_trip_penalty: 0.0
  tokenizer_type: null
  tokenizer_json: null
  tokenizer_vocab_size: 30000
experiment_tracker:
  enabled: false
  project: "marble"
  entity: null
  run_name: null
  
network:
  remote_client:
    url: "http://localhost:8001"
    timeout: 5.0
    max_retries: 3
    backoff_factor: 0.5
    track_latency: true
    auth_token: null
    ssl_verify: true
    connect_retry_interval: 5
    heartbeat_timeout: 10
    use_compression: true
  torrent_client:
    client_id: "main"
    buffer_size: 10
    heartbeat_interval: 30
  remote_server:
    enabled: false
    host: "localhost"
    port: 8000
    remote_url: null
    auth_token: null
    ssl_enabled: false
    ssl_cert_file: "server.crt"
    ssl_key_file: "server.key"
    max_connections: 100
    compression_level: 6
    compression_enabled: true
remote_hardware:
  tier_plugin: null
  grpc:
    address: "localhost:50051"
    max_retries: 3
    backoff_factor: 0.5
metrics_visualizer:
  fig_width: 10
  fig_height: 6
  refresh_rate: 1
  color_scheme: "default"
  show_neuron_ids: false
  dpi: 100
  track_memory_usage: false
  track_cpu_usage: false
  log_dir: "logs"
  csv_log_path: "metrics.csv"
  json_log_path: "metrics.jsonl"
  anomaly_std_threshold: 3.0
metrics_dashboard:
  enabled: false
  host: "localhost"
  port: 8050
  update_interval: 1000
  window_size: 10
autograd:
  enabled: false
  learning_rate: 0.01
  gradient_accumulation_steps: 1
global_workspace:
  enabled: false
  capacity: 100
attention_codelets:
  enabled: false
  coalition_size: 1
pytorch_challenge:
  enabled: false
  loss_penalty: 0.1
  speed_penalty: 0.1
  size_penalty: 0.1
gpt:
  enabled: false
  vocab_size: 50
  block_size: 8
  num_layers: 2
  num_heads: 2
  hidden_dim: 64
  learning_rate: 0.001
  num_train_steps: 50
  dataset_path: "data/sample_text.txt"
  batch_size: 4
distillation:
  enabled: false
  alpha: 0.5
  teacher_model: null
reinforcement_learning:
  enabled: false
  algorithm: q_learning
  episodes: 10
  max_steps: 50
  discount_factor: 0.9
  epsilon_start: 1.0
  epsilon_decay: 0.95
  epsilon_min: 0.1
  seed: 0
  double_q: false
contrastive_learning:
  enabled: false
  temperature: 0.5
  epochs: 1
  batch_size: 4
hebbian_learning:
  learning_rate: 0.01
  weight_decay: 0.0

adversarial_learning:
  enabled: false
  epochs: 1
  batch_size: 4
  noise_dim: 1

autoencoder_learning:
  enabled: false
  epochs: 1
  batch_size: 4
  noise_std: 0.1
  noise_decay: 0.99


semi_supervised_learning:
  enabled: false
  epochs: 1
  batch_size: 4
  unlabeled_weight: 0.5

federated_learning:
  enabled: false
  rounds: 1
  local_epochs: 1

curriculum_learning:
  enabled: false
  epochs: 1
  schedule: "linear"
meta_learning:
  enabled: false
  epochs: 1
  inner_steps: 1
  meta_lr: 0.1
  distill_alpha: 0.0
transfer_learning:
  enabled: false
  epochs: 1
  batch_size: 4
  freeze_fraction: 0.5

continual_learning:
  enabled: false
  epochs: 1
  memory_size: 10
imitation_learning:
  enabled: false
  epochs: 1
  max_history: 10
harmonic_resonance_learning:
  enabled: false
  epochs: 1
  base_frequency: 1.0
  decay: 0.99
synaptic_echo_learning:
  enabled: false
  epochs: 1
  echo_influence: 1.0
fractal_dimension_learning:
  enabled: false
  epochs: 1
  target_dimension: 4.0
quantum_flux_learning:
  enabled: false
  epochs: 1
  phase_rate: 0.1
dream_reinforcement_learning:
  enabled: false
  episodes: 1
  dream_cycles: 1
  dream_strength: 0.5
  dream_interval: 1
  dream_cycle_duration: null

continuous_weight_field_learning:
  enabled: false
  epochs: 1
  num_basis: 10
  bandwidth: 1.0
  reg_lambda: 0.01
  learning_rate: 0.01

neural_schema_induction:
  enabled: false
  epochs: 1
  support_threshold: 2
  max_schema_size: 3

conceptual_integration:
  enabled: false
  blend_probability: 0.1
  similarity_threshold: 0.3

n_dimensional_topology:
  enabled: false
  target_dimensions: 6
  attention_threshold: 0.8
  loss_improve_threshold: 0.01
  stagnation_epochs: 5

unified_learning:
  enabled: false
  gating_hidden: 8
  log_path: "unified_log.json"

theory_of_mind:
  hidden_size: 16
  num_layers: 1
  prediction_horizon: 1
  memory_slots: 16
  attention_hops: 1
  mismatch_threshold: 0.1

predictive_coding:
  num_layers: 2
  latent_dim: 16
  learning_rate: 0.001

# =====================
# Experiment Setups
# =====================
experiments:
  - name: default
    core:
      representation_size: 4
    neuronenblitz:
      learning_rate: 0.01
