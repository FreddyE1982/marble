The configuration YAML file controls all components of MARBLE.  Each top-level
section corresponds to a subsystem and exposes parameters that can be tuned to
alter its behaviour.

core:
  xmin: Minimum X coordinate for the Mandelbrot seed. Defines the left boundary
    of the complex plane used when initializing neuron values.
  xmax: Maximum X coordinate for the seed. Together with ``xmin`` this sets the
    horizontal span of the initial fractal.
  ymin: Minimum Y coordinate of the complex plane.
  ymax: Maximum Y coordinate of the complex plane.
  width: Horizontal resolution of the seed grid. Larger values produce more
    neurons but also require additional memory.
  height: Vertical resolution of the seed grid.
  max_iter: Number of Mandelbrot iterations to run. A higher count yields more
    detailed values. Must be greater than zero.
  representation_size: Length of the representation vector attached to each
    neuron. Larger values allow richer message passing but increase memory and
    computation.
  message_passing_alpha: Mixing factor between each neuron's existing
    representation and the update computed from its neighbours during message
    passing. ``0.0`` relies entirely on new information, while ``1.0`` keeps the
    original representation unchanged. Typical values range from 0.3 to 0.8
    depending on how aggressively information should propagate.
  weight_init_min: Lower bound for initial synapse weights when the core builds
    its default connectivity. Values below ``0.0`` allow inhibitory links while
    higher values force excitatory starts.
  weight_init_max: Upper bound for initial synapse weights. Keeping this near
    ``1.5`` maintains moderate signal strength but it may be increased for more
    aggressive dynamics.
  mandelbrot_escape_radius: Escape radius used when computing the Mandelbrot
    seed. Increasing this explores a wider region of the complex plane.
  mandelbrot_power: Power applied during Mandelbrot iterations. ``2`` produces
    the classic set while higher integers create different fractal patterns.
  tier_autotune_enabled: When enabled the core automatically migrates neurons
    between VRAM, RAM and disk to keep usage below the configured
    ``*_limit_mb`` values. Neurons are moved in order of age with the oldest
    leaving the faster tiers first. This prevents out-of-memory errors while
    keeping recently created neurons in the quickest storage available.
  memory_cleanup_interval: Seconds between automated clean-up passes that remove
    expired objects from lower tiers.
  representation_noise_std: Standard deviation of Gaussian noise added to
    representations after each message passing step.
  gradient_clip_value: Absolute value used to clip weight updates during
    training. ``0`` disables clipping.
  message_passing_iterations: Number of times the message passing routine runs
    per training step. ``Core.run_message_passing`` automatically performs this
    many iterations when called without an explicit count. Increasing the value
    deepens information flow but costs time.
  cluster_algorithm: Name of the clustering method used by ``cluster_neurons``;
    ``"kmeans"`` is the current default.
  vram_limit_mb: Maximum megabytes of GPU memory dedicated to VRAM tiers. If
    CUDA is unavailable this limit is merged into RAM.
  ram_limit_mb: Maximum megabytes of system memory used for RAM tiers.
  disk_limit_mb: Maximum megabytes for disk-based neuron tiers.
  file_tier_path: Path to the file used by the ``FileTier`` for persisting
    modified data. The directory is created automatically if it does not
    exist.
  init_noise_std: Standard deviation of Gaussian noise added to the initial
    Mandelbrot values. ``0.0`` keeps deterministic seeds while larger values
    create varied starting states.
  default_growth_tier: Preferred tier used when new neurons are created and no
    tier choice is provided explicitly.
  random_seed: Seed used for random operations in the core. Keeping this fixed
    ensures reproducible initialization.
  message_passing_dropout: Fraction of messages dropped during propagation to
    regularize learning. ``0.0`` disables dropout.
  representation_activation: Activation function used by the internal MLP for
    message passing. Typical values are ``"tanh"`` or ``"relu"``.
  weight_init_mean: Mean of the normal distribution used to initialize
    synaptic weights.
  message_passing_beta: Secondary mixing factor controlling how quickly new
    representations replace old ones when ``alpha`` alone is insufficient.
  attention_temperature: Softmax temperature used by the attention module
    during message passing. Lower values (<1.0) concentrate weight on the
    highest scoring neighbours while larger values produce smoother
    distributions. Must be greater than zero.
  energy_threshold: Minimum energy level required before neurons participate in
    message passing. Helps filter out inactive units.
  reinforcement_learning_enabled: Enable or disable the internal Q-learning
    engine in the core. Set to ``true`` to allow calling the ``rl_`` methods.
  rl_discount: Discount factor applied to future rewards when updating the
    core's Q-table. Typical values range from ``0.8`` to ``0.99``.
  rl_learning_rate: Step size used for Q-value updates. Smaller values result in
    slower but more stable learning.
  rl_epsilon: Initial exploration rate for epsilon-greedy action selection.
  rl_epsilon_decay: Multiplicative decay applied to ``rl_epsilon`` after each
    update step.
  rl_min_epsilon: Minimum exploration rate once decay has progressed.
  early_cleanup_enabled: If ``true``, unused neurons are removed before normal
    cleanup intervals, reducing memory usage.
  pretraining_epochs: Number of unsupervised epochs to run before regular
    training begins.
  min_cluster_k: Smallest number of clusters allowed when ``cluster_neurons``
    is invoked automatically.
  cross_tier_migration: When ``true`` neurons may move between tiers even if
    their age thresholds have not been reached.

neuronenblitz:
  backtrack_probability: Probability between 0 and 1 that the wandering
    algorithm will revisit a previous neuron when exploring a path.
  consolidation_probability: Chance between 0 and 1 that an activated synapse
    will have its potential increased during consolidation.
  consolidation_strength: Multiplier applied when increasing synapse potential.
    Values above 1.0 accelerate learning while lower values slow it down.
  route_potential_increase: Amount added to a synapse's potential when it is
    traversed during activation.
  route_potential_decay: Factor applied periodically to reduce potentials of
    rarely used synapses. Typical range is 0.0–1.0.
  route_visit_decay_interval: Number of activations between decay steps
    specified by ``route_potential_decay``.
  alternative_connection_prob: Probability of creating a new random connection
    instead of following existing routes. Helps exploration of novel paths.
  split_probability: Likelihood that a wander branches into multiple paths at a
    neuron. Must be between 0 and 1.
  merge_tolerance: Maximum difference allowed between neuron values when merging
    converging paths. Small values enforce stricter merging behaviour.
  plasticity_threshold: Minimum potential a synapse must reach before structural
    changes such as adding or removing connections are applied.
  continue_decay_rate: Factor multiplied with the current continuation
    probability during wandering. Lower values shorten exploration paths,
    while values closer to ``1.0`` keep paths longer. Typical range is
    0.7–0.9.
  struct_weight_multiplier1: Multiplier applied to the first new synapse weight
    created during structural plasticity. Controls how strongly the new forward
    connection influences signal flow.
  struct_weight_multiplier2: Multiplier applied to the second synapse that
    reconnects to the original target neuron after a structural change.
  attention_decay: Fraction by which accumulated type attention scores are
    reduced after each query. Values under ``1.0`` gradually forget old
    preferences.
  max_wander_depth: Maximum recursion depth allowed in ``dynamic_wander``.
    Limits exploration of extremely long paths. The actual limit for each
    call is drawn from this value plus ``wander_depth_noise``.
  learning_rate: Scalar applied to weight updates produced by ``weight_update_fn``.
  weight_decay: Proportional decrease applied to all synapse weights each epoch
    to prevent uncontrolled growth. ``0.0`` disables this effect.
  dropout_probability: Fraction of synapses temporarily ignored during training
    to improve robustness.
  dropout_decay_rate: Multiplicative factor applied to ``dropout_probability``
    after each training epoch. Values below ``1.0`` gradually reduce the
    effective dropout as learning progresses while higher values keep it
    constant. The rate must be between ``0.0`` and ``1.0``.
  exploration_decay: Rate at which the exploration bonus diminishes over time.
    Values slightly below ``1.0`` gradually reduce random behaviour.
  reward_scale: Multiplier applied to reward signals before they adjust
    plasticity.
  stress_scale: Multiplier applied to stress signals before they reduce
    plasticity.
  remote_fallback: When true, neurons attempting remote execution fall back to
    local processing if the remote call fails.
  noise_injection_std: Standard deviation of Gaussian noise injected into neuron
    activations during training.
  dynamic_attention_enabled: Enables per-type attention updates in
    ``update_attention``. Disabling keeps attention static.
  backtrack_depth_limit: Maximum distance for which backtracking may revisit
    previous neurons when ``backtrack_enabled`` is true.
  synapse_update_cap: Maximum absolute magnitude allowed for each individual
    weight update. Prevents large sudden jumps that may destabilize learning.
    The default is 1.0 and values typically range from 0.01 to 5.0 depending on
    how aggressively weights should adapt.
  structural_plasticity_enabled: When false, ``apply_structural_plasticity`` is
    skipped entirely.
  backtrack_enabled: If false, wandering never returns to previously visited
    neurons regardless of ``backtrack_probability``.
  loss_scale: Multiplier for training errors before they are applied to weights.
  loss_module: PyTorch loss module object to compute training errors. If set to
    an instance such as ``torch.nn.MSELoss()``, all weight updates use this
    module's output as the error value.
  exploration_bonus: Additional reward added to rarely used synapses when they
    are traversed, encouraging exploration.
  synapse_potential_cap: Upper bound for ``synapse.potential`` to prevent
    runaway growth.
  attention_update_scale: Scaling factor applied when updating neuron type
    attention statistics.
  plasticity_modulation: Global multiplier applied to weight updates after
    neuromodulation. Values above ``1.0`` accelerate structural changes.
  wander_depth_noise: Standard deviation of Gaussian noise used when
    computing the per-call depth limit. Positive values occasionally
    permit longer explorations while negative samples shorten them.
  reward_decay: Factor by which accumulated reward signals diminish each epoch.
  synapse_prune_interval: Number of epochs between automatic pruning passes
    removing low-potential synapses.
  structural_learning_rate: Step size used when modifying synapse weights
    during structural updates.
  remote_timeout: Timeout in seconds when waiting for responses from a remote
    Neuronenblitz instance.
  gradient_noise_std: Standard deviation of Gaussian noise added to weight
    gradients before updates are applied.
  min_learning_rate: Lower bound used when scheduling the learning rate.
  max_learning_rate: Upper bound allowed for adaptive learning rate schemes.
  top_k_paths: Number of most promising wander paths retained during
    exploration.
  parallel_wanderers: How many ``dynamic_wander`` processes to run in parallel
    for each training example. Setting this above ``1`` launches multiple
    temporary Neuronenblitz copies that explore the graph concurrently in
    separate OS processes.  After all processes finish, only the wanderer that
    achieves the greatest loss reduction, path speed improvement and model size
    decrease is replayed in the main process to apply its weight updates and
    structural plasticity.
  synaptic_fatigue_enabled: When true each synapse maintains a temporary
    fatigue value that reduces its effective weight after repeated use. This
    models biological short-term depression and can help prevent domination by
    a few highly active connections.
  fatigue_increase: Amount added to a synapse's fatigue every time it is
    traversed. Larger values cause quicker weakening. ``0.0`` disables new
    fatigue accumulation.
  fatigue_decay: Multiplicative factor applied to all fatigue values at the
    start of each ``dynamic_wander`` call. Values near ``1.0`` make fatigue
    persist for many steps while lower values allow faster recovery.
  lr_adjustment_factor: Fractional step used by ``adjust_learning_rate`` when
    increasing or decreasing ``learning_rate`` in response to recent error
    trends.
  momentum_coefficient: Coefficient used to blend the previous weight update
    with the current gradient. ``0.0`` disables momentum while values between
    ``0.0`` and ``1.0`` accelerate training by smoothing updates. Momentum is
    applied in ``apply_weight_updates_and_attention`` and is stored per-synapse
    so that frequently updated connections gain additional inertia.
  reinforcement_learning_enabled: Turn on Q-learning inside Neuronenblitz. When
    enabled the ``rl_`` methods may be used to select actions and update
    values directly through the network.
  rl_discount: Discount factor for future rewards used by ``rl_update``.
  rl_epsilon: Initial exploration probability for ``rl_select_action``.
  rl_epsilon_decay: Multiplicative decay applied to ``rl_epsilon`` each time an
    update occurs.
  rl_min_epsilon: Smallest allowed exploration rate once decay has taken place.

brain:
  save_threshold: Minimum improvement in validation loss required before the
    brain writes a new checkpoint to disk.
  max_saved_models: Upper bound on the number of checkpoint files retained in
    ``save_dir``.
  save_dir: Directory where checkpoint files are stored.
  firing_interval_ms: Milliseconds between automatic firing cycles when
    auto-fire is active.
  initial_neurogenesis_factor: Starting multiplier that scales the amount of new
    neurons grown during neurogenesis.
  offload_enabled: When true, highly active lobes are sent to a remote brain
    server for processing.
  torrent_offload_enabled: When true, lobes may be distributed among torrent
    clients using the tracker system.
  mutation_rate: Fraction of synapses to mutate during each evolutionary step.
    Higher values introduce more variation but may destabilize learning.
  mutation_strength: Maximum absolute change applied to a mutated synapse's
    weight. Typical values range from 0.01 to 0.1.
  prune_threshold: Synapses with absolute weight below this value are removed
    during pruning, keeping the network efficient.
  dream_num_cycles: Default number of cycles executed each time the brain
    performs a background dreaming session. Values between 1 and 20 are common
    depending on how much consolidation is desired.
  dream_interval: Seconds to wait between consecutive dreaming sessions when
    ``start_dreaming`` runs in the background. Short intervals keep memory
    consolidation frequent but consume more processing time.
  neurogenesis_base_neurons: Base number of neurons grown during each
    neurogenesis event when no explicit value is provided to
    ``perform_neurogenesis``.
  neurogenesis_base_synapses: Base number of synapses introduced alongside new
    neurons during neurogenesis when defaults are used.
  max_training_epochs: Total number of epochs the training loop should run
    before stopping automatically.
  memory_cleanup_enabled: Toggles periodic removal of stale data from RAM and
    disk tiers.
  manual_seed: Random seed applied during initialization for reproducible
    experiments.
  log_interval: Number of batches between status log messages during training.
  evaluation_interval: Number of epochs between validation runs.
  early_stopping_patience: Number of sequential epochs allowed without a
    sufficient decrease in validation loss. Once this count is exceeded the
    training loop terminates early. Set ``0`` to disable patience-based checks.
    Values between ``3`` and ``10`` work well for most experiments.
  early_stopping_delta: Minimum drop in validation loss required to reset the
    patience counter. This prevents noise in the validation metric from
    triggering false improvements. Typical ranges are ``1e-4`` to ``1e-3``.
  auto_cluster_interval: Epoch interval between automatic clustering of neurons.
  cluster_method: Algorithm used for clustering; ``"kmeans"`` is provided but
    others may be added in future.
  auto_save_enabled: Enables periodic saves controlled by ``auto_save_interval``.
  offload_threshold: Minimum lobe attention required before ``offload_high_attention``
    sends a subcore to the remote brain. Increasing this reduces how frequently
    offloading occurs.
  torrent_offload_threshold: Attention threshold used when distributing lobes
    via the torrent client.
  cluster_high_threshold: Score above which clusters are relocated directly to
    the fast ``vram`` tier when ``relocate_clusters`` runs.
  cluster_medium_threshold: Score above which clusters are kept in ``ram`` but
    below the high threshold; lower scores are moved to disk tiers.
  dream_synapse_decay: Multiplicative decay applied to synapse weights during
    each dreaming cycle.
  dream_decay_arousal_scale: Scaling factor applied to ``dream_synapse_decay``
    proportional to current arousal. ``0.0`` disables arousal-based modulation
    while higher values increase decay when arousal rises.
  dream_decay_stress_scale: Scaling factor that reduces the effective decay
    as stress grows. Values around ``0.0`` keep stress from affecting dream
    strength, whereas ``1.0`` completely cancels decay at full stress.
  neurogenesis_increase_step: Amount added to the neurogenesis factor when
    validation loss worsens.
  neurogenesis_decrease_step: Amount subtracted from the neurogenesis factor
    when validation improves.
  max_neurogenesis_factor: Upper bound that ``neurogenesis_factor`` will not
    exceed, regardless of adjustments.
  cluster_k: Number of clusters created when ``cluster_neurons`` is invoked
    during training.
  auto_save_interval: Number of epochs between automatic calls to
    ``save_model``. ``0`` disables periodic saving.
  auto_firing_enabled: If true the brain starts an auto-firing thread
    immediately after construction.
  dream_enabled: Enables background dreaming when ``start_dreaming`` is called.
  vram_age_threshold: Age in seconds above which neurons in VRAM are considered
    old when deciding growth tiers.
  ram_age_threshold: Equivalent threshold for neurons in RAM.
  status_display_interval: If greater than zero, ``display_live_status`` is
    invoked every N epochs during training.
  neurogenesis_interval: Number of epochs between automatic neurogenesis events.
  min_cluster_size: Minimum number of neurons required to form a cluster.
  prune_frequency: Number of epochs between automatic pruning operations.
  auto_offload: When true, ``offload_high_attention`` runs automatically after
    each training epoch.
  benchmark_enabled: Enables evaluation through ``BenchmarkManager`` at the end
    of each epoch.
  benchmark_interval: Number of epochs between automatic benchmark comparisons
    of pure MARBLE and its autograd pathway. Higher values reduce overhead.
  loss_growth_threshold: Validation loss level that triggers expansion of the
    core during training. The default of ``0.1`` keeps growth infrequent.
  auto_neurogenesis_prob: Baseline probability between 0 and 1 that
    ``maybe_autonomous_neurogenesis`` creates new neurons each epoch. The
    probability scales with the current validation loss.
  dream_cycle_sleep: Seconds to wait between dream cycles. Increase to reduce
    CPU usage during prolonged dreaming.
  tier_decision_params:
    vram_usage_threshold: Fraction of VRAM usage that triggers migration of
      neurons to a lower tier.
    ram_usage_threshold: Fraction of RAM usage that triggers migration to disk
      or file tiers.
  model_name: Descriptive name used when saving checkpoints.
  checkpoint_format: Serialization format for checkpoints. ``"pickle"`` is
    the default but ``"safetensors"`` may be used for faster loading.
  metrics_history_size: Number of past epochs kept in memory for plotting
    performance metrics.
  early_stop_enabled: When ``true`` the trainer monitors the validation loss
    after each epoch and halts once the patience criteria are met. Set to
    ``false`` to force the loop to run ``max_training_epochs`` regardless of
    validation performance.
  lobe_sync_interval: Seconds between background synchronization of lobe data
    when remote syncing is active.
  cleanup_batch_size: Maximum number of objects removed in one cleanup pass.
  remote_sync_enabled: When ``true`` brain state is periodically pushed to a
    remote server.
  default_activation_function: Activation used when constructing new neurons.
  neuron_reservoir_size: Capacity of the internal neuron reservoir used when
    sampling replacements during pruning.
  lobe_decay_rate: Fraction by which lobe attention decays each epoch without
    activity.
  dimensional_search:
    enabled: Toggles automatic growth of the representation vector when
      validation loss improvements stagnate. ``true`` enables monitoring.
    max_size: Highest dimension the search may reach. Should exceed the initial
      ``representation_size`` but remain manageable for available hardware.
    improvement_threshold: Relative decrease in validation loss required to
      deem a new dimension successful. Values around ``0.01`` to ``0.05`` are
      typical.
    plateau_epochs: Number of consecutive epochs below the improvement
      threshold before another dimension is added.
  super_evolution_mode: When true, a controller records loss, speed, complexity
    and resource metrics each epoch and adjusts all configurable parameters via
    self-attention. This prioritizes minimising loss, then maximising speed,
    reducing complexity and finally conserving resources.

formula: Mathematical expression used to compute initial neuron values instead
  of the Mandelbrot seed. If omitted the Mandelbrot algorithm is used.
formula_num_neurons: Number of neurons created when ``formula`` is provided.

meta_controller:
  # The meta-controller monitors validation losses and adjusts
  # ``neuronenblitz.plasticity_threshold`` accordingly. If losses rise the
  # threshold is lowered so structural changes become less likely. When losses
  # fall it increases the threshold within the allowed range to encourage more
  # exploration. This keeps learning stable over long runs.
  history_length: Number of recent validation losses used for the adjustment.
    Typical values range from 3 to 20 depending on how quickly adaptation should
    react.
  adjustment: Amount by which the plasticity threshold changes when performance
    improves or degrades. Values between 0.1 and 1.0 are common.
  min_threshold: Lower bound for the plasticity threshold. Should remain above
    zero, often around 0.5–5.0.
  max_threshold: Upper bound for the plasticity threshold. Values around
    10–50 keep the system from becoming too rigid.

neuromodulatory_system:
  # Represents the internal state of the brain. Arousal and reward boost
  # neurogenesis while stress suppresses it. Emotion provides a qualitative tag
  # that may be used by higher-level modules. Values typically lie between 0.0
  # and 1.0 and change dynamically during training.
  initial:
    arousal: Baseline arousal level in the range 0.0–1.0. Higher arousal
      increases neurogenesis rates.
    stress: Initial stress level in the range 0.0–1.0. Stress reduces
      plasticity and slows learning.
    reward: Starting reward signal in the range 0.0–1.0 that encourages
      reinforcement of successful pathways.
    emotion: Starting emotional state for the system. ``"neutral"`` is typical
      but any descriptive string is allowed.

memory_system:
  # Provides short-term memory in RAM and long-term memory on disk. The system
  # consolidates entries from the volatile store into the persistent file when
  # required.
  long_term_path: Path to the file used for persistent long-term storage of
    memories.
  threshold: When ``arousal`` or ``reward`` exceed this value the
    ``MemorySystem`` stores new entries directly in the long-term layer instead
    of the short-term layer. Typical values range from 0.3 to 0.7 depending on
    how aggressively long-term consolidation should occur.
  consolidation_interval: Number of operations between calls to
    ``consolidate``. Larger values delay writes to disk but reduce I/O.

remote_client:
  # Configuration for forwarding parts of the brain to a remote server. When
  # ``offload_enabled`` is true the brain serializes a subcore and sends it to
  # this endpoint for processing.
  url: Base URL for the remote HTTP brain server.
  timeout: Seconds to wait for remote requests before they are aborted. Values
    between 5 and 30 are typical depending on network latency.
  max_retries: Number of times a remote operation is retried upon failure.
  auth_token: Optional token sent as an ``Authorization`` header on each
    request.
  ssl_verify: Whether HTTPS certificates should be verified. Set to ``false``
    when using self-signed certificates.
  connect_retry_interval: Seconds to wait between connection retry attempts.
  heartbeat_timeout: Maximum time to wait for a heartbeat response when
    maintaining persistent connections.
  use_compression: If ``true`` payloads are compressed before transmission.

torrent_client:
  # Enables distribution of brain parts through the tracker network. Each client
  # processes assigned subcores asynchronously.
  client_id: Identifier used when registering with the torrent tracker. Must be
    unique across participating clients.
  buffer_size: Maximum number of asynchronous tasks queued for a torrent
    client. Larger values allow more parallel requests but use more memory.
  heartbeat_interval: Seconds between status pings sent to the tracker. Has no
    effect in the current simplified implementation but reserved for future use.

data_compressor:
  # Controls how strongly the DataCompressor reduces data size before it is
  # stored or transmitted.
  compression_level: Integer from 0 to 9 passed directly to ``zlib.compress``.
    Higher levels yield smaller output but require more CPU time. The default of
    6 balances speed and ratio for general use.
  compression_enabled: Set to ``false`` to bypass compression entirely. Useful
    during debugging or when working with already compressed data.

dataloader:
  # Settings for the ``DataLoader`` which serializes and compresses data.
  # ``tensor_dtype`` defines the NumPy dtype used when encoding objects and
  # arrays. This affects the precision of the returned tensors and must be a
  # valid dtype name like ``"uint8"`` or ``"int16"``. Wider dtypes can prevent
  # overflow with exceptionally large payloads but increase memory usage.
  tensor_dtype: String specifying the dtype of encoded tensors.

remote_server:
  # Launches an optional local ``RemoteBrainServer`` so this MARBLE instance can
  # forward computation to another machine. When ``enabled`` is ``true`` the
  # server starts automatically using the provided ``host`` and ``port``. If
  # ``remote_url`` is specified the server itself forwards heavy requests to that
  # address, creating a chain of offload targets.
  enabled: Whether to start the server alongside the main process.
  host: Interface address to bind the HTTP server to. Usually ``"localhost"``
    for local testing.
  port: TCP port used by the server.
  remote_url: Optional URL of another remote brain server to which this server
    offloads work.
  ssl_enabled: Enables HTTPS with the provided certificate and key files.
  ssl_cert_file: Path to the SSL certificate used when ``ssl_enabled`` is true.
  ssl_key_file: Path to the private key for the certificate.
  max_connections: Maximum concurrent connections the server will accept.
  compression_level: Compression strength used when exchanging data with
    clients. The value is passed to ``zlib.compress`` and ranges from 0
    (no compression) to 9 (maximum compression).
  compression_enabled: Set to ``false`` to disable compression entirely. When
    disabled the server expects plain JSON payloads.

metrics_visualizer:
  # Configure the live metrics plot size. These values are passed directly to
  # ``matplotlib`` when creating the figure.
  fig_width: Width of the metrics figure in inches.
  fig_height: Height of the metrics figure in inches.
  refresh_rate: How often the plot is refreshed in seconds.
  color_scheme: Name of the Matplotlib style to apply when rendering metrics.
  show_neuron_ids: If true the plot displays neuron identifiers next to data
    points.
  dpi: Resolution of the output figure in dots per inch.

metrics_dashboard:
  # Optional web dashboard built with Plotly Dash for real-time monitoring.
  # When enabled a small HTTP server serves a page that visualizes the same
  # metrics collected by ``MetricsVisualizer`` but updates automatically in the
  # browser. This is useful when running training remotely and you still want
  # to monitor progress live.
  enabled: Set to ``true`` to start the dashboard in a background thread.
  host: Interface address for the Dash server, usually ``"localhost"``.
  port: TCP port used by the dashboard. Ensure this port is free.
  update_interval: Milliseconds between refreshes of the dashboard graphs.

lobe_manager:
  # Parameters controlling how strongly neuron attention is adjusted when the
  # ``LobeManager`` performs self-attention.
  attention_increase_factor: Multiplier applied to neuron attention when a
    lobe's score is below average and the loss is positive. Values slightly above
    ``1.0`` encourage struggling lobes to contribute more.
  attention_decrease_factor: Multiplier used when a lobe's attention is above
    average but the loss is not improving. Numbers below ``1.0`` gradually reduce
    emphasis on those lobes.

autograd:
  enabled: Set to true to wrap the Brain with a transparent PyTorch autograd layer. When enabled, gradients from PyTorch operations are applied to MARBLE synapse weights without altering the underlying architecture.
  learning_rate: Step size used when the autograd layer applies gradient updates to synapse weights during backward passes. Typical values range from 0.001 to 0.1.
pytorch_challenge:
  enabled: If true the training loop compares MARBLE with a pretrained PyTorch model after every training example. When MARBLE's validation loss, inference speed or model size exceed the PyTorch model, neuromodulatory stress is increased which lowers plasticity on subsequent updates.
  loss_penalty: Amount of stress added when MARBLE's loss is worse than the PyTorch baseline. Values around 0.1 provide noticeable pressure without overwhelming the system.
  speed_penalty: Stress increment applied when MARBLE's inference time is slower than the baseline model. Setting this near 0.1 encourages optimisations to execution paths.
  size_penalty: Stress increment when MARBLE grows larger than the baseline model in megabytes. This guides synaptic pruning and structural plasticity to favour compactness.

gpt:
  # The GPT component enables MARBLE to build and train a transformer-based language model entirely from scratch. When
  # `enabled` is true a small GPT architecture is initialised using the parameters below and trained on token sequences
  # provided by the user. This allows experimentation with generative text models without relying on any pretrained weights.
  enabled: Set to true to activate GPT training within MARBLE.
  vocab_size: Size of the discrete token vocabulary used when constructing the embedding layers. Must be greater than one.
  block_size: Number of tokens processed at once. Sequences longer than this are split into blocks. Typical values range
    from 8 to 128 for lightweight experiments.
  num_layers: Number of transformer encoder layers stacked in the GPT. Increasing this improves modelling power but also
    memory and compute requirements. Values between 2 and 8 are common for small models.
  num_heads: Quantity of attention heads per layer. More heads allow the model to capture diverse relationships between
    tokens. Small configurations often use 2 to 8 heads.
  hidden_dim: Dimensionality of the token and positional embeddings as well as the hidden size of each transformer layer.
    Higher values provide more capacity but also increase memory usage.
  learning_rate: Step size for the Adam optimiser when training the GPT model. Start with around 0.001 and adjust based
    on observed convergence.
  num_train_steps: Number of update steps performed during GPT training. Keeping this low (tens to hundreds) ensures test
    runs complete quickly while still demonstrating learning behaviour.
  dataset_path: Path to a plain text file used to generate training sequences. Characters are read from the file and
    mapped to token IDs until ``vocab_size`` unique tokens are collected. The file must contain at least ``block_size``
    characters so that each training example can include a start and target token. Using larger files improves vocabulary
    coverage and allows more diverse sequences.
  batch_size: Number of sequences processed before each optimisation step. Batches between 1 and 64 are typical. Larger
    values yield smoother gradient estimates but require more memory, especially when running on GPU with CuPy.

distillation:
  # Knowledge distillation allows training a "student" brain using guidance from
  # a pre-trained "teacher" brain. When enabled the student blends the teacher's
  # predictions with the ground truth targets before performing regular training
  # updates. This is useful for compressing a large model into a smaller one or
  # for transferring knowledge between different MARBLE instances.
  enabled: Set to ``true`` to activate distillation during training.
  alpha: Weighting factor between the true target and the teacher's output when
    computing the blended target value. ``0.0`` relies solely on the ground
    truth while ``1.0`` exactly matches the teacher. Typical values range from
    ``0.3`` to ``0.7`` depending on how strongly the teacher should influence
    learning.
  teacher_model: Path to a pickled MARBLE system used as the teacher. If ``null``
    the caller must provide the teacher programmatically when invoking the
    distillation trainer.

reinforcement_learning:
  enabled: Activate training in a reinforcement learning loop using the MARBLE
    core and Neuronenblitz. Set to ``true`` to begin episodic interaction with
    the environment defined in ``reinforcement_learning.py``.
  episodes: Number of training episodes to run. More episodes generally yield
    better policies but increase computation time.
  max_steps: Maximum steps per episode before resetting the environment.
  discount_factor: Discount applied to future rewards when computing Q-values.
  epsilon_start: Initial exploration rate for the epsilon-greedy policy.
  epsilon_decay: Multiplicative decay applied to ``epsilon`` after each update.
  epsilon_min: Lowest exploration rate allowed once ``epsilon`` has decayed.

contrastive_learning:
  # Self-supervised contrastive representation learning using the
  # ``ContrastiveLearner``. Two augmented views of each sample are
  # processed by Neuronenblitz and the InfoNCE objective adjusts
  # synaptic weights via ``apply_weight_updates_and_attention``.
  enabled: Set to ``true`` to run contrastive learning after standard
    training. When enabled the ``contrastive_train`` method of
    Neuronenblitz processes the dataset according to the parameters
    below.
  temperature: Softmax temperature applied to similarity scores when
    computing the InfoNCE loss. Lower values sharpen the distribution.
    Typical values range from ``0.1`` to ``1.0``.
  epochs: Number of passes over the dataset for contrastive learning.
    Each epoch iterates over the inputs in mini-batches of size
    ``batch_size`` and updates weights after every batch.
  batch_size: Number of original samples to include in each batch. Each
    sample produces two augmented views, so the effective batch size for
    the loss is ``2 * batch_size``. Values between ``2`` and ``32`` work
    well depending on available memory.

hebbian_learning:
  # Unsupervised Hebbian update rule integrated with Neuronenblitz.
  # After each wander the weights along the traversed path are
  # adjusted based on the product of pre- and post-synaptic values.
  # ``learning_rate`` scales the Hebbian update and ``weight_decay``
  # applies an L2 style penalty to prevent unbounded growth.
  learning_rate: Positive step size controlling how strongly synapse
    weights are increased when correlated neuron activations are
    observed. Typical values range from ``0.001`` to ``0.1``.
  weight_decay: Fraction of the current weight subtracted during each
    update. Helps stabilise learning when set between ``0.0`` and
    ``0.01``.
