The configuration YAML file controls all components of MARBLE.  Each top-level
section corresponds to a subsystem and exposes parameters that can be tuned to
alter its behaviour.  When the file is loaded it is validated against an
internal JSON schema to catch missing sections or invalid values early.

dataset:
  num_shards: Number of shards the training dataset is split into. When greater
    than ``1`` each training worker should set ``shard_index`` to a unique value
    between ``0`` and ``num_shards - 1`` so that only a subset of samples is
    processed. This enables distributed training without duplicating effort.
logging:
  structured: When true, all logs are emitted as JSON objects. This simplifies parsing in tools like Logstash or Splunk.
  log_file: File path where structured logs are written. If set to ``null`` the logs go to stderr.

  shard_index: Index of the shard this instance will process. Set to ``0`` when
    running on a single machine. Values outside the valid range raise an error
    during dataset loading.

core:
  xmin: Minimum X coordinate for the Mandelbrot seed. Defines the left boundary
    of the complex plane used when initializing neuron values.
  xmax: Maximum X coordinate for the seed. Together with ``xmin`` this sets the
    horizontal span of the initial fractal.
  ymin: Minimum Y coordinate of the complex plane.
  ymax: Maximum Y coordinate of the complex plane.
  width: Horizontal resolution of the seed grid. Larger values produce more
    neurons but also require additional memory.
  height: Vertical resolution of the seed grid.
  max_iter: Number of Mandelbrot iterations to run. A higher count yields more
    detailed values. Must be greater than zero.
  representation_size: Length of the representation vector attached to each
    neuron. Larger values allow richer message passing but increase memory and
    computation.
  message_passing_alpha: Mixing factor between each neuron's existing
    representation and the update computed from its neighbours during message
    passing. ``0.0`` relies entirely on new information, while ``1.0`` keeps the
    original representation unchanged. Typical values range from 0.3 to 0.8
    depending on how aggressively information should propagate.
  weight_init_min: Lower bound for initial synapse weights when the core builds
    its default connectivity. Values below ``0.0`` allow inhibitory links while
    higher values force excitatory starts.
  weight_init_max: Upper bound for initial synapse weights. Keeping this near
    ``1.5`` maintains moderate signal strength but it may be increased for more
    aggressive dynamics.
  mandelbrot_escape_radius: Escape radius used when computing the Mandelbrot
    seed. Increasing this explores a wider region of the complex plane.
  mandelbrot_power: Power applied during Mandelbrot iterations. ``2`` produces
    the classic set while higher integers create different fractal patterns.
  tier_autotune_enabled: When enabled the core automatically migrates neurons
    between VRAM, RAM and disk to keep usage below the configured
    ``*_limit_mb`` values. Neurons are moved in order of age with the oldest
    leaving the faster tiers first. This prevents out-of-memory errors while
    keeping recently created neurons in the quickest storage available.
  memory_cleanup_interval: Seconds between automated clean-up passes that remove
    expired objects from lower tiers.
  representation_noise_std: Standard deviation of Gaussian noise added to
    representations after each message passing step.
  gradient_clip_value: Absolute value used to clip weight updates during
    training. ``0`` disables clipping.
  synapse_weight_decay: Fraction of each synapse's weight removed after every
    call to ``run_message_passing``. This gradually shrinks connections to
    prevent runaway growth. Typical values are between ``0.0`` (disabled) and
    ``0.1`` for slow decay.
  message_passing_iterations: Number of times the message passing routine runs
    per training step. ``Core.run_message_passing`` automatically performs this
    many iterations when called without an explicit count. Increasing the value
    deepens information flow but costs time.
  cluster_algorithm: Name of the clustering method used by ``cluster_neurons``;
    ``"kmeans"`` is the current default.
  vram_limit_mb: Maximum megabytes of GPU memory dedicated to VRAM tiers. If
    CUDA is unavailable this limit is merged into RAM.
  ram_limit_mb: Maximum megabytes of system memory used for RAM tiers.
  disk_limit_mb: Maximum megabytes for disk-based neuron tiers.
  file_tier_path: Path to the file used by the ``FileTier`` for persisting
    modified data. The directory is created automatically if it does not
    exist.
  init_noise_std: Standard deviation of Gaussian noise added to the initial
    Mandelbrot values. ``0.0`` keeps deterministic seeds while larger values
    create varied starting states.
  default_growth_tier: Preferred tier used when new neurons are created and no
    tier choice is provided explicitly.
  random_seed: Seed used for random operations in the core. Keeping this fixed
    ensures reproducible initialization.
  message_passing_dropout: Fraction of messages dropped during propagation to
    regularize learning. ``0.0`` disables dropout.
  representation_activation: Activation function used by the internal MLP for
    message passing. Typical values are ``"tanh"`` or ``"relu"``.
  apply_layer_norm: When true the hidden layer of ``_simple_mlp`` is normalised
    to zero mean and unit variance before the final activation. This often
    stabilises training dynamics. Set ``false`` to disable.
  weight_init_mean: Mean of the normal distribution used to initialize
    synaptic weights.
  weight_init_std: Standard deviation of the normal distribution when
    ``weight_init_type`` is set to ``"normal"``. Higher values produce a
    wider spread of initial weights.
  weight_init_type: Distribution used for initial synapse weights. ``"uniform"``
    samples values between ``weight_init_min`` and ``weight_init_max``.
    ``"normal"`` draws from a normal distribution with ``weight_init_mean``
    and ``weight_init_std``. ``"xavier_uniform"`` uses Xavier uniform
    initialization based on the connection fan-in and fan-out.
  weight_init_strategy: Strategy for the message passing MLP weights. Supports
    ``"uniform"`` and ``"normal"`` with the same semantics as
    ``weight_init_type``.
  show_message_progress: When ``true`` ``perform_message_passing`` renders a
    progress bar so long operations provide visual feedback.
  message_passing_beta: Secondary mixing factor controlling how quickly new
    representations replace old ones when ``alpha`` alone is insufficient.
  attention_temperature: Softmax temperature used by the attention module
    during message passing. Lower values (<1.0) concentrate weight on the
    highest scoring neighbours while larger values produce smoother
    distributions. Must be greater than zero.
  attention_dropout: Probability between 0.0 and 1.0 of ignoring an incoming
    message during attention computation. A value of ``0.0`` disables dropout
    while ``1.0`` results in no neighbours being considered. When messages are
    dropped the remaining attention weights are renormalized so the sum equals
    one.
  global_phase_rate: Amount added to ``global_phase`` every time
    ``run_message_passing`` executes. Combined with each synapse's individual
    ``phase`` value, this creates oscillatory gating via the cosine in
    ``Synapse.effective_weight``. Set to ``0.0`` to disable phase-based
    modulation or increase toward ``math.pi`` for quicker oscillations.
  energy_threshold: Minimum energy level required before neurons participate in
    message passing. Helps filter out inactive units.
  reinforcement_learning_enabled: Enable or disable the internal Q-learning
    engine in the core. Set to ``true`` to allow calling the ``rl_`` methods.
  rl_discount: Discount factor applied to future rewards when updating the
    core's Q-table. Typical values range from ``0.8`` to ``0.99``.
  rl_learning_rate: Step size used for Q-value updates. Smaller values result in
    slower but more stable learning.
  rl_epsilon: Initial exploration rate for epsilon-greedy action selection.
  rl_epsilon_decay: Multiplicative decay applied to ``rl_epsilon`` after each
    update step.
  rl_min_epsilon: Minimum exploration rate once decay has progressed.
  early_cleanup_enabled: If ``true``, unused neurons are removed before normal
    cleanup intervals, reducing memory usage.
  pretraining_epochs: Number of unsupervised epochs to run before regular
    training begins.
  min_cluster_k: Smallest number of clusters allowed when ``cluster_neurons``
    is invoked automatically.
  cross_tier_migration: When ``true`` neurons may move between tiers even if
    their age thresholds have not been reached.
  synapse_echo_length: Number of past activations each synapse stores in its
    echo buffer. Larger values retain longer history but increase memory usage.
  synapse_echo_decay: Multiplicative factor applied to incoming activations
    before they are appended to the echo buffer. Values in ``0.0``–``1.0``
    gradually forget older activity.
  interconnection_prob: Probability of creating an interconnection synapse when
    multiple cores are combined. ``0.0`` keeps cores isolated while higher
    values densely link neurons across cores.

neuronenblitz:
  backtrack_probability: Probability between 0 and 1 that the wandering
    algorithm will revisit a previous neuron when exploring a path.
  consolidation_probability: Chance between 0 and 1 that an activated synapse
    will have its potential increased during consolidation.
  consolidation_strength: Multiplier applied when increasing synapse potential.
    Values above 1.0 accelerate learning while lower values slow it down.
  route_potential_increase: Amount added to a synapse's potential when it is
    traversed during activation.
  route_potential_decay: Factor applied periodically to reduce potentials of
    rarely used synapses. Typical range is 0.0–1.0.
  route_visit_decay_interval: Number of activations between decay steps
    specified by ``route_potential_decay``.
  alternative_connection_prob: Probability of creating a new random connection
    instead of following existing routes. Helps exploration of novel paths.
  split_probability: Likelihood that a wander branches into multiple paths at a
    neuron. Must be between 0 and 1.
  merge_tolerance: Maximum difference allowed between neuron values when merging
    converging paths. Small values enforce stricter merging behaviour.
  plasticity_threshold: Minimum potential a synapse must reach before structural
    changes such as adding or removing connections are applied.
  continue_decay_rate: Factor multiplied with the current continuation
    probability during wandering. Lower values shorten exploration paths,
    while values closer to ``1.0`` keep paths longer. Typical range is
    0.7–0.9.
  struct_weight_multiplier1: Multiplier applied to the first new synapse weight
    created during structural plasticity. Controls how strongly the new forward
    connection influences signal flow.
  struct_weight_multiplier2: Multiplier applied to the second synapse that
    reconnects to the original target neuron after a structural change.
  attention_decay: Fraction by which accumulated type attention scores are
    reduced after each query. Values under ``1.0`` gradually forget old
    preferences.
  max_wander_depth: Maximum recursion depth allowed in ``dynamic_wander``.
    Limits exploration of extremely long paths. The actual limit for each
    call is drawn from this value plus ``wander_depth_noise``.
  learning_rate: Scalar applied to weight updates produced by ``weight_update_fn``.
  weight_decay: Proportional decrease applied to all synapse weights each epoch
    to prevent uncontrolled growth. ``0.0`` disables this effect.
  dropout_probability: Fraction of synapses temporarily ignored during training
    to improve robustness.
  dropout_decay_rate: Multiplicative factor applied to ``dropout_probability``
    after each training epoch. Values below ``1.0`` gradually reduce the
    effective dropout as learning progresses while higher values keep it
    constant. The rate must be between ``0.0`` and ``1.0``.
  exploration_decay: Rate at which the exploration bonus diminishes over time.
    Values slightly below ``1.0`` gradually reduce random behaviour.
  reward_scale: Multiplier applied to reward signals before they adjust
    plasticity.
  stress_scale: Multiplier applied to stress signals before they reduce
    plasticity.
  remote_fallback: When true, neurons attempting remote execution fall back to
    local processing if the remote call fails.
  noise_injection_std: Standard deviation of Gaussian noise injected into neuron
    activations during training.
  dynamic_attention_enabled: Enables per-type attention updates in
    ``update_attention``. Disabling keeps attention static.
  backtrack_depth_limit: Maximum distance for which backtracking may revisit
    previous neurons when ``backtrack_enabled`` is true.
  synapse_update_cap: Maximum absolute magnitude allowed for each individual
    weight update. Prevents large sudden jumps that may destabilize learning.
    The default is 1.0 and values typically range from 0.01 to 5.0 depending on
    how aggressively weights should adapt.
  structural_plasticity_enabled: When false, ``apply_structural_plasticity`` is
    skipped entirely.
  backtrack_enabled: If false, wandering never returns to previously visited
    neurons regardless of ``backtrack_probability``.
  loss_scale: Multiplier for training errors before they are applied to weights.
  loss_module: PyTorch loss module object to compute training errors. If set to
    an instance such as ``torch.nn.MSELoss()``, all weight updates use this
    module's output as the error value.
  exploration_bonus: Additional reward added to rarely used synapses when they
    are traversed, encouraging exploration.
  synapse_potential_cap: Upper bound for ``synapse.potential`` to prevent
    runaway growth.
  attention_update_scale: Scaling factor applied when updating neuron type
    attention statistics.
  plasticity_modulation: Global multiplier applied to weight updates after
    neuromodulation. Values above ``1.0`` accelerate structural changes.
  wander_depth_noise: Standard deviation of Gaussian noise used when
    computing the per-call depth limit. Positive values occasionally
    permit longer explorations while negative samples shorten them.
  reward_decay: Factor by which accumulated reward signals diminish each epoch.
  synapse_prune_interval: Number of epochs between automatic pruning passes
    removing low-potential synapses.
  structural_learning_rate: Step size used when modifying synapse weights
    during structural updates.
  remote_timeout: Timeout in seconds when waiting for responses from a remote
    Neuronenblitz instance.
  gradient_noise_std: Standard deviation of Gaussian noise added to weight
    gradients before updates are applied.
  min_learning_rate: Lower bound used when scheduling the learning rate.
  max_learning_rate: Upper bound allowed for adaptive learning rate schemes.
  top_k_paths: Number of most promising wander paths retained during
    exploration.
  parallel_wanderers: How many ``dynamic_wander`` processes to run in parallel
    for each training example. Setting this above ``1`` launches multiple
    temporary Neuronenblitz copies that explore the graph concurrently in
    separate OS processes.  After all processes finish, only the wanderer that
    achieves the greatest loss reduction, path speed improvement and model size
    decrease is replayed in the main process to apply its weight updates and
    structural plasticity.
  beam_width: Number of candidate paths retained at each depth during the
    beam search variant of ``dynamic_wander``. Wider beams explore more options
    but increase computation time. A value of ``1`` disables beam search and
    reverts to the original recursive wander.
  wander_cache_ttl: Time-to-live in seconds for ``dynamic_wander`` cache
    entries. Cached paths older than this value are removed before reuse.
    Values above ``0`` enable expiration while ``0`` keeps results indefinitely.
  phase_rate: Amount added to ``global_phase`` each time ``dynamic_wander``
    runs. Higher values make the oscillator advance more quickly, altering
    the cosine gating applied to every synapse.
  phase_adaptation_rate: Step size used to adjust each synapse's individual
    ``phase`` after a weight update. Positive error values push phases
    forward while negative errors pull them backward. The value should
    typically stay below ``1.0`` to prevent rapid oscillations.
  synaptic_fatigue_enabled: When true each synapse maintains a temporary
    fatigue value that reduces its effective weight after repeated use. This
    models biological short-term depression and can help prevent domination by
    a few highly active connections.
  fatigue_increase: Amount added to a synapse's fatigue every time it is
    traversed. Larger values cause quicker weakening. ``0.0`` disables new
    fatigue accumulation.
  fatigue_decay: Multiplicative factor applied to all fatigue values at the
    start of each ``dynamic_wander`` call. Values near ``1.0`` make fatigue
    persist for many steps while lower values allow faster recovery.
  lr_adjustment_factor: Fractional step used by ``adjust_learning_rate`` when
    increasing or decreasing ``learning_rate`` in response to recent error
    trends.
  lr_scheduler: Scheduler type controlling how ``learning_rate`` evolves over
    epochs. ``"none"`` keeps the rate constant aside from manual adjustments,
    ``"cosine"`` performs a cosine decay over ``scheduler_steps`` epochs,
    ``"exponential"`` multiplies the rate by ``scheduler_gamma`` each epoch, and
    ``"cyclic"`` oscillates the rate between ``min_learning_rate`` and
    ``max_learning_rate`` every ``scheduler_steps`` epochs.
  scheduler_steps: Number of epochs governing scheduler behaviour. For the
    cosine scheduler this is the decay period. For the cyclic scheduler it is
    half of a full up/down cycle. Typical values range between ``10`` and ``100``
    depending on dataset size.
  scheduler_gamma: Multiplicative factor used by the exponential scheduler each
    epoch. Values below ``1.0`` decay the rate while values above ``1.0``
    increase it. ``0.95`` is a gentle decay while ``0.5`` halves the rate every
    epoch.
  momentum_coefficient: Coefficient used to blend the previous weight update
    with the current gradient. ``0.0`` disables momentum while values between
    ``0.0`` and ``1.0`` accelerate training by smoothing updates. Momentum is
    applied in ``apply_weight_updates_and_attention`` and is stored per-synapse
    so that frequently updated connections gain additional inertia.
  use_echo_modulation: When ``true`` weight updates are multiplied by each
    synapse's echo buffer average, enabling the synaptic echo learning
    mechanism. Disable to revert to standard updates.
  reinforcement_learning_enabled: Turn on Q-learning inside Neuronenblitz. When
    enabled the ``rl_`` methods may be used to select actions and update
    values directly through the network.
  rl_discount: Discount factor for future rewards used by ``rl_update``.
  rl_epsilon: Initial exploration probability for ``rl_select_action``.
  rl_epsilon_decay: Multiplicative decay applied to ``rl_epsilon`` each time an
    update occurs.
  rl_min_epsilon: Smallest allowed exploration rate once decay has taken place.
  shortcut_creation_threshold: Number of times the exact synapse path must be
    observed before a direct shortcut connection from the first to the last
    neuron is added. Set ``0`` to disable automatic shortcut formation.
  chaotic_gating_enabled: Enables logistic-map scaling of each weight update.
    When true the gate evolves chaotically and multiplies the computed update
    value, introducing non-linear training dynamics.
  chaotic_gating_param: Bifurcation parameter for the logistic gate update.
    Values between ``3.6`` and ``4.0`` produce strong chaotic behaviour.
  chaotic_gate_init: Starting value for the gate. Must lie between ``0`` and
    ``1`` and determines the initial update scale.
  context_history_size: Number of recent neuromodulatory contexts to keep
    in memory. Larger values allow long-term trends to influence wandering but
    consume more memory. Typical range is 5–20.
  context_embedding_decay: Multiplicative decay applied when computing the
    context embedding from history. ``1.0`` weights all contexts equally while
    lower values emphasise more recent entries.
  emergent_connection_prob: Probability between 0 and 1 that ``dynamic_wander``
    adds a random synapse connecting two neurons after each call. Higher values
    encourage spontaneous cross-links, increasing the chance of unexpected
    behaviours. Recommended range is 0.0–0.2.
  concept_association_threshold: Number of times a specific neuron pair must
    appear consecutively in a wander path before a new concept neuron is
    inserted to bridge them. Higher values create concepts more conservatively.
  concept_learning_rate: Initial weight assigned to each side of a newly
    created concept neuron. This determines how strongly the new concept
    influences signal flow right after insertion.
  weight_limit: Upper bound applied to every synapse weight. Any update that
    would exceed this magnitude is clipped to maintain numerical stability.
  wander_cache_size: Maximum number of recent ``dynamic_wander`` results kept
    in the cache. Older entries are discarded once this count is exceeded.
  rmsprop_beta: Exponential decay factor used by the RMSProp-like gradient
    smoothing in ``apply_weight_updates_and_attention``. Typical values are
    0.9–0.99.
  grad_epsilon: Small constant added when normalising gradients to avoid
    division by zero.
  use_experience_replay: Enable prioritized experience replay. When true
    Neuronenblitz keeps past training examples with their errors and samples
    them again according to those errors after each epoch.
  replay_buffer_size: Maximum number of experiences retained for replay. Old
    entries are discarded once this limit is exceeded.
  replay_alpha: Exponent determining how strongly error magnitude influences
    sampling probability. ``0`` gives uniform sampling while ``1.0`` weights
    fully by error.
  replay_beta: Importance sampling correction applied to replay updates. Values
    near ``1.0`` reduce bias from prioritisation.
  replay_batch_size: Number of stored experiences replayed after each epoch
    when experience replay is active.
  exploration_entropy_scale: Multiplier applied to the entropy of synapse visit
    counts when computing adaptive dropout rates.
  exploration_entropy_shift: Constant offset added after scaling the entropy.
  gradient_score_scale: Factor converting gradient magnitude into additional
    synapse potential during weight updates.
  memory_gate_decay: Multiplicative decay applied to memory-based gating scores
    that bias ``weighted_choice``. Lower values make memories fade quickly.
  memory_gate_strength: Amount added to a synapse's gating score whenever a
    path yields an error below ``episodic_memory_threshold``.
  episodic_memory_size: Number of recent successful paths stored for gating.
  episodic_memory_threshold: Maximum absolute error for a path to be recorded
    in episodic memory.
  episodic_memory_prob: Probability that wandering will be biased by episodic
    memory on a given call.
  curiosity_strength: Weight of novelty-driven curiosity in ``weighted_choice``.
    Higher values encourage exploring rarely visited synapses.
  depth_clip_scaling: Scaling factor controlling how strongly gradient clipping
    tightens for long wander paths.  ``1.0`` halves the update cap at maximum
    depth.
  forgetting_rate: Multiplicative decay applied to stored context history on
    each wander. Values below ``1.0`` gradually forget old neuromodulatory
    signals.
  structural_dropout_prob: Probability that a qualified synapse is skipped
    during structural plasticity. Increasing this value reduces the frequency
    of structural changes.
  gradient_path_score_scale: Multiplier applied to the cumulative gradient
    magnitude of a wander path when selecting the best result. Higher values
    bias exploration toward routes that recently produced strong weight
    updates.
  use_gradient_path_scoring: Set to ``true`` to enable gradient-based path
    scoring in ``dynamic_wander``. When disabled only the final neuron value
    determines which path is chosen.
  activity_gate_exponent: Exponent controlling how strongly synapse visit
    counts gate subsequent weight updates. ``1.0`` scales updates by
    ``1/(1+visits)`` while larger values produce steeper decay.
  subpath_cache_size: Maximum number of path prefixes stored for quick reuse.
    Larger caches speed up wandering on graphs with repeated motifs at the cost
    of additional memory.
  subpath_cache_ttl: Time in seconds before cached subpaths expire. ``0`` keeps
    entries indefinitely.
  use_mixed_precision: Enable PyTorch automatic mixed precision during
    training. This trades some numerical precision for faster execution on
    compatible GPUs.

brain:
  save_threshold: Minimum improvement in validation loss required before the
    brain writes a new checkpoint to disk.
  max_saved_models: Upper bound on the number of checkpoint files retained in
    ``save_dir``.
  save_dir: Directory where checkpoint files are stored.
  firing_interval_ms: Milliseconds between automatic firing cycles when
    auto-fire is active.
  initial_neurogenesis_factor: Starting multiplier that scales the amount of new
    neurons grown during neurogenesis.
  offload_enabled: When true, highly active lobes are sent to a remote brain
    server for processing.
  torrent_offload_enabled: When true, lobes may be distributed among torrent
    clients using the tracker system.
  mutation_rate: Fraction of synapses to mutate during each evolutionary step.
    Higher values introduce more variation but may destabilize learning.
  mutation_strength: Maximum absolute change applied to a mutated synapse's
    weight. Typical values range from 0.01 to 0.1.
  prune_threshold: Synapses with absolute weight below this value are removed
    during pruning, keeping the network efficient.
  dream_num_cycles: Default number of cycles executed each time the brain
    performs a background dreaming session. Values between 1 and 20 are common
    depending on how much consolidation is desired.
  dream_interval: Seconds to wait between consecutive dreaming sessions when
    ``start_dreaming`` runs in the background. Short intervals keep memory
    consolidation frequent but consume more processing time.
  neurogenesis_base_neurons: Base number of neurons grown during each
    neurogenesis event when no explicit value is provided to
    ``perform_neurogenesis``.
  neurogenesis_base_synapses: Base number of synapses introduced alongside new
    neurons during neurogenesis when defaults are used.
  max_training_epochs: Total number of epochs the training loop should run
    before stopping automatically.
  memory_cleanup_enabled: Toggles periodic removal of stale data from RAM and
    disk tiers.
  manual_seed: Random seed applied during initialization for reproducible
    experiments.
  log_interval: Number of batches between status log messages during training.
  evaluation_interval: Number of epochs between validation runs.
  early_stopping_patience: Number of sequential epochs allowed without a
    sufficient decrease in validation loss. Once this count is exceeded the
    training loop terminates early. Set ``0`` to disable patience-based checks.
    Values between ``3`` and ``10`` work well for most experiments.
  early_stopping_delta: Minimum drop in validation loss required to reset the
    patience counter. This prevents noise in the validation metric from
    triggering false improvements. Typical ranges are ``1e-4`` to ``1e-3``.
  auto_cluster_interval: Epoch interval between automatic clustering of neurons.
  cluster_method: Algorithm used for clustering; ``"kmeans"`` is provided but
    others may be added in future.
  auto_save_enabled: Enables periodic saves controlled by ``auto_save_interval``.
  offload_threshold: Minimum lobe attention required before ``offload_high_attention``
    sends a subcore to the remote brain. Increasing this reduces how frequently
    offloading occurs.
  torrent_offload_threshold: Attention threshold used when distributing lobes
    via the torrent client.
  cluster_high_threshold: Score above which clusters are relocated directly to
    the fast ``vram`` tier when ``relocate_clusters`` runs.
  cluster_medium_threshold: Score above which clusters are kept in ``ram`` but
    below the high threshold; lower scores are moved to disk tiers.
  dream_synapse_decay: Multiplicative decay applied to synapse weights during
    each dreaming cycle.
  dream_decay_arousal_scale: Scaling factor applied to ``dream_synapse_decay``
    proportional to current arousal. ``0.0`` disables arousal-based modulation
    while higher values increase decay when arousal rises.
  dream_decay_stress_scale: Scaling factor that reduces the effective decay
    as stress grows. Values around ``0.0`` keep stress from affecting dream
    strength, whereas ``1.0`` completely cancels decay at full stress.
  neurogenesis_increase_step: Amount added to the neurogenesis factor when
    validation loss worsens.
  neurogenesis_decrease_step: Amount subtracted from the neurogenesis factor
    when validation improves.
  max_neurogenesis_factor: Upper bound that ``neurogenesis_factor`` will not
    exceed, regardless of adjustments.
  cluster_k: Number of clusters created when ``cluster_neurons`` is invoked
    during training.
  auto_save_interval: Number of epochs between automatic calls to
    ``save_model``. ``0`` disables periodic saving.
  backup_enabled: When true a background thread periodically copies files from
    ``save_dir`` into ``backup_dir`` so that checkpoints and logs survive
    unexpected crashes.
  backup_interval: Seconds between each automatic backup when
    ``backup_enabled`` is active. Short intervals provide more robust safety but
    may increase disk usage.
  backup_dir: Directory where backup copies are stored. Each run creates a
    timestamped subfolder inside this directory.
  auto_firing_enabled: If true the brain starts an auto-firing thread
    immediately after construction.
  dream_enabled: Enables background dreaming when ``start_dreaming`` is called.
  vram_age_threshold: Age in seconds above which neurons in VRAM are considered
    old when deciding growth tiers.
  ram_age_threshold: Equivalent threshold for neurons in RAM.
  status_display_interval: If greater than zero, ``display_live_status`` is
    invoked every N epochs during training.
  neurogenesis_interval: Number of epochs between automatic neurogenesis events.
  min_cluster_size: Minimum number of neurons required to form a cluster.
  prune_frequency: Number of epochs between automatic pruning operations.
  auto_offload: When true, ``offload_high_attention`` runs automatically after
    each training epoch.
  benchmark_enabled: Enables evaluation through ``BenchmarkManager`` at the end
    of each epoch.
  benchmark_interval: Number of epochs between automatic benchmark comparisons
    of pure MARBLE and its autograd pathway. Higher values reduce overhead.
  loss_growth_threshold: Validation loss level that triggers expansion of the
    core during training. The default of ``0.1`` keeps growth infrequent.
  auto_neurogenesis_prob: Baseline probability between 0 and 1 that
    ``maybe_autonomous_neurogenesis`` creates new neurons each epoch. The
    probability scales with the current validation loss.
  dream_cycle_sleep: Seconds to wait between dream cycles. Increase to reduce
    CPU usage during prolonged dreaming.
  tier_decision_params:
    vram_usage_threshold: Fraction of VRAM usage that triggers migration of
      neurons to a lower tier.
    ram_usage_threshold: Fraction of RAM usage that triggers migration to disk
      or file tiers.
  model_name: Descriptive name used when saving checkpoints.
  checkpoint_format: Serialization format for checkpoints. ``"pickle"`` is
    the default but ``"safetensors"`` may be used for faster loading.
  metrics_history_size: Number of past epochs kept in memory for plotting
    performance metrics.
  profile_enabled: When true a ``UsageProfiler`` records CPU and GPU usage
    as well as epoch runtime during training.
  profile_log_path: CSV file path where profiler entries are appended.
  profile_interval: Number of epochs between profiler recordings. Values of
    1 log every epoch while higher values sample less frequently.
  early_stop_enabled: When ``true`` the trainer monitors the validation loss
    after each epoch and halts once the patience criteria are met. Set to
    ``false`` to force the loop to run ``max_training_epochs`` regardless of
    validation performance.
  lobe_sync_interval: Seconds between background synchronization of lobe data
    when remote syncing is active.
  cleanup_batch_size: Maximum number of objects removed in one cleanup pass.
  remote_sync_enabled: When ``true`` brain state is periodically pushed to a
    remote server.
  default_activation_function: Activation used when constructing new neurons.
  neuron_reservoir_size: Capacity of the internal neuron reservoir used when
    sampling replacements during pruning.
  lobe_decay_rate: Fraction by which lobe attention decays each epoch without
    activity.
  dimensional_search:
    enabled: Toggles automatic growth of the representation vector when
      validation loss improvements stagnate. ``true`` enables monitoring.
    max_size: Highest dimension the search may reach. Should exceed the initial
      ``representation_size`` but remain manageable for available hardware.
    improvement_threshold: Relative decrease in validation loss required to
      deem a new dimension successful. Values around ``0.01`` to ``0.05`` are
      typical.
    plateau_epochs: Number of consecutive epochs below the improvement
      threshold before another dimension is added.
  super_evolution_mode: When true, a controller records loss, speed, complexity
    and resource metrics each epoch and adjusts all configurable parameters via
    self-attention. This prioritizes minimising loss, then maximising speed,
    reducing complexity and finally conserving resources.

formula: Mathematical expression used to compute initial neuron values instead
  of the Mandelbrot seed. If omitted the Mandelbrot algorithm is used.
formula_num_neurons: Number of neurons created when ``formula`` is provided.

meta_controller:
  # The meta-controller monitors validation losses and adjusts
  # ``neuronenblitz.plasticity_threshold`` accordingly. If losses rise the
  # threshold is lowered so structural changes become less likely. When losses
  # fall it increases the threshold within the allowed range to encourage more
  # exploration. This keeps learning stable over long runs.
  history_length: Number of recent validation losses used for the adjustment.
    Typical values range from 3 to 20 depending on how quickly adaptation should
    react.
  adjustment: Amount by which the plasticity threshold changes when performance
    improves or degrades. Values between 0.1 and 1.0 are common.
  min_threshold: Lower bound for the plasticity threshold. Should remain above
    zero, often around 0.5–5.0.
  max_threshold: Upper bound for the plasticity threshold. Values around
    10–50 keep the system from becoming too rigid.

neuromodulatory_system:
  # Represents the internal state of the brain. Arousal and reward boost
  # neurogenesis while stress suppresses it. Emotion provides a qualitative tag
  # that may be used by higher-level modules. Values typically lie between 0.0
  # and 1.0 and change dynamically during training.
  initial:
    arousal: Baseline arousal level in the range 0.0–1.0. Higher arousal
      increases neurogenesis rates.
    stress: Initial stress level in the range 0.0–1.0. Stress reduces
      plasticity and slows learning.
    reward: Starting reward signal in the range 0.0–1.0 that encourages
      reinforcement of successful pathways.
    emotion: Starting emotional state for the system. ``"neutral"`` is typical
      but any descriptive string is allowed.

memory_system:
  # Provides short-term memory in RAM and long-term memory on disk. The system
  # consolidates entries from the volatile store into the persistent file when
  # required.
  long_term_path: Path to the file used for persistent long-term storage of
    memories.
  threshold: When ``arousal`` or ``reward`` exceed this value the
    ``MemorySystem`` stores new entries directly in the long-term layer instead
    of the short-term layer. Typical values range from 0.3 to 0.7 depending on
    how aggressively long-term consolidation should occur.
  consolidation_interval: Number of operations between calls to
    ``consolidate``. Larger values delay writes to disk but reduce I/O.

hybrid_memory:
  # Optional long-range memory combining vector search with symbolic storage.
  vector_store_path: File path for storing dense embeddings of each memory
    entry. If omitted defaults to ``vector_store.pkl`` in the working
    directory.
  symbolic_store_path: File used to persist key/value pairs and timestamps.
    This mirrors the vector store so recalls can return the original data.
  max_entries: Maximum number of items retained in the vector and symbolic
    stores. When the count exceeds this limit the oldest entries are removed
    to keep memory usage bounded.

remote_client:
  # Configuration for forwarding parts of the brain to a remote server. When
  # ``offload_enabled`` is true the brain serializes a subcore and sends it to
  # this endpoint for processing.
  url: Base URL for the remote HTTP brain server.
  timeout: Seconds to wait for remote requests before they are aborted. Values
    between 5 and 30 are typical depending on network latency.
  max_retries: Number of times a remote operation is retried upon failure.
  track_latency: When true the client records the time taken for each
    request so average latency can be inspected through the API.
  auth_token: Optional token sent as an ``Authorization`` header on each
    request.
  ssl_verify: Whether HTTPS certificates should be verified. Set to ``false``
    when using self-signed certificates.
  connect_retry_interval: Seconds to wait between connection retry attempts.
  heartbeat_timeout: Maximum time to wait for a heartbeat response when
    maintaining persistent connections.
  use_compression: If ``true`` payloads are compressed before transmission.

torrent_client:
  # Enables distribution of brain parts through the tracker network. Each client
  # processes assigned subcores asynchronously.
  client_id: Identifier used when registering with the torrent tracker. Must be
    unique across participating clients.
  buffer_size: Maximum number of asynchronous tasks queued for a torrent
    client. Larger values allow more parallel requests but use more memory.
  heartbeat_interval: Seconds between status pings sent to the tracker. Has no
    effect in the current simplified implementation but reserved for future use.

data_compressor:
  # Controls how strongly the DataCompressor reduces data size before it is
  # stored or transmitted.
  compression_level: Integer from 0 to 9 passed directly to ``zlib.compress``.
    Higher levels yield smaller output but require more CPU time. The default of
    6 balances speed and ratio for general use.
  compression_enabled: Set to ``false`` to bypass compression entirely. Useful
    during debugging or when working with already compressed data.
  delta_encoding: When ``true`` arrays are stored as differences between
    consecutive elements. This often yields dramatically higher compression
    ratios for smooth numeric data but slightly increases the cost of
    decompression. The first element is stored verbatim so the process remains
    lossless.

dataloader:
  # Settings for the ``DataLoader`` which serializes and compresses data.
  # ``tensor_dtype`` defines the NumPy dtype used when encoding objects and
  # arrays. This affects the precision of the returned tensors and must be a
  # valid dtype name like ``"uint8"`` or ``"int16"``. Wider dtypes can prevent
  # overflow with exceptionally large payloads but increase memory usage.
  tensor_dtype: String specifying the dtype of encoded tensors.

experiment_tracker:
  # Optional integration with external experiment tracking services such as
  # Weights & Biases. When ``enabled`` is true a ``WandbTracker`` is created
  # and attached to the :class:`MetricsVisualizer` so that training metrics are
  # streamed to the configured project.
  enabled: Activate experiment tracking when set to ``true``.
  project: Name of the remote project used by the tracker.
  entity: Optional account or team under which the run is recorded.
  run_name: Custom name assigned to the run. If ``null`` Wandb generates one.

remote_server:
  # Launches an optional local ``RemoteBrainServer`` so this MARBLE instance can
  # forward computation to another machine. When ``enabled`` is ``true`` the
  # server starts automatically using the provided ``host`` and ``port``. If
  # ``remote_url`` is specified the server itself forwards heavy requests to that
  # address, creating a chain of offload targets.
  enabled: Whether to start the server alongside the main process.
  host: Interface address to bind the HTTP server to. Usually ``"localhost"``
    for local testing.
  port: TCP port used by the server.
  remote_url: Optional URL of another remote brain server to which this server
    offloads work.
  ssl_enabled: Enables HTTPS with the provided certificate and key files.
  ssl_cert_file: Path to the SSL certificate used when ``ssl_enabled`` is true.
  ssl_key_file: Path to the private key for the certificate.
  max_connections: Maximum concurrent connections the server will accept.
  compression_level: Compression strength used when exchanging data with
    clients. The value is passed to ``zlib.compress`` and ranges from 0
    (no compression) to 9 (maximum compression).
  compression_enabled: Set to ``false`` to disable compression entirely. When
    disabled the server expects plain JSON payloads.

metrics_visualizer:
  # Configure the live metrics plot size. These values are passed directly to
  # ``matplotlib`` when creating the figure.
  fig_width: Width of the metrics figure in inches.
  fig_height: Height of the metrics figure in inches.
  refresh_rate: How often the plot is refreshed in seconds.
  color_scheme: Name of the Matplotlib style to apply when rendering metrics.
  show_neuron_ids: If true the plot displays neuron identifiers next to data
    points.
  dpi: Resolution of the output figure in dots per inch.
  track_memory_usage: When true the visualizer records both system RAM and GPU
    memory consumption after each training epoch. This requires the ``psutil``
    package and uses ``torch.cuda`` when a CUDA-capable device is available.
  track_cpu_usage: When enabled the metrics visualizer stores the current CPU
    utilisation percentage at each update. This calls ``psutil.cpu_percent`` and
    allows post-training analysis of processing load.
  log_dir: Directory where TensorBoard event files are written. When set,
    you can run ``tensorboard --logdir=<log_dir>`` to monitor metrics.
  csv_log_path: Path to a CSV file receiving all metric values after each
    update. Useful for analysis with spreadsheet tools.
  json_log_path: Path to a JSON lines file where each update is appended as a
    single JSON object. This provides structured logs for downstream tools.

metrics_dashboard:
  # Optional web dashboard built with Plotly Dash for real-time monitoring.
  # When enabled a small HTTP server serves a page that visualizes the same
  # metrics collected by ``MetricsVisualizer`` but updates automatically in the
  # browser. This is useful when running training remotely and you still want
  # to monitor progress live.
  enabled: Set to ``true`` to start the dashboard in a background thread.
  host: Interface address for the Dash server, usually ``"localhost"``.
  port: TCP port used by the dashboard. Ensure this port is free.
  update_interval: Milliseconds between refreshes of the dashboard graphs.
  window_size: Number of recent points used to compute the moving average shown
    on each graph. Larger windows smooth the curves more aggressively.

lobe_manager:
  # Parameters controlling how strongly neuron attention is adjusted when the
  # ``LobeManager`` performs self-attention.
  attention_increase_factor: Multiplier applied to neuron attention when a
    lobe's score is below average and the loss is positive. Values slightly above
    ``1.0`` encourage struggling lobes to contribute more.
  attention_decrease_factor: Multiplier used when a lobe's attention is above
    average but the loss is not improving. Numbers below ``1.0`` gradually reduce
    emphasis on those lobes.

autograd:
  enabled: Set to true to wrap the Brain with a transparent PyTorch autograd layer. When enabled, gradients from PyTorch operations are applied to MARBLE synapse weights without altering the underlying architecture.
  learning_rate: Step size used when the autograd layer applies gradient updates to synapse weights during backward passes. Typical values range from 0.001 to 0.1.
pytorch_challenge:
  enabled: If true the training loop compares MARBLE with a pretrained PyTorch model after every training example. When MARBLE's validation loss, inference speed or model size exceed the PyTorch model, neuromodulatory stress is increased which lowers plasticity on subsequent updates.
  loss_penalty: Amount of stress added when MARBLE's loss is worse than the PyTorch baseline. Values around 0.1 provide noticeable pressure without overwhelming the system.
  speed_penalty: Stress increment applied when MARBLE's inference time is slower than the baseline model. Setting this near 0.1 encourages optimisations to execution paths.
  size_penalty: Stress increment when MARBLE grows larger than the baseline model in megabytes. This guides synaptic pruning and structural plasticity to favour compactness.

gpt:
  # The GPT component enables MARBLE to build and train a transformer-based language model entirely from scratch. When
  # `enabled` is true a small GPT architecture is initialised using the parameters below and trained on token sequences
  # provided by the user. This allows experimentation with generative text models without relying on any pretrained weights.
  enabled: Set to true to activate GPT training within MARBLE.
  vocab_size: Size of the discrete token vocabulary used when constructing the embedding layers. Must be greater than one.
  block_size: Number of tokens processed at once. Sequences longer than this are split into blocks. Typical values range
    from 8 to 128 for lightweight experiments.
  num_layers: Number of transformer encoder layers stacked in the GPT. Increasing this improves modelling power but also
    memory and compute requirements. Values between 2 and 8 are common for small models.
  num_heads: Quantity of attention heads per layer. More heads allow the model to capture diverse relationships between
    tokens. Small configurations often use 2 to 8 heads.
  hidden_dim: Dimensionality of the token and positional embeddings as well as the hidden size of each transformer layer.
    Higher values provide more capacity but also increase memory usage.
  learning_rate: Step size for the Adam optimiser when training the GPT model. Start with around 0.001 and adjust based
    on observed convergence.
  num_train_steps: Number of update steps performed during GPT training. Keeping this low (tens to hundreds) ensures test
    runs complete quickly while still demonstrating learning behaviour.
  dataset_path: Path to a plain text file used to generate training sequences. Characters are read from the file and
    mapped to token IDs until ``vocab_size`` unique tokens are collected. The file must contain at least ``block_size``
    characters so that each training example can include a start and target token. Using larger files improves vocabulary
    coverage and allows more diverse sequences.
  batch_size: Number of sequences processed before each optimisation step. Batches between 1 and 64 are typical. Larger
    values yield smoother gradient estimates but require more memory, especially when running on GPU with CuPy.

distillation:
  # Knowledge distillation allows training a "student" brain using guidance from
  # a pre-trained "teacher" brain. When enabled the student blends the teacher's
  # predictions with the ground truth targets before performing regular training
  # updates. This is useful for compressing a large model into a smaller one or
  # for transferring knowledge between different MARBLE instances.
  enabled: Set to ``true`` to activate distillation during training.
  alpha: Weighting factor between the true target and the teacher's output when
    computing the blended target value. ``0.0`` relies solely on the ground
    truth while ``1.0`` exactly matches the teacher. Typical values range from
    ``0.3`` to ``0.7`` depending on how strongly the teacher should influence
    learning.
  teacher_model: Path to a pickled MARBLE system used as the teacher. If ``null``
    the caller must provide the teacher programmatically when invoking the
    distillation trainer.

reinforcement_learning:
  enabled: Activate training in a reinforcement learning loop using the MARBLE
    core and Neuronenblitz. Set to ``true`` to begin episodic interaction with
    the environment defined in ``reinforcement_learning.py``.
  episodes: Number of training episodes to run. More episodes generally yield
    better policies but increase computation time.
  max_steps: Maximum steps per episode before resetting the environment.
  discount_factor: Discount applied to future rewards when computing Q-values.
  epsilon_start: Initial exploration rate for the epsilon-greedy policy.
  epsilon_decay: Multiplicative decay applied to ``epsilon`` after each update.
  epsilon_min: Lowest exploration rate allowed once ``epsilon`` has decayed.
  seed: Integer random seed for reproducible training. Set to ``null`` to allow
    nondeterministic runs.
  double_q: Enable Double Q-learning which maintains two separate Q-tables.
    During updates one table selects the greedy action while the other
    evaluates it. This reduces overestimation bias at the cost of extra memory.

contrastive_learning:
  # Self-supervised contrastive representation learning using the
  # ``ContrastiveLearner``. Two augmented views of each sample are
  # processed by Neuronenblitz and the InfoNCE objective adjusts
  # synaptic weights via ``apply_weight_updates_and_attention``.
  enabled: Set to ``true`` to run contrastive learning after standard
    training. When enabled the ``contrastive_train`` method of
    Neuronenblitz processes the dataset according to the parameters
    below.
  temperature: Softmax temperature applied to similarity scores when
    computing the InfoNCE loss. Lower values sharpen the distribution.
    Typical values range from ``0.1`` to ``1.0``.
  epochs: Number of passes over the dataset for contrastive learning.
    Each epoch iterates over the inputs in mini-batches of size
    ``batch_size`` and updates weights after every batch.
  batch_size: Number of original samples to include in each batch. Each
    sample produces two augmented views, so the effective batch size for
    the loss is ``2 * batch_size``. Values between ``2`` and ``32`` work
    well depending on available memory.

hebbian_learning:
  # Unsupervised Hebbian update rule integrated with Neuronenblitz.
  # After each wander the weights along the traversed path are
  # adjusted based on the product of pre- and post-synaptic values.
  # ``learning_rate`` scales the Hebbian update and ``weight_decay``
  # applies an L2 style penalty to prevent unbounded growth.
  learning_rate: Positive step size controlling how strongly synapse
    weights are increased when correlated neuron activations are
    observed. Typical values range from ``0.001`` to ``0.1``.
  weight_decay: Fraction of the current weight subtracted during each
    update. Helps stabilise learning when set between ``0.0`` and
    ``0.01``.

adversarial_learning:
  # Generative adversarial training using two Neuronenblitz networks that
  # share the same Core. The generator receives random noise and tries
  # to produce values that the discriminator classifies as real.
  enabled: Set to ``true`` to run adversarial updates after the standard
    training loop. Both generator and discriminator are updated using
    ``apply_weight_updates_and_attention``.
  epochs: Number of passes over the provided real values. Values between
    ``1`` and ``5`` work well for small experiments.
  batch_size: Number of real samples processed before generator and
    discriminator weights are updated. Typical range is ``4`` to ``64``.
  noise_dim: Dimensionality of the random noise vector fed to the generator.
    Higher values allow more variation but increase compute. Values of
    ``1`` to ``10`` are common.

autoencoder_learning:
  # Denoising autoencoder that trains Neuronenblitz to reconstruct noisy
  # inputs. After each wander the reconstruction error is applied through
  # ``apply_weight_updates_and_attention``.
  enabled: Activate training for the autoencoder module.
  epochs: Number of times the dataset is presented. Typical values are
    ``1`` to ``10`` depending on dataset size.
  batch_size: Number of samples processed before weight updates. Values of
    ``4`` to ``64`` work well in practice.
  noise_std: Standard deviation of Gaussian noise added to inputs. A range
    from ``0.0`` (no noise) to ``0.5`` encourages robust representations.
  noise_decay: Multiplicative factor applied to ``noise_std`` after each epoch.
    Values below ``1.0`` gradually reduce corruption as training progresses,
    allowing the model to focus on fine-grained reconstruction.

semi_supervised_learning:
  # Semi-supervised learning using consistency regularization. Each step
  # processes a labeled example and an unlabeled input. The network first
  # performs a standard supervised update with the labeled pair. It then
  # processes the unlabeled input twice with independent wander paths and
  # adjusts weights to minimise the difference between the two outputs.
  enabled: Set to ``true`` to enable semi-supervised updates during training.
  epochs: Number of passes over the combined labeled and unlabeled datasets.
  batch_size: Count of labeled/unlabeled pairs processed before weight updates.
  unlabeled_weight: Scaling factor applied to the consistency loss derived from
    the two unlabeled outputs. Values between ``0.1`` and ``1.0`` control how
    strongly the unlabeled data influences learning.

federated_learning:
  # Federated averaging trainer that coordinates multiple clients.
  # Each round trains all local Neuronenblitz networks on their
  # respective datasets before averaging synapse weights.
  enabled: Enable federated training when set to ``true``.
  rounds: Number of communication rounds performed.
  local_epochs: Epochs of local training executed on each client per round.

curriculum_learning:
  # Progressive training strategy that orders examples by difficulty.
  # The learner introduces harder examples as training progresses to
  # stabilise optimisation and accelerate convergence.
  enabled: Enable curriculum learning when ``true``.
  epochs: Number of passes over the dataset with increasing difficulty.
  schedule: Determines how quickly more difficult samples are added. Use
    ``linear`` for a steady increase or ``exponential`` to focus on easy
    examples longer.
meta_learning:
  # Reptile-style meta learning for rapid adaptation across tasks.
  # Each task contains input-target pairs. The learner clones the
  # current Core and Neuronenblitz, performs ``inner_steps`` of
  # training on each task and then moves the main network weights
  # toward the average of the task-specific weights using ``meta_lr``.
  enabled: Set to ``true`` to activate meta learning during training.
  epochs: Number of meta-training iterations over the provided tasks.
  inner_steps: Training steps executed inside each temporary clone
    before the meta update is applied. Typically between 1 and 5.
  meta_lr: Step size used when blending task weights back into the
    primary network. Values around ``0.1`` work well for most setups.
transfer_learning:
  # Fine-tune a pretrained model while keeping a portion of synapses fixed.
  # ``freeze_fraction`` controls what percentage of synapses are marked as
  # frozen before training begins. Frozen synapses do not receive weight
  # updates in ``apply_weight_updates_and_attention``.
  enabled: Set to ``true`` to run transfer learning updates.
  epochs: Number of fine-tuning passes over the new dataset.
  batch_size: Mini-batch size used during fine-tuning steps.
  freeze_fraction: Fraction between ``0.0`` and ``1.0`` specifying the portion
    of synapses that remain unchanged.

continual_learning:
  # Replay-based continual learning that stores a limited set of
  # previous examples and replays them during training on new data.
  # This mitigates catastrophic forgetting when tasks arrive
  # sequentially without requiring complex weight consolidation.
  enabled: Set to ``true`` to activate continual learning.
  epochs: Number of passes over each incoming dataset. Typically
    ``1`` or ``2`` is sufficient when replay memory is used.
  memory_size: Maximum number of examples retained for replay. Larger
    values offer better retention of old tasks but increase memory
    usage. Reasonable ranges are between ``10`` and ``100``.

imitation_learning:
  # Behaviour cloning of expert demonstrations using the ``ImitationLearner``.
  # Each demonstration contains an input value and the corresponding action
  # taken by the expert. The learner replays these demonstrations and
  # adjusts synaptic weights so that Neuronenblitz reproduces the expert's
  # behaviour for the same inputs.
  enabled: Enable imitation learning updates. Set to ``true`` to process
    recorded demonstrations after normal training.
  epochs: Number of passes over the stored demonstrations. Values between
    ``1`` and ``10`` work well for small datasets.
  max_history: Maximum number of demonstration pairs kept in memory. Older
    entries are removed once this limit is exceeded.


harmonic_resonance_learning:
  # Experimental paradigm that embeds inputs as sinusoidal phases.
  # Each step encodes the current value using ``base_frequency`` and updates
  # ``base_frequency`` by multiplying it with ``decay`` after training.
  enabled: Enable harmonic resonance learning when set to ``true``.
  epochs: Number of passes over the dataset. Usually ``1`` or ``2`` is enough
    for quick experiments.
  base_frequency: Initial frequency used when computing sine and cosine
    representations. Values around ``1.0`` keep phase changes gradual while
    larger numbers increase oscillation speed.
  decay: Multiplicative factor applied to ``base_frequency`` after each
    training step. ``0.99`` slowly lowers the frequency over time.

synaptic_echo_learning:
  # Uses the echo buffers stored on each synapse to modulate updates.
  # Weight changes are scaled by the average echoed activity.
  enabled: Activate the synaptic echo learner when ``true``.
  epochs: Number of training epochs to run. Usually ``1`` or ``2`` is
    sufficient for simple experiments.
  echo_influence: Multiplier applied to the error signal before updating
    weights. Larger values amplify the effect of echoed activations.

fractal_dimension_learning:
  # Adjusts the representation size by estimating the fractal dimension of
  # neuron embeddings. When the dimension exceeds ``target_dimension`` a new
  # element is appended to each representation via ``Core.increase_representation_size``.
  enabled: Enable fractal dimension learning when ``true``.
  epochs: Number of passes over the dataset used to refine the estimate.
  target_dimension: Threshold for the estimated dimension. Values between
    2.0 and 6.0 work well for small graphs.

quantum_flux_learning:
  # Experimental rule associating a phase with every synapse. Weight updates
  # are scaled by ``sin(phase)`` and the phase drifts by ``phase_rate``.
  enabled: Activate quantum flux learning when set to ``true``.
  epochs: Number of epochs using phase-modulated updates.
  phase_rate: Amount added to each synapse's phase after each update.

dream_reinforcement_learning:
  # Combines regular reinforcement updates with short dream rollouts. After
  # each real step ``dream_cycles`` additional wanders run using the last
  # output as input and apply errors scaled by ``dream_strength``.
  enabled: Enable dream reinforcement learning when ``true``.
  episodes: Number of training episodes consisting of input-target pairs.
  dream_cycles: How many dream updates follow each real step.
  dream_strength: Multiplier applied to dream errors before weight updates.

continuous_weight_field_learning:
  # Optimises a weight vector ``W(x)`` that varies smoothly with the input ``x``.
  # The weight field is represented with radial basis functions whose centres
  # span the input space. For each training sample MARBLE uses Neuronenblitz to
  # produce a feature representation ``phi(x)`` from the core, then predicts
  # ``phi(x) \cdot W(x)``. Weight field coefficients are updated via gradient
  # descent with optional L2 regularisation.
  enabled: Enable continuous weight field learning when set to ``true``.
  epochs: Number of passes over the dataset to fit the field.
  num_basis: Number of radial basis functions forming the field.
  bandwidth: Standard deviation controlling basis width. Larger values make
    the field smoother across inputs.
  reg_lambda: Strength of L2 regularisation on the basis coefficients.
  learning_rate: Step size for gradient descent updates of the field weights.

neural_schema_induction:
  # Structural learner that grows new "schema" neurons representing frequent
  # reasoning patterns discovered during dynamic wanders. When a subsequence
  # of neuron activations appears in at least ``support_threshold`` examples,
  # the learner adds a new neuron connected to all participants in that
  # sequence.
  enabled: Enable neural schema induction when set to ``true``.
  epochs: Number of passes over the dataset used for pattern collection.
  support_threshold: Minimum number of occurrences before creating a schema.
  max_schema_size: Maximum length of activation patterns considered.

conceptual_integration:
  # Creates new "concept" neurons by blending the representations of
  # two dissimilar active neurons. This structural learner relies on
  # Neuronenblitz activation traces and does not use any gradient optimisation.
  enabled: When ``true`` the learner periodically attempts concept blending.
  blend_probability: Chance of triggering a blend at each training step.
  similarity_threshold: Maximum cosine similarity allowed between parent
    neurons for blending. Lower values encourage more diverse concepts.

n_dimensional_topology:
  # Dynamically expands the representation size when learning stalls.
  # A self-attention score compared with ``attention_threshold`` decides whether
  # to add a new dimension. If the validation loss fails to improve by
  # ``loss_improve_threshold`` within ``stagnation_epochs`` epochs, the extra
  # dimension is removed and all neuron representations are truncated.
  enabled: Turn on n-dimensional growth when ``true``.
  target_dimensions: Upper bound on representation size.
  attention_threshold: Minimum self-attention required before trying to grow.
  loss_improve_threshold: Relative loss decrease needed to keep the new dimension.
  stagnation_epochs: Number of stagnant epochs before another dimension may be added.

unified_learning:
  # The UnifiedLearner coordinates all paradigms using a gating network.
  # At each step it constructs a context vector summarising memory usage,
  # plasticity threshold, number of neurons and recent losses. A small neural
  # network turns this context into softmax weights over the learners. The
  # weights modulate ``neuronenblitz.plasticity_modulation`` so each paradigm's
  # updates are scaled appropriately. When ``log_path`` is set every decision is
  # appended to that JSONL file for later inspection.
  enabled: When ``true`` the meta-controller runs during training.
  gating_hidden: Number of hidden units in the gating network. Typical values
    range from 4--32 depending on how many context features are used.
  log_path: File path for the decision log. Set to ``null`` to disable logging.
