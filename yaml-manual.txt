The configuration YAML file controls all components of MARBLE.  Each top-level
section corresponds to a subsystem and exposes parameters that can be tuned to
alter its behaviour.  When the file is loaded it is validated against an
internal JSON schema to catch missing sections or invalid values early.
A sample configuration with inline comments can be generated using the
`config_generator.py` script. Run `python config_generator.py` to create `sample_config_with_comments.yaml` which mirrors `config.yaml` but includes descriptions for each parameter.

Any configuration file may specify only a subset of parameters. Missing values
are automatically filled with the defaults from `config.yaml`, allowing small
YAML snippets to override just the options you need.

dataset:
  source: Path or URL of the primary training dataset. Accepts local filesystem
    paths as well as ``http://`` or ``https://`` URLs. When ``use_kuzu_graph`` is
    ``true`` this field is ignored. Set to ``null`` when supplying data
    programmatically.
  num_shards: Number of shards the training dataset is split into. When greater
    than ``1`` each training worker should set ``shard_index`` to a unique value
    between ``0`` and ``num_shards - 1`` so that only a subset of samples is
    processed. This enables distributed training without duplicating effort.
  offline: When ``true`` no network access is attempted. Remote datasets must
    already exist in ``cache_dir`` or a ``FileNotFoundError`` is raised. Use
    this to run experiments on machines without internet connectivity.
  encryption_key: Optional string used to encrypt serialized dataset objects.
    When provided all objects are encrypted before being written to disk and
    automatically decrypted when loaded. This should be the same key on every
    machine processing the dataset.
  cache_url: Base URL of a running ``DatasetCacheServer``. When set, files are
    fetched from the cache first and only downloaded from the original source if
    missing. This keeps datasets synchronised across multiple machines.
  use_kuzu_graph: Toggle that switches the training input to a Kùzu graph rather
    than a flat dataset. When ``true`` the ``kuzu_graph`` subsection must be
    provided and the ``source`` field above is ignored. This allows Marble to
    train directly from graph-structured data stored in a Kùzu database without
    intermediate export steps.
  version_registry: Directory or HTTP URL containing dataset version ``*.json``
    diff files produced by :mod:`dataset_versioning`. When set together with
    ``version`` the loader patches the base dataset using the specified diff
    enabling reproducible experiments across machines.
  version: Identifier of the dataset diff to apply from ``version_registry``.
    Versions are UUID strings generated by ``dataset_version_cli create``. Set
    to ``null`` to use the base dataset without modifications.
  kuzu_graph:
    db_path: Filesystem location of the Kùzu database storing the training
      graph. The database is opened in read-only mode so it must already exist
      and contain the relevant nodes or relationships.
    query: Cypher query returning training samples. The query should yield at
      least two columns representing input and target values. For example:
      ``MATCH (s:Sample) RETURN s.input AS input, s.label AS target``.
    input_column: Name of the column in the query result used as the model
      input. Defaults to ``"input"`` if omitted.
    target_column: Name of the column providing the expected output for each
      sample. Defaults to ``"target"``.
    limit: Optional integer restricting how many rows are read from the graph.
      ``null`` reads the entire result set. Use this to cap training size during
      experiments or when only a subset is needed.
  hf_token: Path to a file containing your Hugging Face API token. When the
    file exists Marble automatically logs in before downloading datasets or
    models. The default location is ``~/.cache/marble/hf_token``.
logging:
  structured: When true, all logs are emitted as JSON objects via ``JSONFormatter``. This format is machine friendly and can be ingested by tools such as Logstash or Splunk. When ``false`` the classic text format is used.
  log_file: Filesystem path where log records are written. If set to ``null`` the logs go to stderr. When ``rotate`` is enabled this path becomes the base name for rotated files.
  level: Minimum severity level recorded. Accepts either the standard string names (``"DEBUG"``, ``"INFO"``, ``"WARNING"``, ``"ERROR"``, ``"CRITICAL"``) or their numeric equivalents. Lower levels produce more verbose output.
  format: Standard logging format string applied when ``structured`` is ``false``. The default ``"%(levelname)s:%(name)s:%(message)s"`` shows the severity, logger name and message.
  datefmt: ``strftime`` pattern controlling timestamp appearance, e.g. ``"%Y-%m-%d %H:%M:%S"`` for ``2025-03-30 14:53:21``.
  propagate: When ``true`` the root logger forwards records to ancestor loggers allowing integration with external logging setups.
  rotate: Enables a size-based rotating file handler. Once ``max_bytes`` is exceeded the current log file is renamed with a numeric suffix and a new file is started.
  max_bytes: Maximum size in bytes a log file may reach before rotation occurs. Values between ``1_000_000`` and ``100_000_000`` are typical.
  backup_count: Number of rotated log files preserved on disk. Older files are removed once this count is exceeded. Recommended range is ``1``–``10``.
  encoding: Text encoding used for file handlers, e.g. ``"utf-8"`` or ``"utf-16"``.

plugins:
  - List of directories containing plugin modules. Each module implements
    ``register(register_neuron, register_synapse)`` to add custom neuron and
    synapse types. Paths may be absolute or relative to the working directory.

scheduler:
  plugin: Selects the backend used for scheduling asynchronous helper tasks.
    ``"thread"`` utilises a thread pool and works well for CPU-bound work. The
    ``"asyncio"`` option runs an event loop in a dedicated thread allowing
    coroutines and non-blocking operations to overlap with GPU kernels. Both
    options seamlessly handle CPU or CUDA tasks depending on the operations
    performed by the scheduled function. The scheduler can also be overridden at
    runtime using the ``--scheduler-plugin`` CLI flag.

live_kuzu:
  enabled: When set to ``true`` MARBLE streams both its evolving network
    topology and all logged training metrics into a persistent Kùzu database.
    This allows real-time inspection with tools like Kùzu Explorer while
    training continues on the GPU. Disable to avoid the extra disk I/O.
  db_path: Filesystem location for the Kùzu database. A new database is created
    if the path does not yet exist. The database stores neuron and synapse
    nodes alongside ``Metric`` and ``Event`` records, so ensure the directory
    has enough free space for long training runs.

pipeline:
  async_enabled: When set to ``true`` the ``HighLevelPipeline`` executes its
    steps using ``asyncio`` to overlap data loading and computation. This
    reduces idle time for both CPU and GPU workloads, enabling more efficient
    throughput. Some steps may still run synchronously if they are not
    awaitable. Defaults to ``false``.
  cache_dir: Directory on disk where the ``HighLevelPipeline`` stores cached
    results for each step. Reusing cached outputs speeds up iterative
    experiments at the cost of additional storage. When ``null`` the directory
    is chosen automatically: ``pipeline_cache_gpu`` when a CUDA device is
    available or ``pipeline_cache_cpu`` otherwise. Use ``HighLevelPipeline.clear_cache()``
    to remove cached files when space is needed.
  default_step_memory_limit_mb: Global cap on the additional memory each step
    may allocate, measured in megabytes. The pipeline records CPU and GPU
    allocations for every step and raises a ``MemoryError`` when the usage
    exceeds this limit. Set to ``null`` to disable enforcement. Individual
    steps can override the global quota by providing a ``memory_limit_mb``
    field in their step definition.
  macro: A step may include a ``macro`` field containing a list of sub-step
    dictionaries. These sub-steps execute sequentially and return a list of
    results, allowing complex workflows to be bundled into a single step within
    the YAML description. Macros honour dependencies, hooks and caching on both
    CPU and GPU devices.
  tier: Each step may specify a ``tier`` referencing a registered remote
    hardware backend. When provided the step runs on that tier instead of the
    local CPU or GPU, enabling seamless offload to specialised accelerators.
  depends_on: Optional list of step names that must finish before the current
    step executes. The pipeline constructs a dependency graph and reorders steps
    automatically using a topological sort. Referencing an unknown step or
    creating a cycle triggers a clear error. This mechanism behaves the same on
    CPUs and GPUs.
  isolated: When set to ``true`` the step is executed in its own Python process
    to provide fault tolerance. The child process selects the appropriate CPU or
    GPU device and returns the result to the parent pipeline. Use this for
    experimental or unstable operations to prevent crashes from affecting the
    main run.
  memory_limit_mb: Optional field specifying the maximum additional memory in
    megabytes that the step is permitted to allocate. It overrides
    ``default_step_memory_limit_mb`` and causes a ``MemoryError`` when the
    allocation is exceeded.

preprocessing:
  workers: Number of remote preprocessing workers spawned via
    :class:`remote_worker_pool.RemoteWorkerPool`. Each worker runs in a separate
    process and receives transformation jobs over an internal RPC channel. This
    parallelises CPU-heavy dataset preparation on both CPU-only and GPU-enabled
    systems. Setting the value to ``0`` disables the worker pool and executes
    all preprocessing steps in the calling process. Workers automatically
    restart if they crash while processing an item ensuring robustness during
    large scale data ingestion.

tool_manager:
  enabled: Toggle that activates the ``ToolManagerPlugin``. When ``true`` the
    pipeline can autonomously route natural language queries to external tools
    such as web search APIs or graph databases. Disabled by default to ensure
    reproducible experiments. Works on both CPU and GPU devices although most
    tools run on CPU.
  policy: Strategy used to choose which tool receives a query. The default
    ``"heuristic"`` policy inspects the query for keywords and selects the first
    tool whose :meth:`can_handle` method returns ``True``. Other policies may be
    added in the future and should be referenced here by name. Invalid values
    raise a configuration error.
  mode: Controls how the manager accepts queries. ``"direct"`` executes a
    single query supplied to :meth:`execute`. ``"mcp"`` enables integration with
    the :class:`MessageBus`, listening for incoming tool requests from
    components such as :class:`MCPToolBridge` and replying with tool outputs.
    Use ``"mcp"`` when exposing MARBLE tools to external MCP clients.
  agent_id: Identifier used on the :class:`MessageBus` when ``mode`` is
    ``"mcp"``. Multiple managers can run concurrently provided each uses a
    unique ``agent_id``. The identifier is also used by bridges to address
    messages to the correct manager instance.
  tools: Mapping of tool identifiers to configuration dictionaries. Each key
    must correspond to a registered tool plugin and its value supplies keyword
    arguments passed to the tool's constructor. For example:

    ``web_search: {}``
        Uses :class:`WebSearchTool` to query the DuckDuckGo API.

    ``database_query:``
        ``db_path: "knowledge.kuzu"`` – path to a Kùzu database used by
        :class:`DatabaseQueryTool` for executing Cypher queries.

    The manager initialises each listed tool during pipeline start-up and
    forwards queries to the selected tool at runtime. Results are returned to the
    pipeline for further processing or integration into model responses. When
    enabled the initialised manager is exposed on the MARBLE instance as
    ``marble.tool_manager`` for direct use in custom scripts.

cross_validation:
  folds: Number of partitions used during k-fold evaluation. Each fold serves as a
    validation set once while the remaining folds train the model. Must be
    at least 2 and typically 5 or 10. Keeping folds constant ensures
    consistent comparisons across runs.
  seed: Random seed applied before shuffling dataset indices for fold
    creation. Using a fixed value yields deterministic splits so repeated
    experiments produce identical train/validation partitions regardless of
    global RNG state. Both values are read by :func:`cross_validation.cross_validate`
    when ``folds`` or ``seed`` arguments are omitted, providing repository-wide
    defaults for CPU and GPU runs.

serve_model:
  host: Interface address that the HTTP inference server binds to when
    launched from a pipeline step. Set to "0.0.0.0" to accept external
    connections or "localhost" for local-only access.
  port: TCP port for the inference API. Choose an unused value; multiple
    concurrent servers must each use a unique port. The server automatically
    adapts to CPU or GPU execution depending on the active device. The
    ``ServeModelPlugin`` consults this section for default ``host`` and ``port``
    values when a pipeline step omits explicit parameters.

mcp_server:
  host: Network interface where the standalone MCP server binds. Use
    ``"localhost"`` to accept only local connections or ``"0.0.0.0"`` so remote
    machines can reach the service. The server automatically selects CPU or GPU
    execution based on the active device.
  port: TCP port number for the MCP protocol endpoint. Values must be between
    ``1`` and ``65535`` and should not conflict with other services. Each
    concurrent server instance requires a unique port.
  auth:
    token: Optional bearer token that clients must include in the
      ``Authorization`` header. When ``null`` the server does not require token
      authentication. Tokens allow simple shared-secret authentication without
      managing user accounts.
    username: Optional username for HTTP basic authentication. If set, a
      matching ``password`` must also be provided. Both fields may be ``null``
      to disable basic authentication.
    password: Password paired with ``username`` when basic authentication is in
      use. Choose strong, random values when exposing the server on untrusted
      networks. When both ``username`` and ``password`` are ``null`` basic
      authentication is disabled.

sync:
  interval_ms: Number of milliseconds between background tensor synchronization cycles
    across devices in distributed training. Synchronization uses delta encoding
    (XOR for integer tensors, arithmetic differences for floating tensors) and
    applies updates atomically per device. Lower values (100–500) keep model
    parameters closely aligned but increase communication overhead. Higher values
    (1000–10000) reduce network usage at the cost of more stale updates. Must be
    a positive integer. Defaults to ``1000``. When specified, MARBLE constructs a
    :class:`TensorSyncService` available as ``marble.tensor_sync_service`` which
    keeps any registered tensors synchronised on CPU or GPU devices.

evolution:
  population_size: Number of candidate configurations evaluated in each
    generation. Larger populations explore the search space more thoroughly but
    require proportionally more compute. Values of ``4`` to ``32`` work well for
    small experiments; advanced searches may scale into the hundreds.
  selection_size: Number of top-performing candidates preserved after each
    generation. Must be at least ``1`` and no greater than ``population_size``.
    A larger value maintains diversity while a smaller one accelerates
    convergence.
  generations: Total number of evolutionary generations to run. Each generation
    evaluates ``population_size`` candidates and spawns a new population from
    the best ``selection_size`` individuals. Setting this between ``2`` and
    ``20`` balances search depth with runtime.
  steps_per_candidate: Number of training steps performed when estimating a
    candidate's fitness. Evaluating more steps yields a more reliable fitness
    score at the cost of additional training time. Must be a positive integer.
  mutation_rate: Fraction of mutable parameters altered when producing an
    offspring. ``0.0`` disables mutation while ``1.0`` mutates every parameter.
  Typical values between ``0.05`` and ``0.3`` introduce useful diversity
    without drifting too far from the parent configuration.
  parallelism: Maximum number of candidates evaluated concurrently. Increase this
    to exploit multi-core CPUs or multiple GPUs. When set higher than the number
    of available devices the evaluations queue until resources free up. Defaults
    to the number of local CPU cores.

  To launch an evolutionary search with these defaults call
  ``evolution_trainer.run_evolution`` providing a base configuration, a
  training function and a mutation space. Any omitted arguments are
  populated from this section of ``config.yaml`` and the search runs on
  CPU or GPU depending on availability.

core:
  backend: Selects the tensor computation library used throughout MARBLE's core
    operations. ``numpy`` provides maximum compatibility and is the default
    choice. ``jax`` enables differentiable execution and JIT compilation when the
    JAX library is installed. Both backends run on CPU or GPU depending on the
    environment. Switching this parameter changes the behaviour of seed
    generation and message passing at runtime, allowing like-for-like experiments
    across backends.
  xmin: Minimum X coordinate for the Mandelbrot seed. Defines the left boundary
    of the complex plane used when initializing neuron values.
  xmax: Maximum X coordinate for the seed. Together with ``xmin`` this sets the
    horizontal span of the initial fractal.
  ymin: Minimum Y coordinate of the complex plane.
  ymax: Maximum Y coordinate of the complex plane.
  width: Horizontal resolution of the seed grid. Larger values produce more
    neurons but also require additional memory.
  height: Vertical resolution of the seed grid.
  max_iter: Number of Mandelbrot iterations to run. A higher count yields more
    detailed values. Must be greater than zero.
  representation_size: Length of the representation vector attached to each
    neuron. Larger values allow richer message passing but increase memory and
    computation.
  message_passing_alpha: Mixing factor between each neuron's existing
    representation and the update computed from its neighbours during message
    passing. ``0.0`` relies entirely on new information, while ``1.0`` keeps the
    original representation unchanged. Typical values range from 0.3 to 0.8
    depending on how aggressively information should propagate.
  weight_init_min: Lower bound for initial synapse weights when the core builds
    its default connectivity. Values below ``0.0`` allow inhibitory links while
    higher values force excitatory starts.
  weight_init_max: Upper bound for initial synapse weights. Keeping this near
    ``1.5`` maintains moderate signal strength but it may be increased for more
    aggressive dynamics.
  mandelbrot_escape_radius: Escape radius used when computing the Mandelbrot
    seed. Increasing this explores a wider region of the complex plane.
  mandelbrot_power: Power applied during Mandelbrot iterations. ``2`` produces
    the classic set while higher integers create different fractal patterns.
  tier_autotune_enabled: When enabled the core automatically migrates neurons
    between VRAM, RAM and disk to keep usage below the configured
    ``*_limit_mb`` values. Neurons are moved in order of age with the oldest
    leaving the faster tiers first. This prevents out-of-memory errors while
    keeping recently created neurons in the quickest storage available.
  memory_cleanup_interval: Seconds between automated clean-up passes that remove
    expired objects from lower tiers.
  representation_noise_std: Standard deviation of Gaussian noise added to
    representations after each message passing step.
  gradient_clip_value: Absolute value used to clip weight updates during
    training. ``0`` disables clipping.
  synapse_weight_decay: Fraction of each synapse's weight removed after every
    call to ``run_message_passing``. This gradually shrinks connections to
    prevent runaway growth. Typical values are between ``0.0`` (disabled) and
    ``0.1`` for slow decay.
  message_passing_iterations: Number of times the message passing routine runs
    per training step. ``Core.run_message_passing`` automatically performs this
    many iterations when called without an explicit count. Increasing the value
    deepens information flow but costs time.
  cluster_algorithm: Name of the clustering method used by ``cluster_neurons``;
    ``"kmeans"`` is the current default.
  vram_limit_mb: Maximum megabytes of GPU memory dedicated to VRAM tiers. If
    CUDA is unavailable this limit is merged into RAM.
  ram_limit_mb: Maximum megabytes of system memory used for RAM tiers.
  disk_limit_mb: Maximum megabytes for disk-based neuron tiers.
  file_tier_path: Path to the file used by the ``FileTier`` for persisting
    modified data. The directory is created automatically if it does not
    exist.
  init_noise_std: Standard deviation of Gaussian noise added to the initial
    Mandelbrot values. ``0.0`` keeps deterministic seeds while larger values
    create varied starting states.
  default_growth_tier: Preferred memory tier used for newly created neurons when
    ``Core.expand`` is called without an explicit ``target_tier``. Valid options
    are names present in ``TIER_REGISTRY`` such as ``"vram"``, ``"ram"``,
    ``"disk"`` or ``"file"`` (when the ``FileTier`` is enabled). The tier is
    only selected if sufficient capacity remains under the corresponding
    ``<tier>_limit_mb`` setting; otherwise the core falls back to the next tier
    based on availability. By default the system prefers ``"vram"`` so growth
    happens on the fastest memory when possible. Override this to ``"ram`` or
    ``"disk"`` when operating on machines without reliable GPU memory or to
    force new allocations to cheaper storage.
  random_seed: Seed used for random operations in the core. Keeping this fixed
    ensures reproducible initialization.
  backend: Selects the tensor math library for core operations. ``"numpy"``
    executes with NumPy on the CPU while ``"jax"`` uses JAX and transparently
    exploits GPU acceleration when available. Choose ``"jax"`` to enable
    automatic differentiation with GPU/CPU fallback. Defaults to ``"numpy"``.
  message_passing_dropout: Fraction of messages dropped during propagation to
    regularize learning. ``0.0`` disables dropout.
  synapse_dropout_prob: Default probability applied to newly created synapses
    of type ``"dropout"``. ``0.0`` leaves all such synapses active while ``1.0``
    disables them completely on every transmission. Intermediate values randomly
    silence connections with the given probability.
  synapse_batchnorm_momentum: Default momentum for ``batchnorm`` synapses when
    updating their running mean and variance. Values close to ``1.0`` adapt
    quickly while lower values provide smoother statistics. Must lie within
    ``0.0``–``1.0``.
  representation_activation: Activation function used by the internal MLP for
    message passing. Typical values are ``"tanh"`` or ``"relu"``.
  apply_layer_norm: When true the hidden layer of ``_simple_mlp`` is normalised
    to zero mean and unit variance before the final activation. This often
    stabilises training dynamics. Set ``false`` to disable.
  use_mixed_precision: Enable automatic mixed precision for ``_simple_mlp`` and
    other tensor operations. When set to ``true`` and a compatible GPU is
    available, computations run in half precision within a ``torch.autocast``
    context. This can greatly speed up training but may introduce minor numerical
    differences compared to full precision.
  quantization_bits: Number of bits used to quantize the core MLP weights. When
    greater than ``0`` the weights of the message passing MLP are uniformly
    quantized to this precision after loading the configuration. Typical values
    are ``8`` for moderate compression or ``4`` for aggressive size reduction.
    Setting ``0`` disables quantization and keeps weights in full precision.
  weight_init_mean: Mean of the normal distribution used to initialize
    synaptic weights.
  weight_init_std: Standard deviation of the normal distribution when
    ``weight_init_type`` is set to ``"normal"``. Higher values produce a
    wider spread of initial weights.
  weight_init_type: Distribution used for initial synapse weights. ``"uniform"``
    samples values between ``weight_init_min`` and ``weight_init_max``.
    ``"normal"`` draws from a normal distribution with ``weight_init_mean``
    and ``weight_init_std``. ``"xavier_uniform"`` and ``"xavier_normal"`` use
    Xavier initialisation based on both the fan-in and fan-out of each
    connection. ``"kaiming_uniform"`` and ``"kaiming_normal"`` implement
    Kaiming (He) initialisation for deeper networks. ``"constant"`` sets all
    weights to ``weight_init_mean``.
  weight_init_strategy: Strategy for the message passing MLP weights. Supports
    ``"uniform"`` and ``"normal"`` with the same semantics as
    ``weight_init_type``.
  show_message_progress: When ``true`` ``perform_message_passing`` renders a
    progress bar using :mod:`tqdm` so long operations provide visual feedback.
    Requires the optional ``tqdm`` dependency and is controlled via
    ``core.show_message_progress``. Defaults to ``false``.
  message_passing_beta: Secondary mixing factor controlling how quickly new
    representations replace old ones when ``alpha`` alone is insufficient.
  attention_temperature: Softmax temperature used by the attention module
    during message passing. Lower values (<1.0) concentrate weight on the
    highest scoring neighbours while larger values produce smoother
    distributions. Must be greater than zero.
  attention_dropout: Probability between 0.0 and 1.0 of ignoring an incoming
    message during attention computation. A value of ``0.0`` disables dropout
    while ``1.0`` results in no neighbours being considered. When messages are
    dropped the remaining attention weights are renormalized so the sum equals
    one.
  attention_causal: When ``true`` the attention module applies a causal mask so
    that position ``i`` cannot attend to any ``j > i``. This prevents future
    information from leaking into the computation and is essential for
    autoregressive models. Set to ``false`` for bidirectional contexts. The
    mask is generated on the active device and works on both CPU and GPU backends.
  attention_gating:
    enabled: Enables modulation of attention logits by a deterministic gating
      function. When ``false`` scores are unmodified.
    mode: Selects the gating function. ``"sine"`` produces a sinusoidal pattern
      in the range ``[-1, 1]`` while ``"chaos"`` emits a chaotic sequence from
      the logistic map in the range ``(0, 1)``. Any other value disables gating.
    frequency: Number of full sine oscillations across the sequence length when
      ``mode`` is ``"sine"``. Typical range ``0.1``–``10``.
    chaos: Coefficient ``r`` of the logistic map used when ``mode`` is
      ``"chaos"``. Values between ``3.5`` and ``3.9`` yield chaotic behaviour;
      the parameter must lie within ``(0, 4]``.
  salience_weight: Scaling factor applied to salience inputs provided through
    attention codelets. Increase above ``1.0`` to emphasise externally supplied
    salience scores during coalition formation.
  global_phase_rate: Amount added to ``global_phase`` every time
    ``run_message_passing`` executes. Combined with each synapse's individual
    ``phase`` value, this creates oscillatory gating via the cosine in
    ``Synapse.effective_weight``. Set to ``0.0`` to disable phase-based
    modulation or increase toward ``math.pi`` for quicker oscillations.
  energy_threshold: Minimum energy level required before neurons participate in
    message passing. Helps filter out inactive units.
  reinforcement_learning_enabled: Enable or disable the internal Q-learning
    engine in the core. Set to ``true`` to allow calling the ``rl_`` methods.
  rl_discount: Discount factor applied to future rewards when updating the
    core's Q-table. Typical values range from ``0.8`` to ``0.99``.
  rl_learning_rate: Step size used for Q-value updates. Smaller values result in
    slower but more stable learning.
  rl_epsilon: Initial exploration rate for epsilon-greedy action selection.
  rl_epsilon_decay: Multiplicative decay applied to ``rl_epsilon`` after each
    update step.
  rl_min_epsilon: Minimum exploration rate once decay has progressed.
  early_cleanup_enabled: If ``true``, unused neurons are removed before normal
    cleanup intervals, reducing memory usage.
  pretraining_epochs: Number of unsupervised epochs to run before regular
    training begins. During this phase the system performs an autoencoding
    warm-up where each input is used as its own target, stabilising weights
    prior to supervised updates. Set to ``0`` to disable. Recommended range is
    ``0``–``10`` depending on dataset size.
  min_cluster_k: Smallest number of clusters allowed when ``cluster_neurons``
    is invoked automatically. This lower bound prevents the topology from
    collapsing into a single cluster. If the network contains fewer neurons
    than ``min_cluster_k`` the number of neurons is used instead. Typical
    values are between ``2`` and ``10``.
  diffusion_steps: Number of denoising iterations performed during diffusion
    sampling. Values above ``1`` produce smoother results but increase
    computation time.
  noise_start: Standard deviation of the Gaussian noise applied at the first
    diffusion step. Higher values start the process from a noisier state.
  noise_end: Noise level used for the final diffusion step. Typically set close
    to ``0.0`` for crisp outputs.
  noise_schedule: Schedule controlling how the noise decreases across
    ``diffusion_steps``. Valid options are ``"linear"`` or ``"cosine"``.
  workspace_broadcast: Set to ``true`` to publish each diffusion result to the
    Global Workspace. Components subscribed to the workspace can then react to
    new samples in real time.
  activation_output_dir: Directory where a heatmap of neuron activations is
    written after every diffusion run. Useful for visualising internal state
    evolution. The directory is created automatically if it does not exist.
  activation_colormap: Name of the Matplotlib colour map used for activation
    heatmaps. Popular options are ``"viridis"``, ``"plasma"`` and
    ``"cividis"``. Choose a map that provides good contrast when visualising
    neuron activations. Custom names accepted by
    :func:`matplotlib.pyplot.colormaps` may also be used.
  memory_system:
    long_term_path: File used for persistent storage of diffusion outputs. Both
      short- and long-term layers are utilised so recent results can be cached
      in RAM while important entries are kept on disk.
    threshold: Neuromodulatory value above which entries bypass short-term
      storage. Typical values fall between ``0.3`` and ``0.7``.
    consolidation_interval: Number of stored items between automatic disk
      consolidation.
  cwfl:
    num_basis: Number of radial basis functions forming the smooth weight
      field. More bases allow finer detail but require additional computation.
    bandwidth: Spread of each basis function controlling the locality of weight
      influence.
    reg_lambda: L2 penalty applied to basis weights to prevent divergence.
    learning_rate: Step size used when updating the weight field each step.
  harmonic:
    base_frequency: Starting frequency for the harmonic encoder that converts
      scalar inputs into oscillatory representations.
    decay: Multiplicative factor applied after each update so the active
      frequency gradually decreases and stabilises.
  fractal:
    target_dimension: Desired fractal dimension of the representation space.
      When the measured dimension exceeds this threshold the core expands its
      representation size to capture additional structure.
  cross_tier_migration: When ``true`` neurons may move between tiers even if
    their age thresholds have not been reached.
  synapse_echo_length: Number of past activations each synapse stores in its
    echo buffer. Larger values retain longer history but increase memory usage.
  synapse_echo_decay: Multiplicative factor applied to incoming activations
    before they are appended to the echo buffer. Values in ``0.0``–``1.0``
    gradually forget older activity.
  interconnection_prob: Probability in the range ``0.0``–``1.0`` of creating an
    interconnection synapse when multiple cores are combined. ``0.0`` keeps
    cores isolated while ``1.0`` connects every neuron in one core to every
    neuron in the others. When functions such as ``core_interconnect.
    interconnect_cores`` are called without an explicit probability this value
    (averaged across all provided cores) is used. The default ``0.05`` yields a
    sparse cross-core connectivity while values above ``0.2`` lead to dense
    coupling.

neuronenblitz:
  backtrack_probability: Probability between 0 and 1 that the wandering
    algorithm will revisit a previous neuron when exploring a path.
  consolidation_probability: Chance between 0 and 1 that an activated synapse
    will have its potential increased during consolidation.
  consolidation_strength: Multiplier applied when increasing synapse potential.
    Values above 1.0 accelerate learning while lower values slow it down.
  route_potential_increase: Amount added to a synapse's potential when it is
    traversed during activation.
  route_potential_decay: Factor applied periodically to reduce potentials of
    rarely used synapses. Typical range is 0.0–1.0.
  route_visit_decay_interval: Number of activations between decay steps
    specified by ``route_potential_decay``.
  alternative_connection_prob: Probability of creating a new random connection
    instead of following existing routes. Helps exploration of novel paths.
  split_probability: Likelihood that a wander branches into multiple paths at a
    neuron. Must be between 0 and 1.
  merge_tolerance: Maximum difference allowed between neuron values when merging
    converging paths. Small values enforce stricter merging behaviour.
  plasticity_threshold: Minimum potential a synapse must reach before structural
    changes such as adding or removing connections are applied.
  continue_decay_rate: Factor multiplied with the current continuation
    probability during wandering. Lower values shorten exploration paths,
    while values closer to ``1.0`` keep paths longer. Typical range is
    0.7–0.9.
  struct_weight_multiplier1: Multiplier applied to the first new synapse weight
    created during structural plasticity. Controls how strongly the new forward
    connection influences signal flow.
  struct_weight_multiplier2: Multiplier applied to the second synapse that
    reconnects to the original target neuron after a structural change.
  attention_decay: Fraction by which accumulated type attention scores are
    reduced after each query. Values under ``1.0`` gradually forget old
    preferences.
  max_wander_depth: Maximum recursion depth allowed in ``dynamic_wander``.
    Limits exploration of extremely long paths. The actual limit for each
    call is drawn from this value plus ``wander_depth_noise``.
  learning_rate: Scalar applied to weight updates produced by ``weight_update_fn``.
  weight_decay: Proportional decrease applied to all synapse weights each epoch
    to prevent uncontrolled growth. ``0.0`` disables this effect.
  dropout_probability: Fraction of synapses temporarily ignored during training
    to improve robustness.
  dropout_decay_rate: Multiplicative factor applied to ``dropout_probability``
    after each training epoch. Values below ``1.0`` gradually reduce the
    effective dropout as learning progresses while higher values keep it
    constant. The rate must be between ``0.0`` and ``1.0``.
  exploration_decay: Rate at which the exploration bonus diminishes over time.
    Values slightly below ``1.0`` gradually reduce random behaviour.
  reward_scale: Multiplier applied to reward signals before they adjust
    plasticity.
  stress_scale: Multiplier applied to stress signals before they reduce
    plasticity.
  auto_update: Boolean flag controlling automatic refresh of the learning
    state when the dataset changes. When ``true`` a checksum based
    :class:`DatasetWatcher` monitors the dataset directory and triggers
    :meth:`Neuronenblitz.reset_learning_state` whenever files are modified.
    Set to ``false`` to disable this behaviour. The watcher performs all work
    on the CPU so it behaves identically on GPU and CPU environments.
  attention_span_threshold: Cumulative attention mass (0.0–1.0) determining
    how many high-scoring elements are kept when trimming a path. Lower values
    yield shorter spans while ``1.0`` retains all elements. Span computation is
    fully vectorised, supports batched inputs and automatically falls back to
    CPU when CUDA is unavailable.
  max_attention_span: Optional hard limit on the number of elements retained
    after applying ``attention_span_threshold``. ``null`` disables the cap.
    Use this to bound memory usage for very long sequences.
  remote_fallback: When true, neurons attempting remote execution fall back to
    local processing if the remote call fails.
  noise_injection_std: Standard deviation of Gaussian noise injected into neuron
    activations during training.
  dynamic_attention_enabled: Enables per-type attention updates in
    ``update_attention``. Disabling keeps attention static.
  backtrack_depth_limit: Maximum distance for which backtracking may revisit
    previous neurons when ``backtrack_enabled`` is true.
  synapse_update_cap: Maximum absolute magnitude allowed for each individual
    weight update. Prevents large sudden jumps that may destabilize learning.
    The default is 1.0 and values typically range from 0.01 to 5.0 depending on
    how aggressively weights should adapt.
  structural_plasticity_enabled: When false, ``apply_structural_plasticity`` is
    skipped entirely.
  backtrack_enabled: If false, wandering never returns to previously visited
    neurons regardless of ``backtrack_probability``.
  loss_scale: Multiplier for training errors before they are applied to weights.
  loss_module: PyTorch loss module object or registry key to compute training
    errors. Provide either an instance such as ``torch.nn.MSELoss()`` or the
    name of a plugin-registered module (see ``plugin_system``). When a string
    name is supplied the corresponding registered class is instantiated on CPU
    or GPU depending on hardware availability.
  exploration_bonus: Additional reward added to rarely used synapses when they
    are traversed, encouraging exploration.
  synapse_potential_cap: Upper bound for ``synapse.potential`` to prevent
    runaway growth.
  attention_update_scale: Scaling factor applied when updating neuron type
    attention statistics.
  plasticity_modulation: Global multiplier applied to weight updates after
    neuromodulation. Values above ``1.0`` accelerate structural changes.
  wander_depth_noise: Standard deviation of Gaussian noise used when
    computing the per-call depth limit. Positive values occasionally
    permit longer explorations while negative samples shorten them.
  reward_decay: Factor by which accumulated reward signals diminish each epoch.
  synapse_prune_interval: Number of epochs between automatic pruning passes
    removing low-potential synapses.
  structural_learning_rate: Step size used when modifying synapse weights
    during structural updates.
  remote_timeout: Timeout in seconds when waiting for responses from a remote
    Neuronenblitz instance.
  gradient_noise_std: Standard deviation of Gaussian noise added to weight
    gradients before updates are applied.
  min_learning_rate: Lower bound used when scheduling the learning rate.
  max_learning_rate: Upper bound allowed for adaptive learning rate schemes.
  top_k_paths: Number of most promising wander paths retained during
    exploration.
  parallel_wanderers: How many ``dynamic_wander`` processes or training worker
    threads to run in parallel.  The value also sets the default number of
    threads used by ``train_in_parallel``.  Setting this above ``1`` launches
    multiple temporary Neuronenblitz copies that explore the graph concurrently
    in separate OS processes and allows simultaneous training on different
    examples.  After all workers finish, their results are combined according to
    ``parallel_update_strategy``.  Values less than ``1`` are clamped to
    ``1``.  Using more workers than available CPU cores can reduce throughput
    due to context switching overhead.
  parallel_update_strategy: Method for merging results from parallel wanderers.
    ``best`` (default) replays only the wanderer that achieves the greatest
    loss reduction, shortest path and smallest predicted model size. ``average``
    averages the errors from all wanderers and applies each path's weight
    updates scaled by ``1 / parallel_wanderers``. Use ``average`` to smooth
    learning when running many wanderers.
  beam_width: Number of candidate paths retained at each depth during the
    beam search variant of ``dynamic_wander``. Wider beams explore more options
    but increase computation time. A value of ``1`` disables beam search and
    reverts to the original recursive wander.
  wander_cache_ttl: Time-to-live in seconds for ``dynamic_wander`` cache
    entries. Cached paths older than this value are removed before reuse.
    Values above ``0`` enable expiration while ``0`` keeps results indefinitely.
  wander_anomaly_threshold: Number of standard deviations a wander path length
    must differ from the running mean to be flagged as anomalous. When exceeded
    a warning is logged and ``MetricsVisualizer`` records ``wandering_anomaly``
    with a value of ``1``.
  wander_history_size: How many recent path lengths are kept to compute the
    running mean and standard deviation for anomaly detection. Larger values
    smooth out short-term fluctuations.
  phase_rate: Amount added to ``global_phase`` each time ``dynamic_wander``
    runs. Higher values make the oscillator advance more quickly, altering
    the cosine gating applied to every synapse.
  phase_adaptation_rate: Step size used to adjust each synapse's individual
    ``phase`` after a weight update. Positive error values push phases
    forward while negative errors pull them backward. The value should
    typically stay below ``1.0`` to prevent rapid oscillations.
  synaptic_fatigue_enabled: When true each synapse maintains a temporary
    fatigue value that reduces its effective weight after repeated use. This
    models biological short-term depression and can help prevent domination by
    a few highly active connections.
  fatigue_increase: Amount added to a synapse's fatigue every time it is
    traversed. Larger values cause quicker weakening. ``0.0`` disables new
    fatigue accumulation.
  fatigue_decay: Multiplicative factor applied to all fatigue values at the
    start of each ``dynamic_wander`` call. Values near ``1.0`` make fatigue
    persist for many steps while lower values allow faster recovery.
  lr_adjustment_factor: Fractional step used by ``adjust_learning_rate`` when
    increasing or decreasing ``learning_rate`` in response to recent error
    trends.
  lr_scheduler: Scheduler type controlling how ``learning_rate`` evolves over
    epochs. ``"none"`` keeps the rate constant aside from manual adjustments,
    ``"cosine"`` performs a cosine decay over ``scheduler_steps`` epochs,
    ``"exponential"`` multiplies the rate by ``scheduler_gamma`` each epoch, and
    ``"cyclic"`` oscillates the rate between ``min_learning_rate`` and
    ``max_learning_rate`` every ``scheduler_steps`` epochs.
  scheduler_steps: Number of epochs governing scheduler behaviour. For the
    cosine scheduler this is the decay period. For the cyclic scheduler it is
    half of a full up/down cycle. Typical values range between ``10`` and ``100``
    depending on dataset size.
  scheduler_gamma: Multiplicative factor used by the exponential scheduler each
    epoch. Values below ``1.0`` decay the rate while values above ``1.0``
    increase it. ``0.95`` is a gentle decay while ``0.5`` halves the rate every
    epoch.
  epsilon_scheduler: Scheduler controlling how ``rl_epsilon`` evolves. Supports
    the same options as ``lr_scheduler`` and updates after each training batch.
    ``"none"`` keeps the exploration probability constant.
  epsilon_scheduler_steps: Controls the period of cosine or cyclic epsilon
    schedules. Ignored for ``"exponential"``.
  epsilon_scheduler_gamma: Multiplicative factor for the exponential epsilon
    scheduler. Values below ``1.0`` decrease exploration more aggressively.
  momentum_coefficient: Coefficient used to blend the previous weight update
    with the current gradient. ``0.0`` disables momentum while values between
    ``0.0`` and ``1.0`` accelerate training by smoothing updates. Momentum is
    applied in ``apply_weight_updates_and_attention`` and is stored per-synapse
    so that frequently updated connections gain additional inertia.
  use_echo_modulation: When ``true`` weight updates are multiplied by each
    synapse's echo buffer average, enabling the synaptic echo learning
    mechanism. Disable to revert to standard updates.
  reinforcement_learning_enabled: Turn on Q-learning inside Neuronenblitz. When
    enabled the ``rl_`` methods may be used to select actions and update
    values directly through the network.
  rl_discount: Discount factor for future rewards used by ``rl_update``.
  rl_epsilon: Initial exploration probability for ``rl_select_action``.
  rl_epsilon_decay: Multiplicative decay applied to ``rl_epsilon`` each time an
    update occurs.
  rl_min_epsilon: Smallest allowed exploration rate once decay has taken place.
  entropy_epsilon_enabled: When ``true`` the ``update_exploration_schedule``
    method also adjusts ``rl_epsilon`` using the entropy of synapse visit
    counts. Low entropy (meaning few synapses are explored) pushes
    ``rl_epsilon`` toward 1.0 to encourage broader exploration, while high
    entropy reduces it toward ``rl_min_epsilon``. This automatic balancing helps
    trade off exploration and exploitation without manual schedules.
  shortcut_creation_threshold: Number of times the exact synapse path must be
    observed before a direct shortcut connection from the first to the last
    neuron is added. Set ``0`` to disable automatic shortcut formation.
  chaotic_gating_enabled: Enables logistic-map scaling of each weight update.
    When true the gate evolves chaotically and multiplies the computed update
    value, introducing non-linear training dynamics.
  chaotic_gating_param: Bifurcation parameter for the logistic gate update.
    Values between ``3.6`` and ``4.0`` produce strong chaotic behaviour.
  chaotic_gate_init: Starting value for the gate. Must lie between ``0`` and
    ``1`` and determines the initial update scale.
  context_history_size: Number of recent neuromodulatory contexts to keep
    in memory. Larger values allow long-term trends to influence wandering but
    consume more memory. Typical range is 5–20.
  context_embedding_decay: Multiplicative decay applied when computing the
    context embedding from history. ``1.0`` weights all contexts equally while
    lower values emphasise more recent entries.
  emergent_connection_prob: Probability between 0 and 1 that ``dynamic_wander``
    adds a random synapse connecting two neurons after each call. Higher values
    encourage spontaneous cross-links, increasing the chance of unexpected
    behaviours. Recommended range is 0.0–0.2.
  concept_association_threshold: Number of times a specific neuron pair must
    appear consecutively in a wander path before a new concept neuron is
    inserted to bridge them. Higher values create concepts more conservatively.
  concept_learning_rate: Initial weight assigned to each side of a newly
    created concept neuron. This determines how strongly the new concept
    influences signal flow right after insertion.
  weight_limit: Upper bound applied to every synapse weight. Any update that
    would exceed this magnitude is clipped to maintain numerical stability.
  wander_cache_size: Maximum number of recent ``dynamic_wander`` results kept
    in the cache. Older entries are discarded once this count is exceeded.
  rmsprop_beta: Exponential decay factor used by the RMSProp-like gradient
    smoothing in ``apply_weight_updates_and_attention``. Typical values are
    0.9–0.99.
  grad_epsilon: Small constant added when normalising gradients to avoid
    division by zero.
  use_experience_replay: Enable prioritized experience replay. When true
    Neuronenblitz keeps past training examples with their errors and samples
    them again according to those errors after each epoch.
  replay_buffer_size: Maximum number of experiences retained for replay. Old
    entries are discarded once this limit is exceeded.
  replay_alpha: Exponent determining how strongly error magnitude influences
    sampling probability. ``0`` gives uniform sampling while ``1.0`` weights
    fully by error.
  replay_beta: Importance sampling correction applied to replay updates. Values
    near ``1.0`` reduce bias from prioritisation.
  replay_batch_size: Number of stored experiences replayed after each epoch
    when experience replay is active.
  exploration_entropy_scale: Multiplier applied to the entropy of synapse visit
    counts when computing adaptive dropout rates.
  exploration_entropy_shift: Constant offset added after scaling the entropy.
  gradient_score_scale: Factor converting gradient magnitude into additional
    synapse potential during weight updates.
  memory_gate_decay: Multiplicative decay applied to memory-based gating scores
    that bias ``weighted_choice``. Lower values make memories fade quickly.
  memory_gate_strength: Amount added to a synapse's gating score whenever a
    path yields an error below ``episodic_memory_threshold``.
  episodic_memory_size: Number of recent successful paths stored for gating.
  episodic_memory_threshold: Maximum absolute error for a path to be recorded
    in episodic memory.
  episodic_memory_prob: Probability that wandering will be biased by episodic
    memory on a given call.
  episodic_sim_length: Number of episodes replayed during episodic simulation
    planning. Higher values explore more possibilities but increase
    computation time. Typical range is ``1`` to ``10``.
  curiosity_strength: Weight of novelty-driven curiosity in ``weighted_choice``.
    Higher values encourage exploring rarely visited synapses.
  depth_clip_scaling: Scaling factor controlling how strongly gradient clipping
    tightens for long wander paths.  ``1.0`` halves the update cap at maximum
    depth.
  forgetting_rate: Multiplicative decay applied to stored context history on
    each wander. Values below ``1.0`` gradually forget old neuromodulatory
    signals.
  structural_dropout_prob: Probability that a qualified synapse is skipped
    during structural plasticity. Increasing this value reduces the frequency
    of structural changes.
  gradient_path_score_scale: Multiplier applied to the gradient magnitude of
    a wander path when selecting the best result. When
    ``rms_gradient_path_scoring`` is ``false`` the magnitude is computed as the
    sum of absolute recent gradients stored in ``_prev_gradients``; when set to
    ``true`` it becomes the root-mean-square of the RMSProp statistics in
    ``_grad_sq``. Larger values bias exploration toward routes that have
    produced strong updates.
  use_gradient_path_scoring: Set to ``true`` to enable gradient-based path
    scoring in ``dynamic_wander``. When ``false`` only the final neuron value
    determines which path is chosen.
  rms_gradient_path_scoring: Chooses the statistic used for
    ``compute_path_gradient_score``. The default ``false`` sums absolute values
    of the most recent gradients, favoring paths with a single high spike. When
    ``true`` the score uses the root-mean-square of the running squared
    gradients from the RMSProp optimizer, emphasizing paths that maintain large
    gradients over time. Toggle this to ``true`` when noisy gradients cause the
    simple magnitude to be unstable.
  activity_gate_exponent: Exponent controlling how strongly synapse visit
    counts gate subsequent weight updates. ``1.0`` scales updates by
    ``1/(1+visits)`` while larger values produce steeper decay.
  subpath_cache_size: Maximum number of path prefixes stored for quick reuse.
    Larger caches speed up wandering on graphs with repeated motifs at the cost
    of additional memory.
  subpath_cache_ttl: Time in seconds before cached subpaths expire. ``0`` keeps
    entries indefinitely.
  monitor_wander_factor: Multiplicative factor applied to the self-monitoring
    mean error when adjusting ``wander_depth_noise``. Higher values make
    wandering more exploratory as errors increase. Typical range ``0.0`` to
    ``0.5``.
  monitor_epsilon_factor: How strongly the self-monitoring mean error reduces
    the agent's exploration epsilon. ``0.1`` decreases epsilon by ``10%`` of the
    error each step. ``0.0`` disables this coupling.
  use_mixed_precision: Enable PyTorch automatic mixed precision during
    training. This trades some numerical precision for faster execution on
    compatible GPUs.
  quantization_bits: Bit precision used when quantizing the core weights on
    start-up. A value of ``0`` disables quantization. Larger values retain more
    accuracy but provide less compression. Values above ``16`` are not
    supported.

brain:
  save_threshold: Minimum improvement in validation loss required before the
    brain writes a new checkpoint to disk.
  max_saved_models: Upper bound on the number of checkpoint files retained in
    ``save_dir``.
  save_dir: Directory where checkpoint files are stored.
  firing_interval_ms: Milliseconds between automatic firing cycles when
    auto-fire is active.
  initial_neurogenesis_factor: Starting multiplier that scales the amount of new
    neurons grown during neurogenesis.
  offload_enabled: When true, highly active lobes are sent to a remote brain
    server for processing.
  torrent_offload_enabled: When true, lobes may be distributed among torrent
    clients using the tracker system.
  mutation_rate: Fraction of synapses to mutate during each evolutionary step.
    Higher values introduce more variation but may destabilize learning.
  mutation_strength: Maximum absolute change applied to a mutated synapse's
    weight. Typical values range from 0.01 to 0.1.
  prune_threshold: Synapses with absolute weight below this value are removed
    during pruning, keeping the network efficient.
  dream_num_cycles: Default number of cycles executed each time the brain
    performs a background dreaming session. Values between 1 and 20 are common
    depending on how much consolidation is desired.
  dream_interval: Seconds to wait between consecutive dreaming sessions when
    ``start_dreaming`` runs in the background. Short intervals keep memory
    consolidation frequent but consume more processing time.
  neurogenesis_base_neurons: Base number of neurons grown during each
    neurogenesis event when no explicit value is provided to
    ``perform_neurogenesis``.
  neurogenesis_base_synapses: Base number of synapses introduced alongside new
    neurons during neurogenesis when defaults are used.
  max_training_epochs: Total number of epochs the training loop should run
    before stopping automatically.
  memory_cleanup_enabled: Toggles periodic removal of stale data from RAM and
    disk tiers.
  manual_seed: Random seed applied during initialization for reproducible
    experiments.
  log_interval: Number of batches between status log messages during training.
  evaluation_interval: Number of epochs between validation runs.
  early_stopping_patience: Number of sequential epochs allowed without a
    sufficient decrease in validation loss. Once this count is exceeded the
    training loop terminates early. Set ``0`` to disable patience-based checks.
    Values between ``3`` and ``10`` work well for most experiments.
  early_stopping_delta: Minimum drop in validation loss required to reset the
    patience counter. This prevents noise in the validation metric from
    triggering false improvements. Typical ranges are ``1e-4`` to ``1e-3``.
  auto_cluster_interval: Epoch interval between automatic clustering of neurons.
  cluster_method: Algorithm used for clustering; ``"kmeans"`` is provided but
    others may be added in future.
  auto_save_enabled: Enables periodic saves controlled by ``auto_save_interval``.
  offload_threshold: Minimum lobe attention required before ``offload_high_attention``
    sends a subcore to the remote brain. Increasing this reduces how frequently
    offloading occurs.
  torrent_offload_threshold: Attention threshold used when distributing lobes
    via the torrent client.
  cluster_high_threshold: Score above which clusters are relocated directly to
    the fast ``vram`` tier when ``relocate_clusters`` runs.
  cluster_medium_threshold: Score above which clusters are kept in ``ram`` but
    below the high threshold; lower scores are moved to disk tiers.
  dream_synapse_decay: Multiplicative decay applied to synapse weights during
    each dreaming cycle.
  dream_decay_arousal_scale: Scaling factor applied to ``dream_synapse_decay``
    proportional to current arousal. ``0.0`` disables arousal-based modulation
    while higher values increase decay when arousal rises.
  dream_decay_stress_scale: Scaling factor that reduces the effective decay
    as stress grows. Values around ``0.0`` keep stress from affecting dream
    strength, whereas ``1.0`` completely cancels decay at full stress.
  neurogenesis_increase_step: Amount added to the neurogenesis factor when
    validation loss worsens.
  neurogenesis_decrease_step: Amount subtracted from the neurogenesis factor
    when validation improves.
  max_neurogenesis_factor: Upper bound that ``neurogenesis_factor`` will not
    exceed, regardless of adjustments.
  cluster_k: Number of clusters created when ``cluster_neurons`` is invoked
    during training.
  auto_save_interval: Number of epochs between automatic calls to
    ``save_model``. ``0`` disables periodic saving.
  backup_enabled: When true a background thread periodically copies files from
    ``save_dir`` into ``backup_dir`` so that checkpoints and logs survive
    unexpected crashes.
  backup_interval: Seconds between each automatic backup when
    ``backup_enabled`` is active. Short intervals provide more robust safety but
    may increase disk usage.
  backup_dir: Directory where backup copies are stored. Each run creates a
    timestamped subfolder inside this directory.
  auto_firing_enabled: If true the brain starts an auto-firing thread
    immediately after construction.
  dream_enabled: Enables background dreaming when ``start_dreaming`` is called.
  vram_age_threshold: Age in seconds above which neurons in VRAM are considered
    old when deciding growth tiers.
  precompile_graphs: When ``true`` the brain traces frequently executed compute
    graphs, such as the internal message passing MLP, using ``torch.jit`` before
    training begins.  The compiled graphs are cached for the current tensor
    shape, device and activation settings which removes graph construction
    overhead during every training step.  Enable this when model structure and
    input shapes remain stable.  On very small models the overhead of
    precompilation may outweigh the benefit, so the default is ``false``.
  ram_age_threshold: Equivalent threshold for neurons in RAM.
  status_display_interval: If greater than zero, ``display_live_status`` is
    invoked every N epochs during training.
  neurogenesis_interval: Number of epochs between automatic neurogenesis events.
  min_cluster_size: Minimum number of neurons required to form a cluster.
  prune_frequency: Number of epochs between automatic pruning operations.
  auto_offload: When true, ``offload_high_attention`` runs automatically after
    each training epoch.
  benchmark_enabled: Enables evaluation through ``BenchmarkManager`` at the end
    of each epoch.
  benchmark_interval: Number of epochs between automatic benchmark comparisons
    of pure MARBLE and its autograd pathway. Higher values reduce overhead.
  loss_growth_threshold: Validation loss level that triggers expansion of the
    core during training. The default of ``0.1`` keeps growth infrequent.
  auto_neurogenesis_prob: Baseline probability between 0 and 1 that
    ``maybe_autonomous_neurogenesis`` creates new neurons each epoch. The
    probability scales with the current validation loss.
  dream_cycle_sleep: Seconds to wait between dream cycles. Increase to reduce
    CPU usage during prolonged dreaming.
  dream_replay_buffer_size: Maximum number of past experiences stored for
    consolidation during dreaming. When the buffer exceeds this size,
    experiences with the lowest salience are evicted. Typical values range
    from ``50`` to ``1000`` depending on available memory.
  dream_replay_batch_size: Number of experiences sampled from the replay
    buffer in each dream cycle. Batches between ``1`` and ``32`` work well for
    small experiments. Larger batches increase consolidation speed but require
    more computation.
  dream_replay_weighting: Weighting strategy for sampling experiences from the
    replay buffer. ``"linear"`` biases selection proportionally to experience
    salience. ``"exponential"`` applies ``exp(salience)`` to emphasise high-
    salience items. ``"quadratic"`` squares the salience for a strong bias,
    ``"sqrt"`` uses the square root to soften differences and ``"uniform"``
    treats all memories equally.
  dream_instant_buffer_size: Size of the short-term buffer that holds newly
    observed experiences before consolidation. When the instant buffer reaches
    this size it is merged into the main replay buffer. Values between ``1`` and
    ``50`` work well in practice; larger sizes reduce merge frequency at the cost
    of delayed consolidation.
  dream_housekeeping_threshold: Minimum salience required for an experience to
    remain after housekeeping during dreams. Salience is computed from reward,
    emotion, arousal and stress and ranges from ``0`` (unimportant) to ``1``
    (highly important). Experiences below this threshold are pruned to keep the
    buffer focused on meaningful memories. Set to ``0`` to disable pruning
    entirely.
  tier_decision_params:
    vram_usage_threshold: Fraction of VRAM usage that triggers migration of
      neurons to a lower tier.
    ram_usage_threshold: Fraction of RAM usage that triggers migration to disk
      or file tiers.
  model_name: Descriptive name used when saving checkpoints.
  checkpoint_format: Serialization format for checkpoints. ``"pickle"`` is
    the default but ``"safetensors"`` may be used for faster loading.
  checkpoint_compress: When true checkpoints are gzip compressed and saved atomically to avoid corruption.
  metrics_history_size: Number of past epochs kept in memory for plotting
    performance metrics.
  profile_enabled: When true a ``UsageProfiler`` records CPU and GPU usage
    as well as epoch runtime during training.
  profile_log_path: CSV file path where profiler entries are appended.
  profile_interval: Number of epochs between profiler recordings. Values of
    1 log every epoch while higher values sample less frequently.
  early_stop_enabled: When ``true`` the trainer monitors the validation loss
    after each epoch and halts once the patience criteria are met. Set to
    ``false`` to force the loop to run ``max_training_epochs`` regardless of
    validation performance.
  lobe_sync_interval: Seconds between background synchronization of lobe data
    when remote syncing is active.
  cleanup_batch_size: Maximum number of objects removed in one cleanup pass.
  remote_sync_enabled: When ``true`` brain state is periodically pushed to a
    remote server.
  default_activation_function: Activation used when constructing new neurons.
  neuron_reservoir_size: Capacity of the internal neuron reservoir used when
    sampling replacements during pruning.
  lobe_decay_rate: Fraction by which lobe attention decays each epoch without
    activity.
  dimensional_search:
    enabled: Toggles automatic growth of the representation vector when
      validation loss improvements stagnate. ``true`` enables monitoring.
    max_size: Highest dimension the search may reach. Should exceed the initial
      ``representation_size`` but remain manageable for available hardware.
    improvement_threshold: Relative decrease in validation loss required to
      deem a new dimension successful. Values around ``0.01`` to ``0.05`` are
      typical.
    plateau_epochs: Number of consecutive epochs below the improvement
      threshold before another dimension is added.
  super_evolution_mode: When true, a controller records loss, speed, complexity
    and resource metrics each epoch and adjusts all configurable parameters via
    self-attention. This prioritizes minimising loss, then maximising speed,
    reducing complexity and finally conserving resources.

formula: Mathematical expression used to compute initial neuron values instead
  of the Mandelbrot seed. If omitted the Mandelbrot algorithm is used.
formula_num_neurons: Number of neurons created when ``formula`` is provided.

meta_controller:
  # The meta-controller monitors validation losses and adjusts
  # ``neuronenblitz.plasticity_threshold`` accordingly. If losses rise the
  # threshold is lowered so structural changes become less likely. When losses
  # fall it increases the threshold within the allowed range to encourage more
  # exploration. This keeps learning stable over long runs.
  history_length: Number of recent validation losses used for the adjustment.
    Typical values range from 3 to 20 depending on how quickly adaptation should
    react.
  adjustment: Amount by which the plasticity threshold changes when performance
    improves or degrades. Values between 0.1 and 1.0 are common.
  min_threshold: Lower bound for the plasticity threshold. Should remain above
    zero, often around 0.5–5.0.
  max_threshold: Upper bound for the plasticity threshold. Values around
    10–50 keep the system from becoming too rigid.

neuromodulatory_system:
  # Represents the internal state of the brain. Arousal and reward boost
  # neurogenesis while stress suppresses it. Emotion provides a qualitative tag
  # that may be used by higher-level modules. Values typically lie between 0.0
  # and 1.0 and change dynamically during training.
  initial:
    arousal: Baseline arousal level in the range 0.0–1.0. Higher arousal
      increases neurogenesis rates.
    stress: Initial stress level in the range 0.0–1.0. Stress reduces
      plasticity and slows learning.
    reward: Starting reward signal in the range 0.0–1.0 that encourages
      reinforcement of successful pathways.
    emotion: Starting emotional state for the system. ``"neutral"`` is typical
      but any descriptive string is allowed.

memory_system:
  # Provides short-term memory in RAM and long-term memory on disk. The system
  # consolidates entries from the volatile store into the persistent file when
  # required.
  long_term_path: Path to the file used for persistent long-term storage of
    memories.
  threshold: When ``arousal`` or ``reward`` exceed this value the
    ``MemorySystem`` stores new entries directly in the long-term layer instead
    of the short-term layer. Typical values range from 0.3 to 0.7 depending on
    how aggressively long-term consolidation should occur.
  consolidation_interval: Number of operations between calls to
    ``consolidate``. Larger values delay writes to disk but reduce I/O.

hybrid_memory:
  # Optional long-range memory combining vector search with symbolic storage.
  vector_store_path: File path for storing dense embeddings of each memory
    entry. If omitted defaults to ``vector_store.pkl`` in the working
    directory.
  symbolic_store_path: File used to persist key/value pairs and timestamps.
    This mirrors the vector store so recalls can return the original data.
  kuzu_store_path: Optional path to a Kùzu graph database file used to persist
    all memory entries. When provided, both ``vector_store_path`` and
    ``symbolic_store_path`` are ignored and the hybrid memory stores nodes in
    the specified Kùzu database. Each entry becomes a node with the key, value,
    embedding vector and insertion timestamp as properties. The path must be
    distinct from the one used for the topology tracker so both databases can
    operate simultaneously without interference.
  max_entries: Maximum number of items retained in the vector and symbolic
    stores. When the count exceeds this limit the oldest entries are removed
    to keep memory usage bounded.

network:
  remote_client:
    # Configuration for forwarding parts of the brain to a remote server. When
    # ``offload_enabled`` is true the brain serializes a subcore and sends it to
    # this endpoint for processing.
    # The client first performs a ``/ping`` heartbeat to ensure the server is
    # reachable before attempting any offload requests.
    url: Base URL for the remote HTTP brain server.
    timeout: Seconds to wait for remote requests before they are aborted. Values
      between 5 and 30 are typical depending on network latency.
    max_retries: Number of times a remote operation is retried upon failure.
    backoff_factor: Base delay in seconds used for exponential backoff between
      retries. The actual wait time grows as ``backoff_factor * 2^n`` where
      ``n`` is the retry attempt index. With the default ``0.5`` the client waits
      0.5, 1.0, 2.0 … seconds between attempts. Increase this when interacting
      with slow or congested networks.
    track_latency: When true the client records the time taken for each
      request so average latency can be inspected through the API.
    auth_token: Optional token sent as an ``Authorization`` header on each
      request.
    ssl_verify: Whether HTTPS certificates should be verified. Set to ``false``
      when using self-signed certificates.
    connect_retry_interval: Seconds to wait between connection retry attempts.
    heartbeat_timeout: Maximum time to wait for a heartbeat response when
      maintaining persistent connections.
    use_compression: If ``true`` payloads are compressed before transmission.

  torrent_client:
    # Enables distribution of brain parts through the tracker network. Each client
    # processes assigned subcores asynchronously.
    client_id: Identifier used when registering with the torrent tracker. Must be
      unique across participating clients.
    buffer_size: Maximum number of asynchronous tasks queued for a torrent
      client. Larger values allow more parallel requests but use more memory.
    heartbeat_interval: Seconds between status pings sent to the tracker. Has no
      effect in the current simplified implementation but reserved for future use.

data_compressor:
  # Controls how strongly the DataCompressor reduces data size before it is
  # stored or transmitted.
  compression_level: Integer from 0 to 9 passed directly to ``zlib.compress``.
    Higher levels yield smaller output but require more CPU time. The default of
    6 balances speed and ratio for general use.
  compression_enabled: Set to ``false`` to bypass compression entirely. Useful
    during debugging or when working with already compressed data.
  delta_encoding: When ``true`` arrays are stored as differences between
    consecutive elements. This often yields dramatically higher compression
    ratios for smooth numeric data but slightly increases the cost of
    decompression. The first element is stored verbatim so the process remains
    lossless.
  compression_algorithm: Selects the compression backend. ``"zlib"`` offers
    quick general-purpose compression while ``"lzma"`` can provide higher ratios
    at the expense of CPU time. Set according to your network bandwidth and CPU
    availability.
  quantization_bits: If greater than ``0`` each array passed through the
    DataCompressor is uniformly quantized to the specified bit width before
    compression. Lower bit widths drastically reduce size at the cost of
    numerical precision. The same value is exposed as ``core.quantization_bits``
    and can be toggled via the ``--quantize`` CLI flag.
  sparse_threshold: Optional floating point value between ``0`` and ``1``. When
    set, arrays whose non-zero element fraction falls below this threshold are
    converted to ``scipy.sparse.csr_matrix`` prior to compression. This yields
    significant memory savings for synapse matrices or other large, mostly zero
    tensors.

dataloader:
  # Settings for the ``DataLoader`` which serializes and compresses data.
  # ``tensor_dtype`` defines the NumPy dtype used when encoding objects and
  # arrays. This affects the precision of the returned tensors and must be a
  # valid dtype name like ``"uint8"`` or ``"int16"``. Wider dtypes can prevent
  # overflow with exceptionally large payloads but increase memory usage.
  tensor_dtype: String specifying the dtype of encoded tensors.
  track_metadata: When ``true`` each encoded object is stored with metadata
    describing its original Python module and type. This ensures decoding
    faithfully recreates the value even when tokenization or compression is
    applied.
  enable_round_trip_check: Set to ``true`` to verify that decoding an encoded
    value returns an object equal to the original during training. If the
    objects differ the ``round_trip_penalty`` is added to the loss.
  round_trip_penalty: Floating point penalty applied when the round-trip check
    fails. Typical values range from ``0.1`` to ``1.0`` depending on how strongly
    you want to discourage irreversible transformations.
  tokenizer_type: Name of a built-in tokenizer from the ``tokenizers`` library
    such as ``bert_wordpiece`` or ``byte_level_bpe``. When set, the DataLoader
    converts all string inputs to token IDs using this tokenizer.
  tokenizer_json: Path to a tokenizer JSON file to load instead of using a
    built-in tokenizer. This allows custom vocabularies trained on your own
    dataset.
  tokenizer_vocab_size: Vocabulary size to use when training a tokenizer via
    YAML configuration. The parameter is ignored when ``tokenizer_json`` is
    provided.

experiment_tracker:
  # Optional integration with external experiment tracking services such as
  # Weights & Biases. When ``enabled`` is true a ``WandbTracker`` is created
  # and attached to the :class:`MetricsVisualizer` so that training metrics are
  # streamed to the configured project.
  enabled: Activate experiment tracking when set to ``true``.
  project: Name of the remote project used by the tracker.
  entity: Optional account or team under which the run is recorded.
  run_name: Custom name assigned to the run. If ``null`` Wandb generates one.

  remote_server:
    # Launches an optional local ``RemoteBrainServer`` so this MARBLE instance can
    # forward computation to another machine. When ``enabled`` is ``true`` the
    # server starts automatically using the provided ``host`` and ``port``. If
    # ``remote_url`` is specified the server itself forwards heavy requests to that
    # address, creating a chain of offload targets.
    enabled: Whether to start the server alongside the main process.
    host: Interface address to bind the HTTP server to. Usually ``"localhost"``
      for local testing.
    port: TCP port used by the server.
    remote_url: Optional URL of another remote brain server to which this server
      offloads work.
    auth_token: Optional shared secret required in the ``Authorization`` header of
      every request. When set, clients must supply ``"Bearer <token>"`` and the
      server compares it using constant-time logic to avoid timing attacks.
    ssl_enabled: Enables HTTPS with the provided certificate and key files.
    ssl_cert_file: Path to the SSL certificate used when ``ssl_enabled`` is true.
    ssl_key_file: Path to the private key for the certificate.
    max_connections: Maximum concurrent connections the server will accept.
    compression_level: Compression strength used when exchanging data with
      clients. The value is passed to ``zlib.compress`` and ranges from 0
      (no compression) to 9 (maximum compression).
    compression_enabled: Set to ``false`` to disable compression entirely. When
      disabled the server expects plain JSON payloads.
remote_hardware:
  tier_plugin: Import path of a module exposing ``get_remote_tier(cfg)``. When
    provided, MARBLE delegates heavy operations to the returned remote tier.
    Set to ``null`` to disable remote execution.
  grpc:
    address: Address of the gRPC service handling remote computation for the
      built-in ``GrpcRemoteTier`` implementation.
    max_retries: Maximum number of times failed gRPC calls are retried. This
      helps recover from transient disconnects or temporary GPU unavailability.
    backoff_factor: Base delay in seconds for exponential backoff between
      retries. Each failure waits ``backoff_factor * 2^n`` seconds before
      reconnecting, ensuring GPUs are released and network pressure is reduced.

metrics_visualizer:
  # Configure the live metrics plot size. These values are passed directly to
  # ``matplotlib`` when creating the figure.
  fig_width: Width of the metrics figure in inches.
  fig_height: Height of the metrics figure in inches.
  refresh_rate: How often the plot is refreshed in seconds.
  color_scheme: Name of the Matplotlib style to apply when rendering metrics.
  show_neuron_ids: If true the plot displays neuron identifiers next to data
    points.
  dpi: Resolution of the output figure in dots per inch.
  track_memory_usage: When true the visualizer records both system RAM and GPU
    memory consumption after each training epoch. This requires the ``psutil``
    package and uses ``torch.cuda`` when a CUDA-capable device is available.
  track_cpu_usage: When enabled the metrics visualizer stores the current CPU
    utilisation percentage at each update. This calls ``psutil.cpu_percent`` and
    allows post-training analysis of processing load.
  log_dir: Directory where TensorBoard event files are written. When set,
    you can run ``tensorboard --logdir=<log_dir>`` to monitor metrics.
  csv_log_path: Path to a CSV file receiving all metric values after each
    update. Useful for analysis with spreadsheet tools.
  json_log_path: Path to a JSON lines file where each update is appended as a
    single JSON object. This provides structured logs for downstream tools.
  anomaly_std_threshold: Number of standard deviations a new metric value must
    exceed the running mean to be flagged as an anomaly. When this threshold is
    crossed the event is printed and, if TensorBoard logging is enabled, written
    as a text entry. Set to ``0`` to disable anomaly detection.

metrics_dashboard:
  # Optional web dashboard built with Plotly Dash for real-time monitoring.
  # When enabled a small HTTP server serves a page that visualizes the same
  # metrics collected by ``MetricsVisualizer`` but updates automatically in the
  # browser. This is useful when running training remotely and you still want
  # to monitor progress live.
  enabled: Set to ``true`` to start the dashboard in a background thread.
  host: Interface address for the Dash server, usually ``"localhost"``.
  port: TCP port used by the dashboard. Ensure this port is free.
  update_interval: Milliseconds between refreshes of the dashboard graphs.
  window_size: Number of recent points used to compute the moving average shown
    on each graph. Larger windows smooth the curves more aggressively.

lobe_manager:
  # Parameters controlling how strongly neuron attention is adjusted when the
  # ``LobeManager`` performs self-attention.
  attention_increase_factor: Multiplier applied to neuron attention when a
    lobe's score is below average and the loss is positive. Values slightly above
    ``1.0`` encourage struggling lobes to contribute more.
  attention_decrease_factor: Multiplier used when a lobe's attention is above
    average but the loss is not improving. Numbers below ``1.0`` gradually reduce
    emphasis on those lobes.

autograd:
  enabled: Set to true to wrap the Brain with a transparent PyTorch autograd layer. When enabled, gradients from PyTorch operations are applied to MARBLE synapse weights without altering the underlying architecture.
  learning_rate: Step size used when the autograd layer applies gradient updates to synapse weights during backward passes. Typical values range from 0.001 to 0.1.
  gradient_accumulation_steps: Number of training examples to process before applying weight updates. MARBLE accumulates gradients from each call to ``train_example`` and only updates synapse weights once this many examples have been seen. This allows effectively larger batch sizes even when GPU memory is limited. Values between ``1`` and ``32`` are common.
global_workspace:
  # Central broadcast system shared by plugins and components. When enabled all
  # modules can publish short ``BroadcastMessage`` objects which are queued and
  # delivered to subscribed listeners. This acts as a simplified Global
  # Workspace for consciousness-inspired information exchange.
  enabled: Set to ``true`` to create the global workspace on startup.
  capacity: Maximum number of messages kept in the queue. When the queue
    exceeds this length older entries are discarded. Typical values range from
    ``10`` to ``100`` depending on how much history you wish to retain.
  # The workspace is automatically attached to a ``Neuronenblitz`` instance
  # when :func:`global_workspace.activate` is called with ``nb``.
attention_codelets:
  # Attention codelets are lightweight plugins that each propose a piece of
  # information along with a salience ``score``. During every cycle the
  # ``attention_codelets`` module forms a coalition of the top proposals and
  # broadcasts their ``content`` through the global workspace if it is active.
  enabled: Set to ``true`` to allow codelet proposals to be evaluated and
    broadcast.
  coalition_size: Number of top proposals broadcast each cycle when
    ``attention_codelets.run_cycle`` is invoked. A value of ``1`` means only the
    highest scoring proposal is shared. Typical values are between ``1`` and
    ``5`` depending on how much information you want to expose at once.
  # See ``examples/configs/attention_codelets.yaml`` for a minimal
  # configuration demonstrating these settings.
  # To create your own codelet define a callable that returns an
  # :class:`AttentionProposal` and register it with
  # :func:`attention_codelets.register_codelet`.
pytorch_challenge:
  enabled: If true the training loop compares MARBLE with a pretrained PyTorch model after every training example. When MARBLE's validation loss, inference speed or model size exceed the PyTorch model, neuromodulatory stress is increased which lowers plasticity on subsequent updates.
  loss_penalty: Amount of stress added when MARBLE's loss is worse than the PyTorch baseline. Values around 0.1 provide noticeable pressure without overwhelming the system.
  speed_penalty: Stress increment applied when MARBLE's inference time is slower than the baseline model. Setting this near 0.1 encourages optimisations to execution paths.
  size_penalty: Stress increment when MARBLE grows larger than the baseline model in megabytes. This guides synaptic pruning and structural plasticity to favour compactness.

gpt:
  # The GPT component enables MARBLE to build and train a transformer-based language model entirely from scratch. When
  # `enabled` is true a small GPT architecture is initialised using the parameters below and trained on token sequences
  # provided by the user. This allows experimentation with generative text models without relying on any pretrained weights.
  enabled: Set to true to activate GPT training within MARBLE.
  vocab_size: Size of the discrete token vocabulary used when constructing the embedding layers. Must be greater than one.
  block_size: Number of tokens processed at once. Sequences longer than this are split into blocks. Typical values range
    from 8 to 128 for lightweight experiments.
  num_layers: Number of transformer encoder layers stacked in the GPT. Increasing this improves modelling power but also
    memory and compute requirements. Values between 2 and 8 are common for small models.
  num_heads: Quantity of attention heads per layer. More heads allow the model to capture diverse relationships between
    tokens. Small configurations often use 2 to 8 heads.
  hidden_dim: Dimensionality of the token and positional embeddings as well as the hidden size of each transformer layer.
    Higher values provide more capacity but also increase memory usage.
  learning_rate: Step size for the Adam optimiser when training the GPT model. Start with around 0.001 and adjust based
    on observed convergence.
  num_train_steps: Number of update steps performed during GPT training. Keeping this low (tens to hundreds) ensures test
    runs complete quickly while still demonstrating learning behaviour.
  dataset_path: Path to a plain text file used to generate training sequences. Characters are read from the file and
    mapped to token IDs until ``vocab_size`` unique tokens are collected. The file must contain at least ``block_size``
    characters so that each training example can include a start and target token. Using larger files improves vocabulary
    coverage and allows more diverse sequences.
  batch_size: Number of sequences processed before each optimisation step. Batches between 1 and 64 are typical. Larger
    values yield smoother gradient estimates but require more memory, especially when running on GPU with CuPy.

distillation:
  # Knowledge distillation allows training a "student" brain using guidance from
  # a pre-trained "teacher" brain. When enabled the student blends the teacher's
  # predictions with the ground truth targets before performing regular training
  # updates. This is useful for compressing a large model into a smaller one or
  # for transferring knowledge between different MARBLE instances.
  enabled: Set to ``true`` to activate distillation during training.
  alpha: Weighting factor between the true target and the teacher's output when
    computing the blended target value. ``0.0`` relies solely on the ground
    truth while ``1.0`` exactly matches the teacher. Typical values range from
    ``0.3`` to ``0.7`` depending on how strongly the teacher should influence
    learning.
  teacher_model: Path to a pickled MARBLE system used as the teacher. If ``null``
    the caller must provide the teacher programmatically when invoking the
    distillation trainer.

reinforcement_learning:
  enabled: Activate training in a reinforcement learning loop using the MARBLE
    core and Neuronenblitz. Set to ``true`` to begin episodic interaction with
    the environment defined in ``reinforcement_learning.py``.
  algorithm: Selects the RL algorithm implementation. ``"q_learning"`` runs a
    tabular Q-learning agent while ``"policy_gradient"`` trains a small
    differentiable policy network integrated with MARBLE via
    ``MarblePolicyGradientAgent``.
  episodes: Number of training episodes to run. More episodes generally yield
    better policies but increase computation time.
  max_steps: Maximum steps per episode before resetting the environment.
  discount_factor: Discount applied to future rewards when computing Q-values.
  epsilon_start: Initial exploration rate for the epsilon-greedy policy.
  epsilon_decay: Multiplicative decay applied to ``epsilon`` after each update.
  epsilon_min: Lowest exploration rate allowed once ``epsilon`` has decayed.
  seed: Integer random seed for reproducible training. Set to ``null`` to allow
    nondeterministic runs.
  double_q: Enable Double Q-learning which maintains two separate Q-tables.
    During updates one table selects the greedy action while the other
    evaluates it. This reduces overestimation bias at the cost of extra memory.

contrastive_learning:
  # Self-supervised contrastive representation learning using the
  # ``ContrastiveLearner``. Two augmented views of each sample are
  # processed by Neuronenblitz and the InfoNCE objective adjusts
  # synaptic weights via ``apply_weight_updates_and_attention``.
  enabled: Set to ``true`` to run contrastive learning after standard
    training. When enabled the ``contrastive_train`` method of
    Neuronenblitz processes the dataset according to the parameters
    below.
  temperature: Softmax temperature applied to similarity scores when
    computing the InfoNCE loss. Lower values sharpen the distribution.
    Typical values range from ``0.1`` to ``1.0``.
  epochs: Number of passes over the dataset for contrastive learning.
    Each epoch iterates over the inputs in mini-batches of size
    ``batch_size`` and updates weights after every batch.
  batch_size: Number of original samples to include in each batch. Each
    sample produces two augmented views, so the effective batch size for
    the loss is ``2 * batch_size``. Values between ``2`` and ``32`` work
    well depending on available memory.

hebbian_learning:
  # Unsupervised Hebbian update rule integrated with Neuronenblitz.
  # After each wander the weights along the traversed path are
  # adjusted based on the product of pre- and post-synaptic values.
  # ``learning_rate`` scales the Hebbian update and ``weight_decay``
  # applies an L2 style penalty to prevent unbounded growth.
  learning_rate: Positive step size controlling how strongly synapse
    weights are increased when correlated neuron activations are
    observed. Typical values range from ``0.001`` to ``0.1``.
  weight_decay: Fraction of the current weight subtracted during each
    update. Helps stabilise learning when set between ``0.0`` and
    ``0.01``.

adversarial_learning:
  # Generative adversarial training using two Neuronenblitz networks that
  # share the same Core. The generator receives random noise and tries
  # to produce values that the discriminator classifies as real.
  enabled: Set to ``true`` to run adversarial updates after the standard
    training loop. Both generator and discriminator are updated using
    ``apply_weight_updates_and_attention``.
  epochs: Number of passes over the provided real values. Values between
    ``1`` and ``5`` work well for small experiments.
  batch_size: Number of real samples processed before generator and
    discriminator weights are updated. Typical range is ``4`` to ``64``.
  noise_dim: Dimensionality of the random noise vector fed to the generator.
    Higher values allow more variation but increase compute. Values of
    ``1`` to ``10`` are common.

autoencoder_learning:
  # Denoising autoencoder that trains Neuronenblitz to reconstruct noisy
  # inputs. After each wander the reconstruction error is applied through
  # ``apply_weight_updates_and_attention``.
  enabled: Activate training for the autoencoder module.
  epochs: Number of times the dataset is presented. Typical values are
    ``1`` to ``10`` depending on dataset size.
  batch_size: Number of samples processed before weight updates. Values of
    ``4`` to ``64`` work well in practice.
  noise_std: Standard deviation of Gaussian noise added to inputs. A range
    from ``0.0`` (no noise) to ``0.5`` encourages robust representations.
  noise_decay: Multiplicative factor applied to ``noise_std`` after each epoch.
    Values below ``1.0`` gradually reduce corruption as training progresses,
    allowing the model to focus on fine-grained reconstruction.

semi_supervised_learning:
  # Semi-supervised learning using consistency regularization. Each step
  # processes a labeled example and an unlabeled input. The network first
  # performs a standard supervised update with the labeled pair. It then
  # processes the unlabeled input twice with independent wander paths and
  # adjusts weights to minimise the difference between the two outputs.
  enabled: Set to ``true`` to enable semi-supervised updates during training.
  epochs: Number of passes over the combined labeled and unlabeled datasets.
  batch_size: Count of labeled/unlabeled pairs processed before weight updates.
  unlabeled_weight: Scaling factor applied to the consistency loss derived from
    the two unlabeled outputs. Values between ``0.1`` and ``1.0`` control how
    strongly the unlabeled data influences learning.

federated_learning:
  # Federated averaging trainer that coordinates multiple clients.
  # Each round trains all local Neuronenblitz networks on their
  # respective datasets before averaging synapse weights.
  enabled: Enable federated training when set to ``true``.
  rounds: Number of communication rounds performed.
  local_epochs: Epochs of local training executed on each client per round.

curriculum_learning:
  # Progressive training strategy that orders examples by difficulty.
  # The learner introduces harder examples as training progresses to
  # stabilise optimisation and accelerate convergence.
  enabled: Enable curriculum learning when ``true``.
  epochs: Number of passes over the dataset with increasing difficulty.
  schedule: Determines how quickly more difficult samples are added. Use
    ``linear`` for a steady increase or ``exponential`` to focus on easy
    examples longer.
meta_learning:
  # Reptile-style meta learning for rapid adaptation across tasks.
  # Each task contains input-target pairs. The learner clones the
  # current Core and Neuronenblitz, performs ``inner_steps`` of
  # training on each task and then moves the main network weights
  # toward the average of the task-specific weights using ``meta_lr``.
  enabled: Set to ``true`` to activate meta learning during training.
  epochs: Number of meta-training iterations over the provided tasks.
  inner_steps: Training steps executed inside each temporary clone
    before the meta update is applied. Typically between 1 and 5.
  meta_lr: Step size used when blending task weights back into the
    primary network. Values around ``0.1`` work well for most setups.
  distill_alpha: Weight for self-distillation loss. Set between ``0.0`` and
    ``1.0`` to blend the KL divergence between current and previous epoch
    logits into the training objective. ``0.0`` disables distillation while
    values like ``0.1``–``0.5`` encourage gradual alignment of predictions
    over time.
transfer_learning:
  # Fine-tune a pretrained model while keeping a portion of synapses fixed.
  # ``freeze_fraction`` controls what percentage of synapses are marked as
  # frozen before training begins. Frozen synapses do not receive weight
  # updates in ``apply_weight_updates_and_attention``.
  enabled: Set to ``true`` to run transfer learning updates.
  epochs: Number of fine-tuning passes over the new dataset.
  batch_size: Mini-batch size used during fine-tuning steps.
  freeze_fraction: Fraction between ``0.0`` and ``1.0`` specifying the portion
    of synapses that remain unchanged.

continual_learning:
  # Replay-based continual learning that stores a limited set of
  # previous examples and replays them during training on new data.
  # This mitigates catastrophic forgetting when tasks arrive
  # sequentially without requiring complex weight consolidation.
  enabled: Set to ``true`` to activate continual learning.
  epochs: Number of passes over each incoming dataset. Typically
    ``1`` or ``2`` is sufficient when replay memory is used.
  memory_size: Maximum number of examples retained for replay. Larger
    values offer better retention of old tasks but increase memory
    usage. Reasonable ranges are between ``10`` and ``100``.

imitation_learning:
  # Behaviour cloning of expert demonstrations using the ``ImitationLearner``.
  # Each demonstration contains an input value and the corresponding action
  # taken by the expert. The learner replays these demonstrations and
  # adjusts synaptic weights so that Neuronenblitz reproduces the expert's
  # behaviour for the same inputs.
  enabled: Enable imitation learning updates. Set to ``true`` to process
    recorded demonstrations after normal training.
  epochs: Number of passes over the stored demonstrations. Values between
    ``1`` and ``10`` work well for small datasets.
  max_history: Maximum number of demonstration pairs kept in memory. Older
    entries are removed once this limit is exceeded.


harmonic_resonance_learning:
  # Experimental paradigm that embeds inputs as sinusoidal phases.
  # Each step encodes the current value using ``base_frequency`` and updates
  # ``base_frequency`` by multiplying it with ``decay`` after training.
  enabled: Enable harmonic resonance learning when set to ``true``.
  epochs: Number of passes over the dataset. Usually ``1`` or ``2`` is enough
    for quick experiments.
  base_frequency: Initial frequency used when computing sine and cosine
    representations. Values around ``1.0`` keep phase changes gradual while
    larger numbers increase oscillation speed.
  decay: Multiplicative factor applied to ``base_frequency`` after each
    training step. ``0.99`` slowly lowers the frequency over time.

synaptic_echo_learning:
  # Uses the echo buffers stored on each synapse to modulate updates.
  # Weight changes are scaled by the average echoed activity.
  enabled: Activate the synaptic echo learner when ``true``.
  epochs: Number of training epochs to run. Usually ``1`` or ``2`` is
    sufficient for simple experiments.
  echo_influence: Multiplier applied to the error signal before updating
    weights. Larger values amplify the effect of echoed activations.

fractal_dimension_learning:
  # Adjusts the representation size by estimating the fractal dimension of
  # neuron embeddings. When the dimension exceeds ``target_dimension`` a new
  # element is appended to each representation via ``Core.increase_representation_size``.
  enabled: Enable fractal dimension learning when ``true``.
  epochs: Number of passes over the dataset used to refine the estimate.
  target_dimension: Threshold for the estimated dimension. Values between
    2.0 and 6.0 work well for small graphs.

quantum_flux_learning:
  # Experimental rule associating a phase with every synapse. Weight updates
  # are scaled by ``sin(phase)`` and the phase drifts by ``phase_rate``.
  enabled: Activate quantum flux learning when set to ``true``.
  epochs: Number of epochs using phase-modulated updates.
  phase_rate: Amount added to each synapse's phase after each update.

dream_reinforcement_learning:
  # Combines regular reinforcement updates with short dream rollouts. After
  # each real step ``dream_cycles`` additional wanders run using the last
  # output as input and apply errors scaled by ``dream_strength``.
  enabled: Enable dream reinforcement learning when ``true``.
  episodes: Number of training episodes consisting of input-target pairs.
  dream_cycles: How many dream updates follow each real step.
  dream_strength: Multiplier applied to dream errors before weight updates.
  dream_interval: Number of real wander steps between dream batches. ``1`` means
    dream after every wander cycle; higher values postpone dreaming. Values must
    be positive integers. Typical range: 1–10.
  dream_cycle_duration: Optional pause inserted after each dream step expressed
    in seconds (fractional values allow millisecond precision). When ``null`` no
    additional delay is added and dream steps take their natural execution time.

continuous_weight_field_learning:
  # Optimises a weight vector ``W(x)`` that varies smoothly with the input ``x``.
  # The weight field is represented with radial basis functions whose centres
  # span the input space. For each training sample MARBLE uses Neuronenblitz to
  # produce a feature representation ``phi(x)`` from the core, then predicts
  # ``phi(x) \cdot W(x)``. Weight field coefficients are updated via gradient
  # descent with optional L2 regularisation.
  enabled: Enable continuous weight field learning when set to ``true``.
  epochs: Number of passes over the dataset to fit the field.
  num_basis: Number of radial basis functions forming the field.
  bandwidth: Standard deviation controlling basis width. Larger values make
    the field smoother across inputs.
  reg_lambda: Strength of L2 regularisation on the basis coefficients.
  learning_rate: Step size for gradient descent updates of the field weights.

neural_schema_induction:
  # Structural learner that grows new "schema" neurons representing frequent
  # reasoning patterns discovered during dynamic wanders. When a subsequence
  # of neuron activations appears in at least ``support_threshold`` examples,
  # the learner adds a new neuron connected to all participants in that
  # sequence.
  enabled: Enable neural schema induction when set to ``true``.
  epochs: Number of passes over the dataset used for pattern collection.
  support_threshold: Minimum number of occurrences before creating a schema.
  max_schema_size: Maximum length of activation patterns considered.

conceptual_integration:
  # Creates new "concept" neurons by blending the representations of
  # two dissimilar active neurons. This structural learner relies on
  # Neuronenblitz activation traces and does not use any gradient optimisation.
  enabled: When ``true`` the learner periodically attempts concept blending.
  blend_probability: Chance of triggering a blend at each training step.
  similarity_threshold: Maximum cosine similarity allowed between parent
    neurons for blending. Lower values encourage more diverse concepts.

n_dimensional_topology:
  # Dynamically expands the representation size when learning stalls.
  # A self-attention score compared with ``attention_threshold`` decides whether
  # to add a new dimension. If the validation loss fails to improve by
  # ``loss_improve_threshold`` within ``stagnation_epochs`` epochs, the extra
  # dimension is removed and all neuron representations are truncated.
  enabled: Turn on n-dimensional growth when ``true``.
  target_dimensions: Upper bound on representation size.
  attention_threshold: Minimum self-attention required before trying to grow.
  loss_improve_threshold: Relative loss decrease needed to keep the new dimension.
  stagnation_epochs: Number of stagnant epochs before another dimension may be added.

unified_learning:
  # The UnifiedLearner coordinates all paradigms using a gating network.
  # At each step it constructs a context vector summarising memory usage,
  # plasticity threshold, number of neurons and recent losses. A small neural
  # network turns this context into softmax weights over the learners. The
  # weights modulate ``neuronenblitz.plasticity_modulation`` so each paradigm's
  # updates are scaled appropriately. When ``log_path`` is set every decision is
  # appended to that JSONL file for later inspection.
  enabled: When ``true`` the meta-controller runs during training.
  gating_hidden: Number of hidden units in the gating network. Typical values
    range from 4--32 depending on how many context features are used.
  log_path: File path for the decision log. Set to ``null`` to disable logging.

theory_of_mind:
  hidden_size: Size of the LSTM hidden state used to model each character. Larger
    values capture more nuanced behaviours but use more memory.
  num_layers: Number of stacked recurrent layers. Increasing this improves
    modelling capacity at a computational cost.
  prediction_horizon: How many future steps are predicted when observations are
    not supplied. Small values like ``1`` or ``2`` are typical for short-term
    planning.
  memory_slots: Number of key-value slots available for storing beliefs about
    other agents. Each slot holds an encoded key and value vector of size
    ``hidden_size``. Larger capacities retain more beliefs but consume more
    memory.
  attention_hops: How many sequential attention passes are used when querying
    belief memory. More hops allow complex reasoning chains but increase
    compute time. Values between ``1`` and ``3`` balance accuracy and speed.
  mismatch_threshold: Mean squared error threshold between retrieved belief and
    provided belief value that triggers mismatch logging. Lower values record
    smaller discrepancies at the cost of more frequent logs. Typical range is
    ``0.05`` to ``1.0``.

predictive_coding:
  num_layers: Number of predictive coding layers arranged hierarchically.
    Each layer refines the residual error of the previous one. ``1`` to ``4``
    layers are usually sufficient.
  latent_dim: Dimensionality of the latent state per layer. Choose between ``8``
    and ``64`` depending on task complexity.
  learning_rate: Optimizer step size used to minimise prediction error.

experiments:
  # Allows grouping multiple experiment configurations in a single YAML file.
  # Each entry overrides top-level sections for a particular run while sharing
  # the remaining defaults. Use this to sweep over different core sizes or
  # learning rates without duplicating the entire configuration.
  - name: Arbitrary experiment name used for logging and checkpoints.
    core: Subsection containing parameter overrides for the Core.
    neuronenblitz: Subsection with overrides for Neuronenblitz parameters.
