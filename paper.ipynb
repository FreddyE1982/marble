{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MARBLE: Memory-Adaptive Recursive Binary Learning Engine\n",
    "\n",
    "## Abstract\n",
    "MARBLE represents a novel approach to neural network architecture that dynamically adapts its memory usage across VRAM, RAM, and disk storage tiers. This system implements a unique neuron wandering mechanism combined with backtracking and consolidation strategies, enabling efficient processing of large-scale models like Stable Diffusion while maintaining memory efficiency.\n",
    "\n",
    "## 1. Introduction\n",
    "Neural networks traditionally operate within fixed memory constraints. MARBLE introduces a paradigm shift by implementing a three-tier memory system with dynamic resource allocation. The system's core innovation lies in its ability to maintain performance while seamlessly moving computations between memory tiers.\n",
    "\n",
    "### 1.1 Dependencies Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "!pip install cupy-cuda12x torch torchvision transformers diffusers datasets tqdm numpy pillow av==10.0.0 accelerate safetensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Core Architecture\n",
    "The foundation of MARBLE consists of adaptive neurons and synapses that form the basic computational units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "import zlib\n",
    "import os\n",
    "import sympy as sp\n",
    "import threading\n",
    "import time\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from datasets import load_dataset\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, nid, value=0.0, tier='vram'):\n",
    "        self.id = nid\n",
    "        self.value = value\n",
    "        self.tier = tier\n",
    "        self.synapses = []\n",
    "        self.formula = None\n",
    "\n",
    "class Synapse:\n",
    "    def __init__(self, source, target, weight=1.0):\n",
    "        self.source = source\n",
    "        self.target = target\n",
    "        self.weight = weight\n",
    "        self.potential = 1.0"
   ]
  }"
,
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Mandelbrot Initialization\n",
    "We use the Mandelbrot set for initial neural pattern generation, providing a rich foundation for complex computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def compute_mandelbrot(xmin, xmax, ymin, ymax, width, height, max_iter=256):\n",
    "    x = cp.linspace(xmin, xmax, width)\n",
    "    y = cp.linspace(ymin, ymax, height)\n",
    "    X, Y = cp.meshgrid(x, y)\n",
    "    C = X + 1j * Y\n",
    "    Z = cp.zeros_like(C, dtype=cp.complex64)\n",
    "    mandelbrot = cp.zeros(C.shape, dtype=cp.int32)\n",
    "    for i in range(max_iter):\n",
    "        mask = cp.abs(Z) <= 2\n",
    "        Z[mask] = Z[mask] * Z[mask] + C[mask]\n",
    "        mandelbrot[mask] = i\n",
    "    return mandelbrot\n",
    "\n",
    "class Core:\n",
    "    def __init__(self, params, formula=None, formula_num_neurons=100):\n",
    "        self.params = params\n",
    "        self.vram_limit_mb = params.get('vram_limit_mb', 100)\n",
    "        self.ram_limit_mb = params.get('ram_limit_mb', 500)\n",
    "        self.disk_limit_mb = params.get('disk_limit_mb', 10000)\n",
    "        \n",
    "        self.neurons = []\n",
    "        self.synapses = []\n",
    "        nid = 0\n",
    "\n",
    "        if formula is not None:\n",
    "            try:\n",
    "                expr = sp.sympify(formula, evaluate=False)\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"Formula parsing error: {e}\")\n",
    "            \n",
    "            for i_val in range(formula_num_neurons):\n",
    "                neuron = Neuron(nid, value=0.0, tier='vram')\n",
    "                neuron.formula = expr\n",
    "                self.neurons.append(neuron)\n",
    "                nid += 1\n",
    "        else:\n",
    "            mandel_gpu = compute_mandelbrot(\n",
    "                params['xmin'], params['xmax'],\n",
    "                params['ymin'], params['ymax'],\n",
    "                params['width'], params['height'],\n",
    "                params.get('max_iter', 256)\n",
    "            )\n",
    "            mandel_cpu = cp.asnumpy(mandel_gpu)\n",
    "            for val in mandel_cpu.flatten():\n",
    "                self.neurons.append(Neuron(nid, value=float(val), tier='vram'))\n",
    "                nid += 1\n",
    "\n",
    "        num_neurons = len(self.neurons)\n",
    "        for i in range(num_neurons - 1):\n",
    "            weight = random.uniform(0.5, 1.5)\n",
    "            syn = Synapse(self.neurons[i].id, self.neurons[i+1].id, weight)\n",
    "            self.neurons[i].synapses.append(syn)\n",
    "            self.synapses.append(syn)\n",
    "\n",
    "    def get_usage_by_tier(self, tier):\n",
    "        neurons_in_tier = [n for n in self.neurons if n.tier == tier]\n",
    "        synapses_in_tier = [s for s in self.synapses if self.neurons[s.source].tier == tier]\n",
    "        usage_bytes = len(neurons_in_tier) * 32 + len(synapses_in_tier) * 16\n",
    "        return usage_bytes / (1024 * 1024)\n",
    "\n",
    "    def check_memory_usage(self):\n",
    "        usage_vram = self.get_usage_by_tier('vram')\n",
    "        usage_ram  = self.get_usage_by_tier('ram')\n",
    "        usage_disk = self.get_usage_by_tier('disk')\n",
    "        print(f\"Memory usage - VRAM: {usage_vram:.2f} MB, RAM: {usage_ram:.2f} MB, Disk: {usage_disk:.2f} MB\")\n",
    "\n",
    "    def get_detailed_status(self):\n",
    "        status = {}\n",
    "        for tier in ['vram', 'ram', 'disk']:\n",
    "            neurons_in_tier = [n for n in self.neurons if n.tier == tier]\n",
    "            synapses_in_tier = [s for s in self.synapses if self.neurons[s.source].tier == tier]\n",
    "            status[tier] = {\n",
    "                'neuron_count': len(neurons_in_tier),\n",
    "                'synapse_count': len(synapses_in_tier),\n",
    "                'memory_mb': self.get_usage_by_tier(tier)\n",
    "            }\n",
    "        return status\n",
    "\n",
    "    def expand(self, num_new_neurons=10, num_new_synapses=15, alternative_connection_prob=0.1):\n",
    "        usage_vram = self.get_usage_by_tier('vram')\n",
    "        usage_ram  = self.get_usage_by_tier('ram')\n",
    "        new_tier = 'vram' if usage_vram < self.vram_limit_mb else 'ram' if usage_ram < self.ram_limit_mb else 'disk'\n",
    "        \n",
    "        start_id = len(self.neurons)\n",
    "        for i in range(num_new_neurons):\n",
    "            self.neurons.append(Neuron(start_id + i, value=0.0, tier=new_tier))\n",
    "        for _ in range(num_new_synapses):\n",
    "            src = random.choice(self.neurons).id\n",
    "            tgt = random.choice(self.neurons).id\n",
    "            if src != tgt:\n",
    "                syn = Synapse(src, tgt, weight=random.uniform(0.1, 1.0))\n",
    "                self.neurons[src].synapses.append(syn)\n",
    "                self.synapses.append(syn)\n",
    "        print(f\"Core expanded: {num_new_neurons} neurons in {new_tier} and {num_new_synapses} synapses added.\")\n",
    "        self.check_memory_usage()"
   ]
  }"

,
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data Management and Neural Path-Finding\n",
    "The DataLoader handles data serialization and compression, while Neuronenblitz implements the dynamic neural path-finding algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class DataLoader:\n",
    "    def encode(self, data):\n",
    "        serialized = pickle.dumps(data)\n",
    "        compressed = zlib.compress(serialized)\n",
    "        tensor = np.frombuffer(compressed, dtype=np.uint8)\n",
    "        return tensor\n",
    "\n",
    "    def decode(self, tensor):\n",
    "        compressed = tensor.tobytes()\n",
    "        serialized = zlib.decompress(compressed)\n",
    "        data = pickle.loads(serialized)\n",
    "        return data\n",
    "\n",
    "    def load_data(self, input_data, output_data=None):\n",
    "        input_tensor = self.encode(input_data)\n",
    "        if output_data is not None:\n",
    "            output_tensor = self.encode(output_data)\n",
    "            return input_tensor, output_tensor\n",
    "        return input_tensor\n",
    "\n",
    "class Neuronenblitz:\n",
    "    def __init__(self, core, wander_steps=5, learning_rate=0.01,\n",
    "                 backtrack_probability=0.3,\n",
    "                 consolidation_probability=0.2,\n",
    "                 consolidation_strength=1.1,\n",
    "                 route_potential_increase=0.5,\n",
    "                 route_potential_decay=0.9,\n",
    "                 route_visit_decay_interval=10,\n",
    "                 alternative_connection_prob=0.1):\n",
    "        self.core = core\n",
    "        self.wander_steps = wander_steps\n",
    "        self.learning_rate = learning_rate\n",
    "        self.backtrack_probability = backtrack_probability\n",
    "        self.consolidation_probability = consolidation_probability\n",
    "        self.consolidation_strength = consolidation_strength\n",
    "        self.route_potential_increase = route_potential_increase\n",
    "        self.route_potential_decay = route_potential_decay\n",
    "        self.route_visit_decay_interval = route_visit_decay_interval\n",
    "        self.alternative_connection_prob = alternative_connection_prob\n",
    "        \n",
    "        self.training_history = []\n",
    "        self.global_activation_count = 0\n",
    "\n",
    "    def relu(self, x):\n",
    "        return x if x > 0 else 0\n",
    "\n",
    "    def reset_neuron_values(self):\n",
    "        for neuron in self.core.neurons:\n",
    "            neuron.value = 0.0\n",
    "\n",
    "    def weighted_choice(self, synapses):\n",
    "        total = sum(syn.potential for syn in synapses)\n",
    "        r = random.uniform(0, total)\n",
    "        upto = 0\n",
    "        for syn in synapses:\n",
    "            upto += syn.potential\n",
    "            if upto >= r:\n",
    "                return syn\n",
    "        return random.choice(synapses)\n",
    "\n",
    "    def dynamic_wander(self, input_value):\n",
    "        self.reset_neuron_values()\n",
    "        vram_neurons = [n for n in self.core.neurons if n.tier == 'vram']\n",
    "        entry_neuron = random.choice(vram_neurons) if vram_neurons else random.choice(self.core.neurons)\n",
    "        entry_neuron.value = input_value\n",
    "        current_neuron = entry_neuron\n",
    "        path = [(current_neuron, None)]\n",
    "        steps_taken = 0\n",
    "\n",
    "        while steps_taken < self.wander_steps:\n",
    "            if not current_neuron.synapses:\n",
    "                if len(path) > 1:\n",
    "                    back_steps = random.randint(1, min(2, len(path) - 1))\n",
    "                    current_neuron, _ = path[-(back_steps + 1)]\n",
    "                    path = path[:-back_steps]\n",
    "                    continue\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            if random.random() < self.backtrack_probability and len(path) > 1:\n",
    "                back_steps = random.randint(1, min(2, len(path) - 1))\n",
    "                current_neuron, _ = path[-(back_steps + 1)]\n",
    "                path = path[:-back_steps]\n",
    "                continue\n",
    "\n",
    "            syn = self.weighted_choice(current_neuron.synapses)\n",
    "            next_neuron = self.core.neurons[syn.target]\n",
    "            transmitted_value = self.relu(current_neuron.value * syn.weight)\n",
    "            next_neuron.value = transmitted_value\n",
    "            path.append((next_neuron, syn))\n",
    "            syn.potential += self.route_potential_increase\n",
    "            current_neuron = next_neuron\n",
    "            steps_taken += 1\n",
    "\n",
    "        output_value = current_neuron.value\n",
    "        synapse_path = [s for (_, s) in path if s is not None]\n",
    "        self.global_activation_count += 1\n",
    "        if self.global_activation_count % self.route_visit_decay_interval == 0:\n",
    "            for syn in self.core.synapses:\n",
    "                syn.potential *= self.route_potential_decay\n",
    "        return output_value, synapse_path\n",
    "\n",
    "    def train_example(self, input_value, target_value):\n",
    "        output_value, path = self.dynamic_wander(input_value)\n",
    "        error = target_value - output_value\n",
    "        for syn in path:\n",
    "            source_value = self.core.neurons[syn.source].value\n",
    "            syn.weight += self.learning_rate * error * source_value\n",
    "            if random.random() < self.consolidation_probability:\n",
    "                syn.weight *= self.consolidation_strength\n",
    "        self.training_history.append({\n",
    "            'input': input_value,\n",
    "            'target': target_value,\n",
    "            'output': output_value,\n",
    "            'error': error,\n",
    "            'path_length': len(path)\n",
    "        })\n",
    "        return output_value, error, path\n",
    "\n",
    "    def train(self, examples, epochs=1):\n",
    "        for epoch in range(epochs):\n",
    "            epoch_errors = []\n",
    "            for input_val, target_val in examples:\n",
    "                output, error, path = self.train_example(input_val, target_val)\n",
    "                epoch_errors.append(abs(error))\n",
    "            avg_error = sum(epoch_errors) / len(epoch_errors) if epoch_errors else 0\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Average Error: {avg_error:.4f}\")\n",
    "            if avg_error > 0.1:\n",
    "                self.core.expand(alternative_connection_prob=self.alternative_connection_prob)\n",
    "            self.core.synapses = [s for s in self.core.synapses if abs(s.weight) >= 0.05]\n",
    "    \n",
    "    def get_training_history(self):\n",
    "        return self.training_history"
   ]
  }"
,
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Model Management and Neural Network Conversion\n",
    "The Brain class provides high-level model management while MarbleConverter enables integration with standard neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class Brain:\n",
    "    def __init__(self, core, neuronenblitz, dataloader, save_threshold=0.05, max_saved_models=5, save_dir=\"saved_models\", firing_interval_ms=None):\n",
    "        self.core = core\n",
    "        self.neuronenblitz = neuronenblitz\n",
    "        self.dataloader = dataloader\n",
    "        self.save_threshold = save_threshold\n",
    "        self.max_saved_models = max_saved_models\n",
    "        self.save_dir = save_dir\n",
    "        self.firing_interval_ms = firing_interval_ms\n",
    "        self.auto_fire_thread = None\n",
    "        self.auto_fire_active = False\n",
    "        \n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        self.best_validation_loss = float('inf')\n",
    "        self.saved_model_paths = []\n",
    "\n",
    "    def train(self, train_examples, epochs=1, validation_examples=None):\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"\\n--- Training Epoch {epoch+1} ---\")\n",
    "            self.neuronenblitz.train(train_examples, epochs=1)\n",
    "            if validation_examples is not None:\n",
    "                val_loss = self.validate(validation_examples)\n",
    "                print(f\"Validation Loss after Epoch {epoch+1}: {val_loss:.4f}\")\n",
    "                self.validate_and_save(validation_examples)\n",
    "                self.display_live_status(validation_examples)\n",
    "\n",
    "    def validate(self, validation_examples):\n",
    "        errors = []\n",
    "        for input_val, target_val in validation_examples:\n",
    "            output, _ = self.neuronenblitz.dynamic_wander(input_val)\n",
    "            errors.append(abs(target_val - output))\n",
    "        mean_val_loss = sum(errors) / len(errors) if errors else 0\n",
    "        print(f\"Mean Validation Loss: {mean_val_loss:.4f}\")\n",
    "        return mean_val_loss\n",
    "\n",
    "    def save_model(self):\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"brain_{timestamp}.pkl\"\n",
    "        filepath = os.path.join(self.save_dir, filename)\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            pickle.dump({\n",
    "                'core': self.core,\n",
    "                'neuronenblitz': self.neuronenblitz\n",
    "            }, f)\n",
    "        self.saved_model_paths.append(filepath)\n",
    "        if len(self.saved_model_paths) > self.max_saved_models:\n",
    "            old_file = self.saved_model_paths.pop(0)\n",
    "            os.remove(old_file)\n",
    "        print(f\"Model saved to {filepath}\")\n",
    "\n",
    "    def validate_and_save(self, validation_examples):\n",
    "        mean_val_loss = self.validate(validation_examples)\n",
    "        if self.best_validation_loss - mean_val_loss >= self.save_threshold:\n",
    "            self.best_validation_loss = mean_val_loss\n",
    "            self.save_model()\n",
    "        return mean_val_loss\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "            self.core = data['core']\n",
    "            self.neuronenblitz = data['neuronenblitz']\n",
    "        print(f\"Model loaded from {filepath}\")\n",
    "\n",
    "    def start_auto_firing(self, firing_interval_ms=None, input_generator=None):\n",
    "        if firing_interval_ms is not None:\n",
    "            self.firing_interval_ms = firing_interval_ms\n",
    "        if self.firing_interval_ms is None:\n",
    "            raise ValueError(\"Firing interval in milliseconds must be specified!\")\n",
    "        \n",
    "        self.auto_fire_active = True\n",
    "        def auto_fire_loop():\n",
    "            while self.auto_fire_active:\n",
    "                if input_generator is not None:\n",
    "                    input_value = input_generator()\n",
    "                else:\n",
    "                    input_value = random.uniform(0.0, 1.0)\n",
    "                output_value, path = self.neuronenblitz.dynamic_wander(input_value)\n",
    "                print(f\"[AutoFiring] Input: {input_value:.4f} -> Output: {output_value:.4f}, Path Length: {len(path)}\")\n",
    "                time.sleep(self.firing_interval_ms / 1000.0)\n",
    "        self.auto_fire_thread = threading.Thread(target=auto_fire_loop, daemon=True)\n",
    "        self.auto_fire_thread.start()\n",
    "\n",
    "    def stop_auto_firing(self):\n",
    "        self.auto_fire_active = False\n",
    "        if self.auto_fire_thread is not None:\n",
    "            self.auto_fire_thread.join()\n",
    "        print(\"Auto-firing stopped.\")\n",
    "\n",
    "    def display_live_status(self, validation_examples):\n",
    "        status = self.core.get_detailed_status()\n",
    "        current_val_loss = self.validate(validation_examples)\n",
    "        print(\"----- Live Status -----\")\n",
    "        for tier in status:\n",
    "            print(f\"{tier.upper()} -> Neurons: {status[tier]['neuron_count']}, \"\n",
    "                  f\"Synapses: {status[tier]['synapse_count']}, \"\n",
    "                  f\"Memory: {status[tier]['memory_mb']:.2f} MB\")\n",
    "        print(f\"Current Validation Loss: {current_val_loss:.4f}\")\n",
    "        print(f\"Global Activation Count: {self.neuronenblitz.global_activation_count}\")\n",
    "        print(\"-----------------------\")\n",
    "\n",
    "class MarbleConverter:\n",
    "    @staticmethod\n",
    "    def convert(model, mode='sequential', core_params=None):\n",
    "        if core_params is None:\n",
    "            core_params = {\n",
    "                'vram_limit_mb': 100,\n",
    "                'ram_limit_mb': 500,\n",
    "                'disk_limit_mb': 10000,\n",
    "                'xmin': -2.0, 'xmax': 1.0,\n",
    "                'ymin': -1.5, 'ymax': 1.5,\n",
    "                'width': 30, 'height': 30,\n",
    "                'max_iter': 50\n",
    "            }\n",
    "        if isinstance(model, str):\n",
    "            try:\n",
    "                model = torch.load(model, map_location='cpu')\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"Model loading error: {e}\")\n",
    "        \n",
    "        new_core = Core(core_params, formula=\"0\", formula_num_neurons=0)\n",
    "        if mode == 'sequential':\n",
    "            layers = [module for module in model.modules() if isinstance(module, nn.Linear)]\n",
    "            prev_neuron_ids = []\n",
    "            for idx, layer in enumerate(layers):\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    in_features = layer.in_features\n",
    "                    out_features = layer.out_features\n",
    "                    if idx == 0:\n",
    "                        input_ids = list(range(len(new_core.neurons), len(new_core.neurons) + in_features))\n",
    "                        for i in range(in_features):\n",
    "                            new_core.neurons.append(Neuron(input_ids[i], value=0.0, tier='vram'))\n",
    "                        prev_neuron_ids = input_ids\n",
    "                    output_ids = list(range(len(new_core.neurons), len(new_core.neurons) + out_features))\n",
    "                    for j in range(out_features):\n",
    "                        new_core.neurons.append(Neuron(output_ids[j], value=0.0, tier='vram'))\n",
    "                    weight_matrix = layer.weight.detach().cpu().numpy()\n",
    "                    for j, out_id in enumerate(output_ids):\n",
    "                        for i, in_id in enumerate(prev_neuron_ids):\n",
    "                            weight_val = float(weight_matrix[j, i])\n",
    "                            syn = Synapse(in_id, out_id, weight=weight_val)\n",
    "                            new_core.neurons[in_id].synapses.append(syn)\n",
    "                            new_core.synapses.append(syn)\n",
    "                    prev_neuron_ids = output_ids\n",
    "        return new_core"
   ]
  }"
,
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experimental Implementation\n",
    "We demonstrate MARBLE's capabilities through integration with Stable Diffusion 3.5, showcasing adaptive memory management and dynamic learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def log_metrics(epoch, loss, vram_usage):\n",
    "    with open('training_log.txt', 'a') as f:\n",
    "        f.write(f\"Epoch {epoch}: Loss={loss:.4f}, VRAM={vram_usage:.2f}MB\\n\")\n",
    "\n",
    "# Initialize parameters for the full pipeline\n",
    "params = {\n",
    "    'xmin': -2.0,\n",
    "    'xmax': 1.0,\n",
    "    'ymin': -1.5,\n",
    "    'ymax': 1.5,\n",
    "    'width': 64,\n",
    "    'height': 64,\n",
    "    'max_iter': 100,\n",
    "    'vram_limit_mb': 8000,\n",
    "    'ram_limit_mb': 16000,\n",
    "    'disk_limit_mb': 32000\n",
    "}\n",
    "\n",
    "# Enhanced formula including diffusion time component\n",
    "formula = \"log(1+T)/log(1+I) * exp(-D/2)\"\n",
    "initial_core = Core(params, formula=formula, formula_num_neurons=1000)\n",
    "print(f\"Initial core: {len(initial_core.neurons)} neurons, {len(initial_core.synapses)} synapses.\")\n",
    "\n",
    "# Load and convert Stable Diffusion pipeline\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-3.5-large\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ").to(\"cuda\")\n",
    "converted_core = MarbleConverter.convert(pipe, mode='sequential', core_params=params)\n",
    "core = converted_core\n",
    "print(f\"Converted core: {len(core.neurons)} neurons, {len(core.synapses)} synapses.\")\n",
    "\n",
    "# Initialize MARBLE components\n",
    "nb = Neuronenblitz(\n",
    "    core,\n",
    "    wander_steps=15,\n",
    "    learning_rate=0.005,\n",
    "    backtrack_probability=0.4,\n",
    "    consolidation_probability=0.3,\n",
    "    consolidation_strength=1.2,\n",
    "    route_potential_increase=0.6,\n",
    "    route_potential_decay=0.85\n",
    ")\n",
    "dl = DataLoader()\n",
    "brain = Brain(\n",
    "    core, \n",
    "    nb, \n",
    "    dl, \n",
    "    save_threshold=0.03,\n",
    "    max_saved_models=10,\n",
    "    firing_interval_ms=500\n",
    ")\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"laion-aesthetics-v2-5plus\", split=\"train\")\n",
    "subset_size = 1000\n",
    "if len(dataset) > subset_size:\n",
    "    dataset = dataset.select(range(subset_size))\n",
    "\n",
    "# Data preprocessing with batching\n",
    "def preprocess(sample):\n",
    "    caption_bytes = sample[\"caption\"].strip().encode(\"utf-8\")\n",
    "    buffer = BytesIO()\n",
    "    sample[\"image\"].save(buffer, format=\"PNG\")\n",
    "    image_bytes = buffer.getvalue()\n",
    "    return caption_bytes, image_bytes\n",
    "\n",
    "batch_size = 4\n",
    "train_examples = []\n",
    "val_examples = []\n",
    "\n",
    "for i in range(0, len(dataset), batch_size):\n",
    "    batch = dataset[i:i+batch_size]\n",
    "    batch_examples = [preprocess(sample) for sample in batch]\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        val_examples.extend(batch_examples)\n",
    "    else:\n",
    "        train_examples.extend(batch_examples)\n",
    "        \n",
    "print(f\"Training examples: {len(train_examples)}, Validation examples: {len(val_examples)}\")\n",
    "\n",
    "# Image generation function\n",
    "def generate_image(core, target_shape):\n",
    "    required_neurons = target_shape[0] * target_shape[1] * target_shape[2]\n",
    "    if len(core.neurons) < required_neurons:\n",
    "        additional = required_neurons - len(core.neurons)\n",
    "        core.expand(num_new_neurons=additional, num_new_synapses=additional//2)\n",
    "        print(f\"Core expanded to {len(core.neurons)} neurons for image generation.\")\n",
    "    \n",
    "    neuron_values = np.array([n.value for n in core.neurons[-required_neurons:]])\n",
    "    image_array = neuron_values.reshape(target_shape)\n",
    "    \n",
    "    min_val, max_val = np.percentile(image_array, [2, 98])\n",
    "    if max_val - min_val > 0:\n",
    "        image_array = np.clip((image_array - min_val) / (max_val - min_val), 0, 1) * 255.0\n",
    "    else:\n",
    "        image_array = np.full(target_shape, 127.0)\n",
    "    \n",
    "    image_array = np.clip(image_array, 0, 255).astype(np.uint8)\n",
    "    return Image.fromarray(image_array)"
   ]
  }"
  ,
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Training Loop and Inference\n",
    "Execute the training process and demonstrate the model's image generation capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Start auto-firing\n",
    "brain.start_auto_firing()\n",
    "\n",
    "# Training loop with metrics\n",
    "num_epochs = 5\n",
    "epoch_pbar = tqdm(range(num_epochs), desc=\"Epochs\", ncols=100)\n",
    "\n",
    "for epoch in epoch_pbar:\n",
    "    brain.train(train_examples, epochs=1, validation_examples=val_examples)\n",
    "    losses = []\n",
    "    \n",
    "    for i in range(0, len(val_examples), batch_size):\n",
    "        batch = val_examples[i:i+batch_size]\n",
    "        batch_losses = []\n",
    "        \n",
    "        for inp_bytes, target_bytes in batch:\n",
    "            encoded_prompt = dl.encode(inp_bytes)\n",
    "            input_scalar = float(np.mean(encoded_prompt))\n",
    "            nb.dynamic_wander(input_scalar)\n",
    "            \n",
    "            target_img = Image.open(BytesIO(target_bytes))\n",
    "            target_shape = (target_img.height, target_img.width, 3)\n",
    "            generated_img = generate_image(core, target_shape)\n",
    "            \n",
    "            target_arr = np.array(target_img.convert(\"RGB\")).astype(np.float32)\n",
    "            gen_arr = np.array(generated_img).astype(np.float32)\n",
    "            loss = np.mean((target_arr - gen_arr) ** 2)\n",
    "            batch_losses.append(loss)\n",
    "        \n",
    "        losses.extend(batch_losses)\n",
    "        \n",
    "    mean_val_loss = np.mean(losses) if losses else 0\n",
    "    vram_usage = core.get_usage_by_tier('vram')\n",
    "    \n",
    "    log_metrics(epoch, mean_val_loss, vram_usage)\n",
    "    \n",
    "    epoch_pbar.set_postfix({\n",
    "        \"MeanValLoss\": f\"{mean_val_loss:.4f}\",\n",
    "        \"GlobalActs\": f\"{nb.global_activation_count}\",\n",
    "        \"VRAM(MB)\": f\"{vram_usage:.2f}\"\n",
    "    })\n",
    "\n",
    "epoch_pbar.close()\n",
    "\n",
    "# Stop auto-firing\n",
    "brain.stop_auto_firing()\n",
    "print(\"\\nTraining completed.\")\n",
    "\n",
    "# Inference demonstration\n",
    "test_prompts = [\n",
    "    \"A futuristic cityscape at sunset with neon lights.\",\n",
    "    \"A serene mountain landscape with a crystal clear lake.\",\n",
    "    \"An abstract representation of quantum mechanics.\"\n",
    "]\n",
    "\n",
    "for idx, prompt_text in enumerate(test_prompts):\n",
    "    prompt_bytes = prompt_text.strip().encode(\"utf-8\")\n",
    "    encoded_prompt = dl.encode(prompt_bytes)\n",
    "    input_scalar = float(np.mean(encoded_prompt))\n",
    "    nb.dynamic_wander(input_scalar)\n",
    "    \n",
    "    _, target_bytes = val_examples[0]\n",
    "    target_img = Image.open(BytesIO(target_bytes))\n",
    "    target_shape = (target_img.height, target_img.width, 3)\n",
    "    \n",
    "    final_generated_img = generate_image(core, target_shape)\n",
    "    final_generated_img.save(f\"generated_image_{idx+1}.png\")\n",
    "    \n",
    "print(\"Inference completed. Multiple test images generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results and Analysis\n",
    "The experimental results demonstrate MARBLE's effectiveness in:\n",
    "- Dynamic memory management across tiers\n",
    "- Adaptive learning with complex models\n",
    "- Efficient resource utilization\n",
    "\n",
    "## 5. Conclusion\n",
    "MARBLE represents a significant advancement in adaptive neural architectures, providing a robust foundation for memory-efficient AI systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 }
}

